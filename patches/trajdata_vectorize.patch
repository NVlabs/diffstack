diff --git a/.gitignore b/.gitignore
index 1e55dd9..1ad9436 100644
--- a/.gitignore
+++ b/.gitignore
@@ -157,3 +157,4 @@ cython_debug/
 #  option (not recommended) you can uncomment the following to ignore the entire idea folder.
 #.idea/
 
+tests/Drivesim_scene_generation.html
diff --git a/CITATION.cff b/CITATION.cff
index e793eb6..dbf28a0 100644
--- a/CITATION.cff
+++ b/CITATION.cff
@@ -5,23 +5,7 @@ authors:
   given-names: "Boris"
   orcid: "https://orcid.org/0000-0002-8698-202X"
 title: "trajdata: A unified interface to many trajectory forecasting datasets"
-version: 1.3.3
+version: 1.0.3
 doi: 10.5281/zenodo.6671548
-date-released: 2023-08-22
-url: "https://github.com/nvr-avg/trajdata"
-preferred-citation:
-  type: conference-paper
-  authors:
-  - family-names: "Ivanovic"
-    given-names: "Boris"
-    orcid: "https://orcid.org/0000-0002-8698-202X"
-  - family-names: "Song"
-    given-names: "Guanyu"
-  - family-names: "Gilitschenski"
-    given-names: "Igor"
-  - family-names: "Pavone"
-    given-names: "Marco"
-  journal: "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks"
-  month: 12
-  title: "trajdata: A Unified Interface to Multiple Human Trajectory Datasets"
-  year: 2023
\ No newline at end of file
+date-released: 2022-06-20
+url: "https://github.com/nvr-avg/trajdata"
\ No newline at end of file
diff --git a/README.md b/README.md
index 774d6d6..a715ebc 100644
--- a/README.md
+++ b/README.md
@@ -1,4 +1,4 @@
-# trajdata: A Unified Interface to Multiple Human Trajectory Datasets
+# Unified Trajectory Data Loader
 
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
 [![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)
@@ -6,10 +6,6 @@
 [![DOI](https://zenodo.org/badge/488789438.svg)](https://zenodo.org/badge/latestdoi/488789438)
 [![PyPI version](https://badge.fury.io/py/trajdata.svg)](https://badge.fury.io/py/trajdata)
 
-### Announcements
-
-**Sept 2023**: [Our paper about trajdata](https://arxiv.org/abs/2307.13924) has been accepted to the NeurIPS 2023 Datasets and Benchmarks Track!
-
 ## Installation
 
 The easiest way to install trajdata is through PyPI with
@@ -211,20 +207,6 @@ for t in range(1, sim_scene.scene.length_timesteps):
 
 `examples/sim_example.py` contains a more comprehensive example which initializes a simulation from a scene in the nuScenes mini dataset, steps through it by replaying agents' GT motions, and computes metrics based on scene statistics (e.g., displacement error from the original GT data, velocity/acceleration/jerk histograms).
 
-## Citation
-
-If you use this software, please cite it as follows:
-```
-@Inproceedings{ivanovic2023trajdata,
-  author = {Ivanovic, Boris and Song, Guanyu and Gilitschenski, Igor and Pavone, Marco},
-  title = {{trajdata}: A Unified Interface to Multiple Human Trajectory Datasets},
-  booktitle = {{Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks}},
-  month = dec,
-  year = {2023},
-  address = {New Orleans, USA},
-  url = {https://arxiv.org/abs/2307.13924}
-}
-```
-
 ## TODO
 - Create a method like finalize() which writes all the batch information to a TFRecord/WebDataset/some other format which is (very) fast to read from for higher epoch training.
+- Add more examples to the README.
diff --git a/copy_to_public.sh b/copy_to_public.sh
new file mode 100755
index 0000000..dba3133
--- /dev/null
+++ b/copy_to_public.sh
@@ -0,0 +1 @@
+rsync -ravh --progress --exclude ".gitlab*" --exclude "public/" --exclude "opendrive" --exclude "copy_to_public.sh" --exclude "*.pyc" --exclude "ngc/" --exclude "*.egg-info/" --exclude "__pycache__/" ./* public/
\ No newline at end of file
diff --git a/examples/map_api_example.py b/examples/map_api_example.py
index 759b2e0..75077ce 100644
--- a/examples/map_api_example.py
+++ b/examples/map_api_example.py
@@ -4,6 +4,7 @@ from typing import Dict, List, Optional
 
 import matplotlib.pyplot as plt
 import numpy as np
+import os
 
 from trajdata import MapAPI, VectorMap
 from trajdata.caching.df_cache import DataFrameCache
@@ -23,7 +24,7 @@ def load_random_scene(cache_path: Path, env_name: str, scene_dt: float) -> Scene
 
 
 def main():
-    cache_path = Path("~/.unified_data_cache").expanduser()
+    cache_path = Path(os.environ["TRAJDATA_CACHE_DIR"]).expanduser()
     map_api = MapAPI(cache_path)
 
     ### Loading random scene and initializing VectorMap.
diff --git a/ngc/Dockerfile b/ngc/Dockerfile
new file mode 100644
index 0000000..c5841bc
--- /dev/null
+++ b/ngc/Dockerfile
@@ -0,0 +1,27 @@
+FROM nvcr.io/nvidia/pytorch:21.03-py3
+ARG PYTHON_VERSION=3.8
+
+RUN apt-get update
+RUN apt-get install htop -y
+RUN apt-get install screen -y
+RUN apt-get install psmisc -y
+
+RUN pip install --upgrade pip
+
+RUN pip install \
+numpy==1.19 \
+tqdm==4.62 \
+matplotlib==3.5 \
+dill==0.3.4 \
+pandas==1.4.1 \
+pyarrow==7.0.0 \
+nuscenes-devkit==1.1.9 \
+l5kit==1.5.0 \
+black==22.1.0 \
+isort==5.10.1 \
+pytest==7.1.1 \
+pytest-xdist==2.5.0 \
+zarr==2.11.0 \
+kornia==0.6.4 
+
+WORKDIR /workspace/trajdata
diff --git a/ngc/copy_to_ngc.sh b/ngc/copy_to_ngc.sh
new file mode 100755
index 0000000..c42315f
--- /dev/null
+++ b/ngc/copy_to_ngc.sh
@@ -0,0 +1,4 @@
+#!/bin/bash
+
+cd ../..
+rsync -ravh --progress --exclude="*.pyc" --exclude=".git" --exclude="__pycache__" --exclude="*.egg-info" --exclude=".pytest_cache" trajdata/ ngc-trajdata/
diff --git a/ngc/jupyter_lab.json b/ngc/jupyter_lab.json
new file mode 100644
index 0000000..7d43c0c
--- /dev/null
+++ b/ngc/jupyter_lab.json
@@ -0,0 +1,49 @@
+{
+    "userLabels":[
+        
+    ],
+    "aceId":257,
+    "aceInstance":"dgx1v.16g.8.norm",
+    "dockerImageName":"nvidian/nvr-av/trajdata:1.0",
+    "aceName":"nv-us-west-2",
+    "systemLabels":[
+        
+    ],
+    "datasetMounts":[
+        {
+            "containerMountPoint":"/workspace/lyft",
+            "id":90893
+        },
+        {
+            "containerMountPoint":"/workspace/nuScenes",
+            "id":78251
+        }
+    ],
+    "workspaceMounts":[
+        {
+            "containerMountPoint":"/workspace/trajdata",
+            "id":"aWUsWZ_5SA6caFr30uWmmw",
+            "mountMode":"RW"
+        },
+        {
+            "containerMountPoint":"/workspace/trajdata_cache",
+            "id":"BgHSNdSaR0ywsc-UY-xm3w",
+            "mountMode":"RW"
+        }
+    ],
+    "replicaCount":1,
+    "publishedContainerPorts":[
+        8888
+    ],
+    "reservedLabels":[
+        
+    ],
+    "name":"ml-model.notamodel-trajdata-processing",
+    "command":"pip install -e .; jupyter lab --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' --notebook-dir=/ --NotebookApp.allow_origin='*' & date; nvidia-smi; sleep 1d",
+    "runPolicy":{
+        "minTimesliceSeconds":1,
+        "totalRuntimeSeconds":86400,
+        "preemptClass":"RUNONCE"
+    },
+    "resultContainerMountPoint":"/results"
+}
diff --git a/ngc/ngc_test.py b/ngc/ngc_test.py
new file mode 100644
index 0000000..c63312e
--- /dev/null
+++ b/ngc/ngc_test.py
@@ -0,0 +1,57 @@
+import os
+from collections import defaultdict
+
+from torch.utils.data import DataLoader
+from tqdm import tqdm
+
+from trajdata import AgentBatch, AgentType, UnifiedDataset
+from trajdata.augmentation import NoiseHistories
+from trajdata.visualization.vis import plot_agent_batch
+
+
+# @profile
+def main():
+    noise_hists = NoiseHistories()
+
+    dataset = UnifiedDataset(
+        desired_data=["nusc"],
+        centric="agent",
+        # desired_dt=0.1,
+        history_sec=(0.1, 1.5),
+        future_sec=(0.1, 5.0),
+        only_types=[AgentType.VEHICLE, AgentType.PEDESTRIAN],
+        agent_interaction_distances=defaultdict(lambda: 40.0),
+        incl_robot_future=True,
+        incl_raster_map=True,
+        raster_map_params={
+            "px_per_m": 2,
+            "map_size_px": 224,
+            "offset_frac_xy": (-0.5, 0.0),
+        },
+        # augmentations=[noise_hists],
+        data_dirs={
+            "nusc": "/workspace/datasets/nuScenes",
+        },
+        cache_location="/workspace/unified_data_cache",
+        num_workers=os.cpu_count(),
+        # verbose=True,
+    )
+
+    print(f"# Data Samples: {len(dataset):,}")
+
+    dataloader = DataLoader(
+        dataset,
+        batch_size=64,
+        shuffle=True,
+        collate_fn=dataset.get_collate_fn(),
+        num_workers=os.cpu_count(),
+    )
+
+    batch: AgentBatch
+    for batch in tqdm(dataloader):
+        pass
+        # plot_agent_batch(batch, batch_idx=0)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/ngc/preprocess_data_ngc.py b/ngc/preprocess_data_ngc.py
new file mode 100644
index 0000000..920f439
--- /dev/null
+++ b/ngc/preprocess_data_ngc.py
@@ -0,0 +1,33 @@
+from trajdata import UnifiedDataset
+
+
+def main():
+    dataset = UnifiedDataset(
+        desired_data=[
+            "nusc_trainval",
+            "nusc_mini",
+            "lyft_sample",
+            "lyft_train",
+            # "lyft_train_full",
+            "lyft_val",
+        ],
+        data_dirs={
+            "nusc_trainval": "/workspace/nuScenes",
+            "nusc_mini": "/workspace/nuScenes",
+            "lyft_sample": "/workspace/lyft/lyft_prediction/scenes/sample.zarr",
+            "lyft_train": "/workspace/lyft/lyft_prediction/scenes/train.zarr",
+            # "lyft_train_full": "/workspace/lyft/lyft_prediction/scenes/train_full.zarr",
+            "lyft_val": "/workspace/lyft/lyft_prediction/scenes/validate.zarr",
+        },
+        cache_location="/workspace/trajdata_cache",
+        rebuild_cache=True,
+        rebuild_maps=True,
+        num_workers=64,
+        verbose=True,
+    )
+
+    print(f"Total Data Samples: {len(dataset):,}")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..97d46f9
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,26 @@
+numpy
+tqdm
+matplotlib==3.3.4
+dill
+pandas
+seaborn
+pyarrow
+torch
+zarr
+kornia
+intervaltree
+
+# nuScenes devkit
+nuscenes-devkit==1.1.9
+
+# Lyft Level 5 devkit
+protobuf==3.19.4
+l5kit==1.5.0
+
+# Development
+black
+isort
+pytest
+pytest-xdist
+twine
+build
diff --git a/setup.cfg b/setup.cfg
new file mode 100644
index 0000000..01016fc
--- /dev/null
+++ b/setup.cfg
@@ -0,0 +1,50 @@
+[metadata]
+name = trajdata
+version = 1.1.0
+author = Boris Ivanovic
+author_email = bivanovic@nvidia.com
+description = A unified interface to many trajectory forecasting datasets.
+long_description = file: README.md
+long_description_content_type = text/markdown
+# license = Apache License 2.0
+url = https://github.com/nvr-avg/trajdata
+classifiers =
+    Development Status :: 3 - Alpha
+    Intended Audience :: Developers
+    Programming Language :: Python :: 3.8
+    License :: OSI Approved :: Apache Software License
+
+[options]
+package_dir =
+    = src
+packages = find:
+python_requires = >=3.8
+install_requires =
+    numpy>=1.19
+    tqdm>=4.62
+    matplotlib>=3.5
+    dill>=0.3.4
+    pandas>=1.4.1
+    pyarrow>=7.0.0
+    torch>=1.10.2
+    zarr>=2.11.0
+    kornia>=0.6.4
+    seaborn>=0.12
+    intervaltree
+
+[options.packages.find]
+where = src
+
+[options.extras_require]
+dev =
+    black
+    isort
+    pytest
+    pytest-xdist
+    twine
+    build
+nusc =
+    nuscenes-devkit==1.1.9
+lyft =
+    protobuf==3.20.3
+    l5kit==1.5.0
diff --git a/src/trajdata/caching/df_cache.py b/src/trajdata/caching/df_cache.py
index eb0efef..654977c 100644
--- a/src/trajdata/caching/df_cache.py
+++ b/src/trajdata/caching/df_cache.py
@@ -30,6 +30,7 @@ from trajdata.data_structures.scene_metadata import Scene
 from trajdata.data_structures.state import NP_STATE_TYPES, StateArray
 from trajdata.maps.traffic_light_status import TrafficLightStatus
 from trajdata.utils import arr_utils, df_utils, raster_utils, state_utils
+from trajdata.utils.scene_utils import is_integer_robust
 
 STATE_COLS: Final[List[str]] = ["x", "y", "z", "vx", "vy", "ax", "ay"]
 EXTENT_COLS: Final[List[str]] = ["length", "width", "height"]
@@ -320,7 +321,7 @@ class DataFrameCache(SceneCache):
     def _upsample_data(
         self, new_index: pd.MultiIndex, upsample_dt_ratio: float, method: str
     ) -> pd.DataFrame:
-        upsample_dt_factor: int = int(upsample_dt_ratio)
+        upsample_dt_factor: int = int(round(upsample_dt_ratio))
 
         interpolated_df: pd.DataFrame = pd.DataFrame(
             index=new_index, columns=self.scene_data_df.columns
@@ -353,7 +354,7 @@ class DataFrameCache(SceneCache):
     def _downsample_data(
         self, new_index: pd.MultiIndex, downsample_dt_ratio: float
     ) -> pd.DataFrame:
-        downsample_dt_factor: int = int(downsample_dt_ratio)
+        downsample_dt_factor: int = int(round(downsample_dt_ratio))
 
         subsample_index: pd.MultiIndex = new_index.set_levels(
             new_index.levels[1] * downsample_dt_factor, level=1
@@ -368,7 +369,8 @@ class DataFrameCache(SceneCache):
     def interpolate_data(self, desired_dt: float, method: str = "linear") -> None:
         upsample_dt_ratio: float = self.scene.env_metadata.dt / desired_dt
         downsample_dt_ratio: float = desired_dt / self.scene.env_metadata.dt
-        if not upsample_dt_ratio.is_integer() and not downsample_dt_ratio.is_integer():
+        
+        if not is_integer_robust(upsample_dt_ratio) and not is_integer_robust(downsample_dt_ratio):
             raise ValueError(
                 f"{str(self.scene)}'s dt of {self.scene.dt}s "
                 f"is not integer divisible by the desired dt {desired_dt}s."
diff --git a/src/trajdata/data_structures/agent.py b/src/trajdata/data_structures/agent.py
index 7d21217..0687d8e 100644
--- a/src/trajdata/data_structures/agent.py
+++ b/src/trajdata/data_structures/agent.py
@@ -11,6 +11,7 @@ class AgentType(IntEnum):
     PEDESTRIAN = 2
     BICYCLE = 3
     MOTORCYCLE = 4
+    STATIC = 5
 
 
 class Extent:
diff --git a/src/trajdata/data_structures/batch.py b/src/trajdata/data_structures/batch.py
index c93fb8b..1903557 100644
--- a/src/trajdata/data_structures/batch.py
+++ b/src/trajdata/data_structures/batch.py
@@ -1,6 +1,6 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
+from dataclasses import dataclass, replace
 from typing import Dict, List, Optional, Union
 
 import torch
@@ -9,7 +9,7 @@ from torch import Tensor
 from trajdata.data_structures.agent import AgentType
 from trajdata.data_structures.state import StateTensor
 from trajdata.maps import VectorMap
-from trajdata.utils.arr_utils import PadDirection
+from trajdata.utils.arr_utils import PadDirection, batch_nd_transform_xyvvaahh_pt, roll_with_tensor, transform_xyh_torch
 
 
 @dataclass
@@ -39,6 +39,10 @@ class AgentBatch:
     map_names: Optional[List[str]]
     maps: Optional[Tensor]
     maps_resolution: Optional[Tensor]
+    lane_xyh: Optional[Tensor]
+    lane_adj: Optional[Tensor]
+    lane_ids: Optional[List[List[str]]]
+    lane_mask: Optional[Tensor]
     vector_maps: Optional[List[VectorMap]]
     rasters_from_world_tf: Optional[Tensor]
     agents_from_world_tf: Tensor
@@ -141,12 +145,16 @@ class AgentBatch:
             maps_resolution=_filter(self.maps_resolution)
             if self.maps_resolution is not None
             else None,
-            vector_maps=_filter(self.vector_maps)
+            vector_maps=_filter_tensor_or_list(self.vector_maps)
             if self.vector_maps is not None
             else None,
             rasters_from_world_tf=_filter(self.rasters_from_world_tf)
             if self.rasters_from_world_tf is not None
             else None,
+            lane_xyh=_filter(self.lane_xyh) if self.lane_xyh is not None else None,
+            lane_adj=_filter(self.lane_adj) if self.lane_adj is not None else None,
+            lane_ids=self.lane_ids,
+            lane_mask=_filter(self.lane_mask) if self.lane_mask is not None else None,
             agents_from_world_tf=_filter(self.agents_from_world_tf),
             scene_ids=_filter_tensor_or_list(self.scene_ids),
             history_pad_dir=self.history_pad_dir,
@@ -155,6 +163,48 @@ class AgentBatch:
             },
         )
 
+    def to_scene_batch(self, agent_ind: int) -> SceneBatch:
+        """
+        Converts AgentBatch to SeceneBatch by combining neighbors and agent.
+
+        The agent of AgentBatch will be treated as if it was the last neighbor.
+        self.extras will be simply copied over, any custom conversion must be
+        implemented externally.
+        """
+
+        batch_size = self.neigh_hist.shape[0]
+        num_neigh = self.neigh_hist.shape[1]
+
+        combine = lambda neigh, agent: torch.cat((neigh, agent.unsqueeze(0)), dim=0)
+        combine_list = lambda neigh, agent: neigh + [agent]
+
+        return SceneBatch(
+            data_idx=self.data_idx,
+            scene_ts=self.scene_ts,
+            dt=self.dt,
+            num_agents=self.num_neigh + 1,
+            agent_type=combine(self.neigh_types, self.agent_type),
+            centered_agent_state=self.curr_agent_state,  # TODO this is not actually the agent but the `global` coordinate frame
+            agent_names=combine_list(["UNKNOWN" for _ in range(num_neigh)], self.agent_name),
+            agent_hist=combine(self.neigh_hist, self.agent_hist),
+            agent_hist_extent=combine(self.neigh_hist_extents, self.agent_hist_extent),
+            agent_hist_len=combine(self.neigh_hist_len, self.agent_hist_len),
+            agent_fut=combine(self.neigh_fut, self.agent_fut),
+            agent_fut_extent=combine(self.neigh_fut_extents, self.agent_fut_extent),
+            agent_fut_len=combine(self.neigh_fut_len, self.agent_fut_len),
+            robot_fut=self.robot_fut,
+            robot_fut_len=self.robot_fut_len,
+            map_names=self.map_names,  # TODO
+            maps=self.maps,
+            maps_resolution=self.maps_resolution,
+            vector_maps=self.vector_maps,
+            rasters_from_world_tf=self.rasters_from_world_tf,
+            centered_agent_from_world_tf=self.agents_from_world_tf,
+            centered_world_from_agent_tf=torch.linalg.inv(self.agents_from_world_tf),
+            scene_ids=self.scene_ids,
+            history_pad_dir=self.history_pad_dir,
+            extras=self.extras,
+        )
 
 @dataclass
 class SceneBatch:
@@ -164,7 +214,8 @@ class SceneBatch:
     num_agents: Tensor
     agent_type: Tensor
     centered_agent_state: StateTensor
-    agent_names: List[str]
+    agent_names: List[List[str]]
+    track_ids:Optional[List[List[str]]]
     agent_hist: StateTensor
     agent_hist_extent: Tensor
     agent_hist_len: Tensor
@@ -177,6 +228,10 @@ class SceneBatch:
     maps: Optional[Tensor]
     maps_resolution: Optional[Tensor]
     vector_maps: Optional[List[VectorMap]]
+    lane_xyh: Optional[Tensor]
+    lane_adj: Optional[Tensor]
+    lane_ids: Optional(List[List[str]])
+    lane_mask: Optional[Tensor]
     rasters_from_world_tf: Optional[Tensor]
     centered_agent_from_world_tf: Tensor
     centered_world_from_agent_tf: Tensor
@@ -186,12 +241,19 @@ class SceneBatch:
 
     def to(self, device) -> None:
         excl_vals = {
+            "num_agents",
             "agent_names",
+            "track_ids",
+            "agent_type",
+            "agent_hist_len",
+            "agent_fut_len",
+            "robot_fut_len",
             "map_names",
             "vector_maps",
             "history_pad_dir",
             "scene_ids",
             "extras",
+            "lane_ids",
         }
 
         for val in vars(self).keys():
@@ -205,6 +267,31 @@ class SceneBatch:
                 self.extras[key] = val.__to__(device, non_blocking=True)
             else:
                 self.extras[key] = val.to(device, non_blocking=True)
+        return self
+
+    def astype(self, dtype) -> None:
+        new_obj = replace(self)
+        excl_vals = {
+            "num_agents",
+            "agent_names",
+            "track_ids",
+            "agent_type",
+            "agent_hist_len",
+            "agent_fut_len",
+            "robot_fut_len",
+            "map_names",
+            "vector_maps",
+            "history_pad_dir",
+            "scene_ids",
+            "extras",
+            "lane_ids",
+        }
+
+        for val in vars(self).keys():
+            tensor_val = getattr(self, val)
+            if val not in excl_vals and tensor_val is not None:
+                setattr(new_obj, val, tensor_val.type(dtype))
+        return new_obj
 
     def agent_types(self) -> List[AgentType]:
         unique_types: Tensor = torch.unique(self.agent_type)
@@ -214,13 +301,31 @@ class SceneBatch:
             if unique_type >= 0
         ]
 
-    def for_agent_type(self, agent_type: AgentType) -> SceneBatch:
-        match_type = self.agent_type == agent_type
-        return self.filter_batch(match_type)
+    def copy(self):
+        # Shallow copy
+        return replace(self)
+
+    def convert_pad_direction(self, pad_dir: PadDirection) -> SceneBatch:
+        if self.history_pad_dir == pad_dir:
+            return self
+        batch: SceneBatch = self.copy()
+        if self.history_pad_dir == PadDirection.BEFORE:
+            # n, n, -2 , -1, 0 -->  -2, -1, 0, n, n
+            shifts = batch.agent_hist_len  
+        else: 
+            #  -2, -1, 0, n, n --> n, n, -2 , -1, 0
+            shifts = -batch.agent_hist_len  
+        batch.agent_hist = roll_with_tensor(batch.agent_hist, shifts, dim=-2)
+        batch.agent_hist_extent = roll_with_tensor(batch.agent_hist_extent, shifts, dim=-2)
+        batch.history_pad_dir = pad_dir
+        return batch
 
-    def filter_batch(self, filter_mask: torch.tensor) -> SceneBatch:
+    def filter_batch(self, filter_mask: torch.Tensor) -> SceneBatch:
         """Build a new batch with elements for which filter_mask[i] == True."""
 
+        if filter_mask.ndim != 1:
+            raise ValueError("Expected 1d filter mask.")
+
         # Some of the tensors might be on different devices, so we define some convenience functions
         # to make sure the filter_mask is always on the same device as the tensor we are indexing.
         filter_mask_dict = {}
@@ -229,7 +334,8 @@ class SceneBatch:
             self.agent_hist.device
         )
 
-        _filter = lambda tensor: tensor[filter_mask_dict[str(tensor.device)]]
+        # Use tensor.__class__ to keep TensorState. This might 
+        _filter = lambda tensor: tensor.__class__(tensor[filter_mask_dict[str(tensor.device)]])
         _filter_tensor_or_list = lambda tensor_or_list: (
             _filter(tensor_or_list)
             if isinstance(tensor_or_list, torch.Tensor)
@@ -248,6 +354,8 @@ class SceneBatch:
             dt=_filter(self.dt),
             num_agents=_filter(self.num_agents),
             agent_type=_filter(self.agent_type),
+            agent_names=_filter_tensor_or_list(self.agent_names),
+            track_ids=_filter_tensor_or_list(self.track_ids),
             centered_agent_state=_filter(self.centered_agent_state),
             agent_hist=_filter(self.agent_hist),
             agent_hist_extent=_filter(self.agent_hist_extent),
@@ -266,9 +374,13 @@ class SceneBatch:
             maps_resolution=_filter(self.maps_resolution)
             if self.maps_resolution is not None
             else None,
-            vector_maps=_filter(self.vector_maps)
+            vector_maps=_filter_tensor_or_list(self.vector_maps)
             if self.vector_maps is not None
             else None,
+            lane_xyh=_filter(self.lane_xyh) if self.lane_xyh is not None else None,
+            lane_adj=_filter(self.lane_adj) if self.lane_adj is not None else None,
+            lane_ids = self.lane_ids,
+            lane_mask = _filter(self.lane_mask) if self.lane_mask is not None else None,
             rasters_from_world_tf=_filter(self.rasters_from_world_tf)
             if self.rasters_from_world_tf is not None
             else None,
@@ -277,7 +389,7 @@ class SceneBatch:
             scene_ids=_filter_tensor_or_list(self.scene_ids),
             history_pad_dir=self.history_pad_dir,
             extras={
-                key: _filter_tensor_or_list(val, filter_mask)
+                key: _filter_tensor_or_list(val)
                 for key, val in self.extras.items()
             },
         )
@@ -321,31 +433,68 @@ class SceneBatch:
             scene_ts=self.scene_ts,
             dt=self.dt,
             agent_name=index_agent_list(self.agent_names),
+            track_ids=index_agent_list(self.track_ids),
             agent_type=index_agent(self.agent_type),
             curr_agent_state=self.centered_agent_state,  # TODO this is not actually the agent but the `global` coordinate frame
-            agent_hist=index_agent(self.agent_hist),
+            agent_hist=StateTensor.from_array(index_agent(self.agent_hist), self.agent_hist._format),
             agent_hist_extent=index_agent(self.agent_hist_extent),
             agent_hist_len=index_agent(self.agent_hist_len),
-            agent_fut=index_agent(self.agent_fut),
+            agent_fut=StateTensor.from_array(index_agent(self.agent_fut), self.agent_fut._format),
             agent_fut_extent=index_agent(self.agent_fut_extent),
             agent_fut_len=index_agent(self.agent_fut_len),
             num_neigh=self.num_agents - 1,
             neigh_types=index_neighbors(self.agent_type),
-            neigh_hist=index_neighbors(self.agent_hist),
+            neigh_hist=StateTensor.from_array(index_neighbors(self.agent_hist), self.agent_hist._format),
             neigh_hist_extents=index_neighbors(self.agent_hist_extent),
             neigh_hist_len=index_neighbors(self.agent_hist_len),
-            neigh_fut=index_neighbors(self.agent_fut),
+            neigh_fut=StateTensor.from_array(index_neighbors(self.agent_fut), self.agent_fut._format),
             neigh_fut_extents=index_neighbors(self.agent_fut_extent),
             neigh_fut_len=index_neighbors(self.agent_fut_len),
             robot_fut=self.robot_fut,
             robot_fut_len=self.robot_fut_len,
-            map_names=index_agent_list(self.map_names),
-            maps=index_agent(self.maps),
-            vector_maps=index_agent(self.vector_maps),
-            maps_resolution=index_agent(self.maps_resolution),
-            rasters_from_world_tf=index_agent(self.rasters_from_world_tf),
+            map_names=self.map_names,
+            maps=self.maps,
+            vector_maps=self.vector_maps,
+            maps_resolution=self.maps_resolution,
+            rasters_from_world_tf=self.rasters_from_world_tf,
             agents_from_world_tf=self.centered_agent_from_world_tf,
             scene_ids=self.scene_ids,
             history_pad_dir=self.history_pad_dir,
             extras=self.extras,
         )
+
+    def apply_transform(self, tf: torch.Tensor, dtype: Optional[torch.dtype] = None) -> SceneBatch:
+        """
+        Applies a transformation matrix to all coordinates stored in the SceneBatch.
+
+        Returns a shallow copy, only coordinate fields are replaced.
+        self.extras will be simply copied over (shallow copy), any custom conversion must be
+        implemented externally.
+        """
+        assert tf.ndim == 3  # b, 3, 3
+        assert tf.shape[-1] == 3 and tf.shape[-1] == 3
+        assert tf.dtype == torch.double  # tf should be double precision, otherwise we have large numerical errors
+        if dtype is None:
+            dtype = self.agent_hist.dtype
+
+        # Shallow copy
+        batch: SceneBatch = replace(self)
+
+        # TODO (pkarkus) support generic format
+        assert batch.agent_hist._format == "x,y,xd,yd,xdd,ydd,s,c"
+        assert batch.agent_fut._format == "x,y,xd,yd,xdd,ydd,s,c"
+        state_class = batch.agent_hist.__class__
+
+        # Transforms
+        batch.agent_hist = state_class(batch_nd_transform_xyvvaahh_pt(batch.agent_hist.double(), tf).type(dtype))
+        batch.agent_fut = state_class(batch_nd_transform_xyvvaahh_pt(batch.agent_fut.double(), tf).type(dtype))
+        batch.rasters_from_world_tf = tf.unsqueeze(1) @ batch.rasters_from_world_tf if batch.rasters_from_world_tf is not None else None
+        batch.centered_agent_from_world_tf = tf @ batch.centered_agent_from_world_tf
+        centered_world_from_agent_tf = torch.linalg.inv(batch.centered_agent_from_world_tf)
+        if batch.lane_xyh is not None:
+            batch.lane_xyh = transform_xyh_torch(batch.lane_xyh.double(), tf).type(dtype)
+        # sanity check
+        assert torch.isclose(batch.centered_world_from_agent_tf @ torch.linalg.inv(tf), centered_world_from_agent_tf, atol=1e-5).all()
+        batch.centered_world_from_agent_tf = centered_world_from_agent_tf
+
+        return batch        
diff --git a/src/trajdata/data_structures/batch_element.py b/src/trajdata/data_structures/batch_element.py
index f18e34d..012f3b8 100644
--- a/src/trajdata/data_structures/batch_element.py
+++ b/src/trajdata/data_structures/batch_element.py
@@ -10,6 +10,11 @@ from trajdata.data_structures.scene import SceneTime, SceneTimeAgent
 from trajdata.data_structures.state import StateArray
 from trajdata.maps import MapAPI, RasterizedMapPatch, VectorMap
 from trajdata.utils.state_utils import convert_to_frame_state, transform_from_frame
+from trajdata.utils.arr_utils import transform_xyh_np, get_close_lanes
+
+from trajdata.utils.map_utils import LaneSegRelation
+
+
 
 
 class AgentBatchElement:
@@ -158,6 +163,20 @@ class AgentBatchElement:
                 else None,
                 **vector_map_params if vector_map_params is not None else None,
             )
+            if vector_map_params.get("calc_lane_graph", False):
+                # not tested
+                ego_xyh = np.concatenate([self.curr_agent_state_np.position, self.curr_agent_state_np.heading])
+                num_pts = vector_map_params.get("num_lane_pts", 30)
+                max_num_lanes = vector_map_params.get("max_num_lanes",20)
+                remove_single_successor = vector_map_params.get("remove_single_successor",False)
+                radius = vector_map_params.get("radius", 100)
+                self.num_lanes,self.lane_xyh,self.lane_adj,self.lane_ids = gen_lane_graph(self.vec_map,ego_xyh,self.agent_from_world_tf,num_pts,max_num_lanes,radius,remove_single_successor)
+                
+            else:
+                self.lane_xyh = None
+                self.lane_adj = None
+                self.lane_ids=list()
+                self.num_lanes = 0
 
         self.scene_id = scene_time_agent.scene.name
 
@@ -404,7 +423,7 @@ class SceneBatchElement:
         nearby_agents, self.agent_types_np = self.get_nearby_agents(
             scene_time, self.centered_agent, distance_limit
         )
-
+        self.agents = nearby_agents
         self.num_agents = len(nearby_agents)
         self.agent_names = [agent.name for agent in nearby_agents]
         (
@@ -440,7 +459,22 @@ class SceneBatchElement:
                 self.cache if self.cache.is_traffic_light_data_cached() else None,
                 **vector_map_params if vector_map_params is not None else None,
             )
-
+            if vector_map_params.get("calc_lane_graph", False):
+                # not tested
+                ego_xyh = np.concatenate([self.centered_agent_state_np.position, self.centered_agent_state_np.heading])
+                num_pts = vector_map_params.get("num_lane_pts", 30)
+                max_num_lanes = vector_map_params.get("max_num_lanes",20)
+                self.num_lanes,self.lane_xyh,self.lane_adj,self.lane_ids = gen_lane_graph(self.vec_map,ego_xyh,self.centered_agent_from_world_tf,num_pts,max_num_lanes)
+                
+            else:
+                self.lane_xyh = None
+                self.lane_adj = None
+                self.lane_ids = list()
+                self.num_lanes = 0
+                    
+                    
+                    
+                
         self.scene_id = scene_time.scene.name
 
         ### ROBOT DATA ###
@@ -585,6 +619,73 @@ class SceneBatchElement:
         ).view(self.cache.obs_type)
         return robot_curr_and_fut_np
 
+        
+def gen_lane_graph(vec_map,ego_xyh,agent_from_world,num_pts=20,max_num_lanes=15,radius=150,remove_single_successor=True):
+    close_lanes,dis = get_close_lanes(radius,ego_xyh,vec_map,num_pts)
+    lanes_by_id = {lane.id:lane for lane in close_lanes}
+    dis_by_id = {lane.id:dis[i] for i,lane in enumerate(close_lanes)}
+    if remove_single_successor:
+        for lane in close_lanes:
+            
+            while len(lane.next_lanes) == 1:
+                # if there are more than one succeeding lanes, then we abort the merging
+                next_id = list(lane.next_lanes)[0]
+                
+                if next_id in lanes_by_id:
+                    next_lane = lanes_by_id[next_id]
+                    shared_next = False
+                    for id in next_lane.prev_lanes:
+                        if id != lane.id and id in lanes_by_id:
+                            shared_next = True
+                            break
+                    if shared_next:
+                        # if the next lane shares two prev lanes in the close_lanes, then we abort the merging
+                        break
+                    lane.combine_next(lanes_by_id[next_id])
+                    lanes_by_id.pop(next_id)
+                else:
+                    break
+        close_lanes = list(lanes_by_id.values())
+        dis = np.array([dis_by_id[lane.id] for lane in close_lanes])
+    num_lanes = len(close_lanes)
+    if num_lanes > max_num_lanes:
+        idx = dis.argsort()[:max_num_lanes]
+        close_lanes = [lane for i,lane in enumerate(close_lanes) if i in idx]
+        num_lanes = max_num_lanes
+    
+    if num_lanes >0:
+        lane_xyh = list()
+        lane_adj = np.zeros([len(close_lanes), len(close_lanes)],dtype=np.int32)
+        lane_ids = [lane.id for lane in close_lanes]
+        
+        for i,lane in enumerate(close_lanes):
+            center = lane.center.interpolate(num_pts).points[:,[0,1,3]]
+            center_local = transform_xyh_np(center, agent_from_world[None])
+            lane_xyh.append(center_local)
+            # construct lane adjacency matrix
+            for adj_lane_id in lane.next_lanes:
+                if adj_lane_id in lane_ids:
+                    lane_adj[i,lane_ids.index(adj_lane_id)] = LaneSegRelation.NEXT.value
+            
+            for adj_lane_id in lane.prev_lanes:
+                if adj_lane_id in lane_ids:
+                    lane_adj[i,lane_ids.index(adj_lane_id)] = LaneSegRelation.PREV.value
+            
+            for adj_lane_id in lane.adj_lanes_left:
+                if adj_lane_id in lane_ids:
+                    lane_adj[i,lane_ids.index(adj_lane_id)] = LaneSegRelation.LEFT.value
+                    
+            for adj_lane_id in lane.adj_lanes_right:
+                if adj_lane_id in lane_ids:
+                    lane_adj[i,lane_ids.index(adj_lane_id)] = LaneSegRelation.RIGHT.value
+        lane_xyh = np.stack(lane_xyh, axis=0)
+        lane_xyh = lane_xyh
+        lane_adj = lane_adj
+    else:
+        lane_xyh = np.zeros([0,num_pts,3])
+        lane_adj = np.zeros([0,0])
+        lane_ids = list()
+    return num_lanes,lane_xyh,lane_adj,lane_ids
 
 def is_agent_stationary(cache: SceneCache, agent_info: AgentMetadata) -> bool:
     # Agent is considered stationary if it moves less than 1m between the first and last valid timestep.
diff --git a/src/trajdata/data_structures/collation.py b/src/trajdata/data_structures/collation.py
index 1b4d2be..2b911d6 100644
--- a/src/trajdata/data_structures/collation.py
+++ b/src/trajdata/data_structures/collation.py
@@ -11,8 +11,9 @@ from torch.nn.utils.rnn import pad_sequence
 from trajdata.augmentation import BatchAugmentation
 from trajdata.data_structures.batch import AgentBatch, SceneBatch
 from trajdata.data_structures.batch_element import AgentBatchElement, SceneBatchElement
+from trajdata.maps import VectorMap, RasterizedMapPatch
+from trajdata.utils.map_utils import batch_transform_raster_maps
 from trajdata.data_structures.state import TORCH_STATE_TYPES
-from trajdata.maps import VectorMap
 from trajdata.utils import arr_utils
 
 
@@ -34,10 +35,26 @@ def _collate_data(elems):
     else:
         return torch.as_tensor(np.stack(elems))
 
+def _collate_lane_graph(elems):
+    num_lanes = [elem.num_lanes for elem in elems]
+    bs = len(elems)
+    M = max(num_lanes)
+    lane_xyh = np.zeros([bs,M,*elems[0].lane_xyh.shape[-2:]])
+    lane_adj = np.zeros([bs,M,M],dtype=int)
+    lane_ids = list()
+    lane_mask = np.zeros([bs,M],dtype=int)
+    for i,elem in enumerate(elems):
+        lane_xyh[i,:num_lanes[i]] = elem.lane_xyh
+        lane_adj[i,:num_lanes[i],:num_lanes[i]] = elem.lane_adj
+        lane_ids.append(elem.lane_ids)
+        lane_mask[i,:num_lanes[i]] = 1
+    return torch.as_tensor(lane_xyh),torch.as_tensor(lane_adj), torch.as_tensor(lane_mask), lane_ids
 
 def raster_map_collate_fn_agent(
     batch_elems: List[AgentBatchElement],
 ):
+    # TODO(pkarkus) refactor with batch_rotate_raster_maps
+
     if batch_elems[0].map_patch is None:
         return None, None, None, None
 
@@ -181,89 +198,48 @@ def raster_map_collate_fn_scene(
     batch_elems: List[SceneBatchElement],
     max_agent_num: Optional[int] = None,
     pad_value: Any = np.nan,
-) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
+) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
 
     if batch_elems[0].map_patches is None:
         return None, None, None, None
 
-    patch_size: int = batch_elems[0].map_patches[0].crop_size
-    assert all(
-        batch_elem.map_patches[0].crop_size == patch_size for batch_elem in batch_elems
-    )
-
+    # Collect map patches for all elements and agents into a flat list
     map_names: List[str] = list()
     num_agents: List[int] = list()
-    agents_rasters_from_world_tfs: List[np.ndarray] = list()
-    agents_patches: List[np.ndarray] = list()
-    agents_rot_angles_list: List[float] = list()
-    agents_res_list: List[float] = list()
+    map_patches: List[RasterizedMapPatch] = list()
 
     for elem in batch_elems:
         map_names.append(elem.map_name)
         num_agents.append(min(elem.num_agents, max_agent_num))
-        agents_rasters_from_world_tfs += [
-            x.raster_from_world_tf for x in elem.map_patches[:max_agent_num]
-        ]
-        agents_patches += [x.data for x in elem.map_patches[:max_agent_num]]
-        agents_rot_angles_list += [
-            x.rot_angle for x in elem.map_patches[:max_agent_num]
-        ]
-        agents_res_list += [x.resolution for x in elem.map_patches[:max_agent_num]]
+        map_patches += elem.map_patches[:max_agent_num]
 
-    patch_data: Tensor = torch.as_tensor(np.stack(agents_patches), dtype=torch.float)
-    agents_rot_angles: Tensor = torch.as_tensor(
-        np.stack(agents_rot_angles_list), dtype=torch.float
-    )
-    agents_rasters_from_world_tf: Tensor = torch.as_tensor(
-        np.stack(agents_rasters_from_world_tfs), dtype=torch.float
-    )
-    agents_resolution: Tensor = torch.as_tensor(
-        np.stack(agents_res_list), dtype=torch.float
+    # Batch transform map patches and pad
+    (
+        rot_crop_patches, 
+        agents_resolution, 
+        agents_rasters_from_world_tf
+    ) = batch_rotate_raster_maps_for_agents_in_scene(
+        map_patches, num_agents, max_agent_num, pad_value
     )
 
-    patch_size_y, patch_size_x = patch_data.shape[-2:]
-    center_y: int = patch_size_y // 2
-    center_x: int = patch_size_x // 2
-    half_extent: int = patch_size // 2
-
-    if torch.count_nonzero(agents_rot_angles) == 0:
-        agents_rasters_from_world_tf = torch.bmm(
-            torch.tensor(
-                [
-                    [
-                        [1.0, 0.0, half_extent],
-                        [0.0, 1.0, half_extent],
-                        [0.0, 0.0, 1.0],
-                    ]
-                ],
-                dtype=agents_rasters_from_world_tf.dtype,
-                device=agents_rasters_from_world_tf.device,
-            ).expand((agents_rasters_from_world_tf.shape[0], -1, -1)),
-            agents_rasters_from_world_tf,
-        )
+    return map_names, rot_crop_patches, agents_resolution, agents_rasters_from_world_tf
 
-        rot_crop_patches = patch_data
-    else:
-        agents_rasters_from_world_tf = torch.bmm(
-            arr_utils.transform_matrices(
-                -agents_rot_angles,
-                torch.tensor([[half_extent, half_extent]]).expand(
-                    (agents_rot_angles.shape[0], -1)
-                ),
-            ),
-            agents_rasters_from_world_tf,
-        )
 
-        # Batch rotating patches by rot_angles.
-        rot_patches: Tensor = rotate(patch_data, torch.rad2deg(agents_rot_angles))
+def batch_rotate_raster_maps_for_agents_in_scene(
+    map_patches: List[RasterizedMapPatch],
+    num_agents: List[int],
+    max_agent_num: Optional[int] = None,
+    pad_value: Any = np.nan,
+) -> Tuple[Tensor, Tensor, Tensor]:
 
-        # Center cropping via slicing.
-        rot_crop_patches = rot_patches[
-            ...,
-            center_y - half_extent : center_y + half_extent,
-            center_x - half_extent : center_x + half_extent,
-        ]
+    # Batch transform map patches
+    (
+        rot_crop_patches, 
+        agents_resolution, 
+        agents_rasters_from_world_tf
+    ) = batch_transform_raster_maps(map_patches)
 
+    # Separate batch and agents
     rot_crop_patches = split_pad_crop(
         rot_crop_patches, num_agents, pad_value=pad_value, desired_size=max_agent_num
     )
@@ -278,7 +254,7 @@ def raster_map_collate_fn_scene(
         agents_resolution, num_agents, pad_value=0, desired_size=max_agent_num
     )
 
-    return map_names, rot_crop_patches, agents_resolution, agents_rasters_from_world_tf
+    return rot_crop_patches, agents_resolution, agents_rasters_from_world_tf
 
 
 def agent_collate_fn(
@@ -673,6 +649,10 @@ def agent_collate_fn(
     vector_maps: Optional[List[VectorMap]] = None
     if batch_elems[0].vec_map is not None:
         vector_maps = [batch_elem.vec_map for batch_elem in batch_elems]
+    
+    lane_xyh,lane_adj,lane_mask,lane_ids = None,None,None,None
+    if hasattr(batch_elems[0],"lane_xyh") and batch_elems[0].lane_xyh is not None:
+        lane_xyh,lane_adj,lane_mask, lane_ids = _collate_lane_graph(batch_elems)
 
     agents_from_world_tf = torch.as_tensor(
         np.stack([batch_elem.agent_from_world_tf for batch_elem in batch_elems]),
@@ -712,6 +692,10 @@ def agent_collate_fn(
         robot_fut_len=robot_future_len,
         map_names=map_names,
         maps=map_patches,
+        lane_xyh=lane_xyh,
+        lane_adj=lane_adj,
+        lane_mask=lane_mask,
+        lane_ids = lane_ids,
         maps_resolution=maps_resolution,
         vector_maps=vector_maps,
         rasters_from_world_tf=rasters_from_world_tf,
@@ -781,6 +765,9 @@ def scene_collate_fn(
     return_dict: bool,
     pad_format: str,
     batch_augments: Optional[List[BatchAugmentation]] = None,
+    desired_num_agents = None,
+    desired_hist_len=None,
+    desired_fut_len=None,
 ) -> SceneBatch:
     batch_size: int = len(batch_elems)
     history_pad_dir: arr_utils.PadDirection = (
@@ -800,6 +787,9 @@ def scene_collate_fn(
     AgentObsTensor = TORCH_STATE_TYPES[obs_format]
 
     max_agent_num: int = max(elem.num_agents for elem in batch_elems)
+    if desired_num_agents is not None:
+        max_agent_num = max(max_agent_num,desired_num_agents)
+
 
     centered_agent_state: List[AgentStateTensor] = list()
     agents_types: List[Tensor] = list()
@@ -820,6 +810,10 @@ def scene_collate_fn(
 
     max_history_len: int = max(elem.agent_history_lens_np.max() for elem in batch_elems)
     max_future_len: int = max(elem.agent_future_lens_np.max() for elem in batch_elems)
+    if desired_hist_len is not None:
+        max_history_len = max(max_history_len,desired_hist_len)
+    if desired_fut_len is not None:
+        max_future_len = max(max_future_len,desired_fut_len)
 
     robot_future: List[AgentObsTensor] = list()
     robot_future_len: Tensor = torch.zeros((batch_size,), dtype=torch.long)
@@ -951,6 +945,10 @@ def scene_collate_fn(
     if batch_elems[0].vec_map is not None:
         vector_maps = [batch_elem.vec_map for batch_elem in batch_elems]
 
+    lane_xyh,lane_adj,lane_mask, lane_ids = None,None,None,None
+    if hasattr(batch_elems[0],"lane_xyh") and batch_elems[0].lane_xyh is not None:
+        lane_xyh,lane_adj,lane_mask, lane_ids = _collate_lane_graph(batch_elems)
+        
     centered_agent_from_world_tf = torch.as_tensor(
         np.stack(
             [batch_elem.centered_agent_from_world_tf for batch_elem in batch_elems]
@@ -990,6 +988,7 @@ def scene_collate_fn(
         agent_type=agents_types_t,
         centered_agent_state=centered_agent_state_t,
         agent_names=agent_names,
+        track_ids = None,
         agent_hist=agents_histories_t,
         agent_hist_extent=agents_history_extents_t,
         agent_hist_len=agents_history_len,
@@ -1000,6 +999,10 @@ def scene_collate_fn(
         robot_fut_len=robot_future_len,
         map_names=map_names,
         maps=map_patches,
+        lane_xyh = lane_xyh,
+        lane_adj = lane_adj,
+        lane_mask = lane_mask,
+        lane_ids = lane_ids,
         maps_resolution=maps_resolution,
         vector_maps=vector_maps,
         rasters_from_world_tf=rasters_from_world_tf,
diff --git a/src/trajdata/dataset.py b/src/trajdata/dataset.py
index 64f80eb..609e25e 100644
--- a/src/trajdata/dataset.py
+++ b/src/trajdata/dataset.py
@@ -1,4 +1,5 @@
 import gc
+import json
 import random
 import time
 from collections import defaultdict
@@ -49,7 +50,7 @@ from trajdata.data_structures import (
 from trajdata.dataset_specific import RawDataset
 from trajdata.maps.map_api import MapAPI
 from trajdata.parallel import ParallelDatasetPreprocessor, scene_paths_collate_fn
-from trajdata.utils import agent_utils, env_utils, scene_utils, string_utils
+from trajdata.utils import agent_utils, env_utils, py_utils, scene_utils, string_utils
 from trajdata.utils.parallel_utils import parallel_iapply
 
 # TODO(bivanovic): Move this to a better place in the codebase.
@@ -111,6 +112,7 @@ class UnifiedDataset(Dataset):
         cache_location: str = "~/.unified_data_cache",
         rebuild_cache: bool = False,
         rebuild_maps: bool = False,
+        save_index: bool = False,
         num_workers: int = 0,
         verbose: bool = False,
         extras: Dict[str, Callable[..., np.ndarray]] = dict(),
@@ -152,12 +154,17 @@ class UnifiedDataset(Dataset):
             cache_location (str, optional): Where to store and load preprocessed, cached data. Defaults to "~/.unified_data_cache".
             rebuild_cache (bool, optional): If True, process and cache trajectory data even if it is already cached. Defaults to False.
             rebuild_maps (bool, optional): If True, process and cache maps even if they are already cached. Defaults to False.
+            save_index (bool, optional): If True, save the resulting agent (or scene) data index after it is computed (speeding up subsequent initializations with the same argument values).
             num_workers (int, optional): Number of parallel workers to use for dataset preprocessing and loading. Defaults to 0.
             verbose (bool, optional):  If True, print internal data loading information. Defaults to False.
             extras (Dict[str, Callable[..., np.ndarray]], optional): Adds extra data to each batch element. Each Callable must take as input a filled {Agent,Scene}BatchElement and return an ndarray which will subsequently be added to the batch element's `extra` dict.
             transforms (Iterable[Callable], optional): Allows for custom modifications of batch elements. Each Callable must take in a filled {Agent,Scene}BatchElement and return a {Agent,Scene}BatchElement.
             rank (int, optional): Proccess rank when using torch DistributedDataParallel for multi-GPU training. Only the rank 0 process will be used for caching.
         """
+        self.desired_data: List[str] = desired_data
+        self.scene_description_contains: Optional[
+            List[str]
+        ] = scene_description_contains
         self.centric: str = centric
         self.desired_dt: float = desired_dt
 
@@ -204,6 +211,11 @@ class UnifiedDataset(Dataset):
                 # Collation can be quite slow if vector maps are included,
                 # so we do not unless the user requests it.
                 "collate": False,
+                # Whether loaded maps should be stored in memory (memoized) for later re-use.
+                # For datasets which provide full maps ahead-of-time (i.e., all except Waymo),
+                # this should be True. However, for Waymo it should be False because maps
+                # are already partitioned geographically and keeping them around significantly grows memory.
+                "keep_in_memory": True,
             }
         )
         if self.desired_dt is not None:
@@ -233,13 +245,15 @@ class UnifiedDataset(Dataset):
         self.torch_obs_type = TORCH_STATE_TYPES[obs_format]
 
         # Ensuring scene description queries are all lowercase
-        if scene_description_contains is not None:
-            scene_description_contains = [s.lower() for s in scene_description_contains]
+        if self.scene_description_contains is not None:
+            self.scene_description_contains = [
+                s.lower() for s in self.scene_description_contains
+            ]
 
         self.envs: List[RawDataset] = env_utils.get_raw_datasets(data_dirs)
         self.envs_dict: Dict[str, RawDataset] = {env.name: env for env in self.envs}
 
-        matching_datasets: List[SceneTag] = self.get_matching_scene_tags(desired_data)
+        matching_datasets: List[SceneTag] = self._get_matching_scene_tags(desired_data)
         if self.verbose:
             print(
                 "Loading data for matched scene tags:",
@@ -249,7 +263,7 @@ class UnifiedDataset(Dataset):
 
         self._map_api: Optional[MapAPI] = None
         if self.incl_vector_map:
-            self._map_api = MapAPI(self.cache_path)
+            self._map_api = MapAPI(self.cache_path, keep_in_memory=vector_map_params.get("keep_in_memory", True))
 
         all_scenes_list: Union[List[SceneMetadata], List[Scene]] = list()
         for env in self.envs:
@@ -257,8 +271,8 @@ class UnifiedDataset(Dataset):
                 all_data_cached: bool = False
                 all_maps_cached: bool = not env.has_maps or not require_map_cache
                 if self.env_cache.env_is_cached(env.name) and not self.rebuild_cache:
-                    scenes_list: List[Scene] = self.get_desired_scenes_from_env(
-                        matching_datasets, scene_description_contains, env
+                    scenes_list: List[Scene] = self._get_desired_scenes_from_env(
+                        matching_datasets, env
                     )
 
                     all_data_cached: bool = all(
@@ -314,9 +328,9 @@ class UnifiedDataset(Dataset):
                         ):
                             distributed.barrier()
 
-                    scenes_list: List[SceneMetadata] = self.get_desired_scenes_from_env(
-                        matching_datasets, scene_description_contains, env
-                    )
+                    scenes_list: List[
+                        SceneMetadata
+                    ] = self._get_desired_scenes_from_env(matching_datasets, env)
 
                 if self.incl_vector_map and env.metadata.map_locations is not None:
                     # env.metadata.map_locations can be none for map-containing
@@ -330,7 +344,7 @@ class UnifiedDataset(Dataset):
                 all_scenes_list += scenes_list
 
         # List of cached scene paths.
-        scene_paths: List[Path] = self.preprocess_scene_data(
+        scene_paths: List[Path] = self._preprocess_scene_data(
             all_scenes_list, num_workers
         )
         if self.verbose:
@@ -343,7 +357,11 @@ class UnifiedDataset(Dataset):
         data_index: Union[
             List[Tuple[str, int, np.ndarray]],
             List[Tuple[str, int, List[Tuple[str, np.ndarray]]]],
-        ] = self.get_data_index(num_workers, scene_paths)
+        ]
+        if self._index_cache_path().exists():
+            data_index = self._load_data_index()
+        else:
+            data_index = self._get_data_index(num_workers, scene_paths)
 
         # Done with this list. Cutting memory usage because
         # of multiprocessing later on.
@@ -360,8 +378,99 @@ class UnifiedDataset(Dataset):
         )
         self._data_len: int = len(self._data_index)
 
+        # Use only rank 0 process for caching when using multi-GPU torch training.
+        if save_index and rank == 0:
+            if self._index_cache_path().exists():
+                print(
+                    "WARNING: Overwriting already-cached data index (since save_index is True).",
+                    flush=True,
+                )
+
+            self._cache_data_index(data_index)
+        if (
+            distributed.is_initialized()
+            and distributed.get_world_size() > 1
+        ):
+            distributed.barrier()
         self._cached_batch_elements = None
 
+    def _index_cache_path(
+        self, ret_args: bool = False
+    ) -> Union[Path, Tuple[Path, Dict[str, Any]]]:
+        # Whichever UnifiedDataset arguments affect data indexing are captured
+        # and hashed together here.
+        impactful_args: Dict[str, Any] = {
+            "desired_data": tuple(self.desired_data),
+            "scene_description_contains": tuple(self.scene_description_contains)
+            if self.scene_description_contains is not None
+            else None,
+            "centric": self.centric,
+            "desired_dt": self.desired_dt,
+            "history_sec": self.history_sec,
+            "future_sec": self.future_sec,
+            "incl_robot_future": self.incl_robot_future,
+            "only_types": tuple(t.name for t in self.only_types)
+            if self.only_types is not None
+            else None,
+            "only_predict": tuple(t.name for t in self.only_predict)
+            if self.only_predict is not None
+            else None,
+            "no_types": tuple(t.name for t in self.no_types)
+            if self.no_types is not None
+            else None,
+            "ego_only": self.ego_only,
+        }
+        index_hash: str = py_utils.hash_dict(impactful_args)
+        index_cache_path: Path = self.cache_path / "data_indexes" / index_hash
+
+        if ret_args:
+            return index_cache_path, impactful_args
+        else:
+            return index_cache_path
+
+    def _cache_data_index(
+        self,
+        data_index: Union[
+            List[Tuple[str, int, np.ndarray]],
+            List[Tuple[str, int, List[Tuple[str, np.ndarray]]]],
+        ],
+    ) -> None:
+        index_cache_dir, index_args = self._index_cache_path(ret_args=True)
+
+        # Create it if it doesn't exist yet.
+        index_cache_dir.mkdir(parents=True, exist_ok=True)
+
+        index_cache_file: Path = index_cache_dir / "data_index.dill"
+        with open(index_cache_file, "wb") as f:
+            dill.dump(data_index, f)
+
+        args_file: Path = index_cache_dir / "index_args.json"
+        with open(args_file, "w") as f:
+            json.dump(index_args, f, indent=4)
+
+        print(
+            f"Cached data index to {str(index_cache_file)}",
+            flush=True,
+        )
+
+    def _load_data_index(
+        self,
+    ) -> Union[
+        List[Tuple[str, int, np.ndarray]],
+        List[Tuple[str, int, List[Tuple[str, np.ndarray]]]],
+    ]:
+        index_cache_file: Path = self._index_cache_path() / "data_index.dill"
+        with open(index_cache_file, "rb") as f:
+            data_index = dill.load(f)
+
+        if self.verbose:
+            print(
+                f"Loaded data index from {str(index_cache_file)}",
+                flush=True,
+            )
+
+        return data_index
+
     def load_or_create_cache(
         self, cache_path: str, num_workers=0, filter_fn=None
     ) -> None:
@@ -494,7 +603,7 @@ class UnifiedDataset(Dataset):
             f"Kept {self._data_len}/{old_len} elements, {self._data_len/old_len*100.0:.2f}%."
         )
 
-    def get_data_index(
+    def _get_data_index(
         self, num_workers: int, scene_paths: List[Path]
     ) -> Union[
         List[Tuple[str, int, np.ndarray]],
@@ -681,13 +790,16 @@ class UnifiedDataset(Dataset):
                 return_dict=return_dict,
                 pad_format=pad_format,
                 batch_augments=batch_augments,
+                desired_num_agents = self.max_agent_num,
+                desired_hist_len = int(self.history_sec[1]/self.desired_dt),
+                desired_fut_len = int(self.future_sec[1]/self.desired_dt),
             )
         else:
             raise ValueError(f"{self.centric}-centric data batches are not supported.")
 
         return collate_fn
 
-    def get_matching_scene_tags(self, queries: List[str]) -> List[SceneTag]:
+    def _get_matching_scene_tags(self, queries: List[str]) -> List[SceneTag]:
         # if queries is None:
         #     return list(chain.from_iterable(env.components for env in self.envs))
 
@@ -708,10 +820,9 @@ class UnifiedDataset(Dataset):
 
         return matching_scene_tags
 
-    def get_desired_scenes_from_env(
+    def _get_desired_scenes_from_env(
         self,
         scene_tags: List[SceneTag],
-        scene_description_contains: Optional[List[str]],
         env: RawDataset,
     ) -> Union[List[Scene], List[SceneMetadata]]:
         scenes_list: Union[List[Scene], List[SceneMetadata]] = list()
@@ -721,14 +832,14 @@ class UnifiedDataset(Dataset):
             if env.name in scene_tag:
                 scenes_list += env.get_matching_scenes(
                     scene_tag,
-                    scene_description_contains,
+                    self.scene_description_contains,
                     self.env_cache,
                     self.rebuild_cache,
                 )
 
         return scenes_list
 
-    def preprocess_scene_data(
+    def _preprocess_scene_data(
         self,
         scenes_list: Union[List[SceneMetadata], List[Scene]],
         num_workers: int,
diff --git a/src/trajdata/dataset_specific/carla/__init__.py b/src/trajdata/dataset_specific/carla/__init__.py
new file mode 100644
index 0000000..8854e8a
--- /dev/null
+++ b/src/trajdata/dataset_specific/carla/__init__.py
@@ -0,0 +1 @@
+from .carla_dataset import CarlaDataset
\ No newline at end of file
diff --git a/src/trajdata/dataset_specific/carla/carla_dataset.py b/src/trajdata/dataset_specific/carla/carla_dataset.py
new file mode 100644
index 0000000..2692e26
--- /dev/null
+++ b/src/trajdata/dataset_specific/carla/carla_dataset.py
@@ -0,0 +1,415 @@
+import warnings
+from copy import deepcopy
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
+
+import pandas as pd
+from nuscenes.eval.prediction.splits import NUM_IN_TRAIN_VAL
+from nuscenes.map_expansion.map_api import NuScenesMap, locations
+from nuscenes.nuscenes import NuScenes
+from nuscenes.utils.splits import create_splits_scenes
+from tqdm import tqdm
+
+from trajdata.caching import EnvCache, SceneCache
+from trajdata.data_structures.agent import (
+    Agent,
+    AgentMetadata,
+    AgentType,
+    FixedExtent,
+    VariableExtent,
+)
+from trajdata.data_structures.environment import EnvMetadata
+from trajdata.data_structures.scene_metadata import Scene, SceneMetadata
+from trajdata.data_structures.scene_tag import SceneTag
+from trajdata.dataset_specific.nusc import nusc_utils
+from trajdata.dataset_specific.raw_dataset import RawDataset
+from trajdata.dataset_specific.scene_records import CarlaSceneRecord
+from trajdata.maps import VectorMap
+from trajdata.maps.map_api import MapAPI
+
+from pdb import set_trace as st
+import glob, os
+import pickle
+from collections import defaultdict
+import re
+
+import torch
+import numpy as np
+
+carla_to_trajdata_object_type = {
+    0: AgentType.VEHICLE,
+    1: AgentType.BICYCLE,  # Motorcycle
+    2: AgentType.BICYCLE,
+    3: AgentType.PEDESTRIAN,
+    4: AgentType.UNKNOWN,
+    # ?: AgentType.STATIC,
+}
+
+def create_splits_scenes(data_dir:str) -> Dict[str, List[str]]:
+    all_scenes = {}
+    all_scenes['train'] = [scene_path.split('/')[-1] for scene_path in glob.glob(data_dir+'/train/route*')]
+    all_scenes['val'] = [scene_path.split('/')[-1] for scene_path in glob.glob(data_dir+'/val/route*')]
+    return all_scenes
+
+# TODO: (Yulong) format in object class
+def tracklet_to_pred(tracklet_mem,ego=False):
+    if ego:
+        x, y, z = np.split(tracklet_mem['location'],3,axis=-1)
+        hx, hy, hz = np.split(np.deg2rad(tracklet_mem['rotation']),3,axis=-1)
+        vx, vy, _ = np.split(tracklet_mem['velocity'],3,axis=-1)
+        ax, ay, _ = np.split(tracklet_mem['acceleration'],3,axis=-1)
+    else:
+        x, y, z = np.split(tracklet_mem['location'][0,:,-1,:],3,axis=-1)
+        hx, hy, hz = np.split(np.deg2rad(tracklet_mem['rotation'])[0,:,-1,:],3,axis=-1)
+        vx, vy, _ = np.split(tracklet_mem['velocity'][0,:,-1,:],3,axis=-1)
+        ax, ay, _ = np.split(tracklet_mem['acceleration'][0,:,-1,:],3,axis=-1)
+    pred_state = np.concatenate([
+        x, -y, z, vx, -vy, ax, -ay, -hy
+    ],axis=-1)
+    return pred_state
+
+def CarlaTracking(dataroot):
+    dataset_obj = defaultdict(lambda: defaultdict(dict))
+    frames = list(dataroot.glob('*/route*/metadata/tracking/*.pkl'))
+    for frame in frames:
+        frame = str(frame)
+        with open(frame, 'rb') as f:
+            track_mem = pickle.load(f)
+        frame_idx = frame.split('/')[-1].split('.')[0]
+        scene = frame.split('/')[-4]
+        dataset_obj[scene]["all"][frame_idx] = track_mem
+
+    frames = list(dataroot.glob('*/route*/metadata/ego/*.pkl'))
+    for frame in frames:
+        frame = str(frame)
+        with open(frame, 'rb') as f:
+            track_mem = pickle.load(f)
+        frame_idx = frame.split('/')[-1].split('.')[0]
+        scene = frame.split('/')[-4]
+        dataset_obj[scene]["ego"][frame_idx] = track_mem
+    
+    return dataset_obj
+
+def agg_ego_data(all_frames):
+    agent_data = []
+    agent_frame = []
+    for frame_idx, frame_info in enumerate(all_frames):
+        pred_state = tracklet_to_pred(frame_info, ego=True)
+        agent_data.append(pred_state)
+        agent_frame.append(frame_idx)
+
+    agent_data_df = pd.DataFrame(
+        agent_data,
+        columns=["x", "y", "z", "vx", "vy", "ax", "ay", "heading"],
+        index=pd.MultiIndex.from_tuples(
+            [
+                ("ego", idx)
+                for idx in agent_frame
+            ],
+            names=["agent_id", "scene_ts"],
+        ),
+    )
+
+    agent_metadata = AgentMetadata(
+        name="ego",
+        agent_type=AgentType.VEHICLE,
+        first_timestep=agent_frame[0],
+        last_timestep=agent_frame[-1],
+        extent=FixedExtent(
+            length=all_frames[-1]["size"][1], width=all_frames[-1]["size"][0], height=all_frames[-1]["size"][2]
+        ),
+    )
+    return Agent(
+        metadata=agent_metadata,
+        data=agent_data_df,
+    )
+
+def agg_agent_data(all_frames, agent_info, frame_idx):
+    agent_data = []
+    agent_frame = []
+    Agent_list = []
+    for frame_idx, frame_info in enumerate(all_frames):
+        for idx in range(frame_info['id'].shape[1]):
+            if frame_info["id"][0,idx,0] == agent_info["id"]:
+                # two segments of tracking
+                if len(agent_frame) > 0 and frame_idx != agent_frame[-1] + 1:
+                    agent_data_df = pd.DataFrame(
+                        agent_data,
+                        columns=["x", "y", "z", "vx", "vy", "ax", "ay", "heading"],
+                        index=pd.MultiIndex.from_tuples(
+                            [
+                                (f'{int(agent_info["id"])}_{len(Agent_list)}', idx)
+                                for idx in agent_frame
+                            ],
+                            names=["agent_id", "scene_ts"],
+                        ),
+                    )
+
+                    agent_metadata = AgentMetadata(
+                        name=f'{int(agent_info["id"])}_{len(Agent_list)}',
+                        agent_type=carla_to_trajdata_object_type[agent_info["cls"]],
+                        first_timestep=agent_frame[0],
+                        last_timestep=agent_frame[-1],
+                        extent=FixedExtent(
+                            length=agent_info["size"][1], width=agent_info["size"][0], height=agent_info["size"][2]
+                        ),
+                    )
+                    Agent_list.append(Agent(
+                        metadata=agent_metadata,
+                        data=agent_data_df,
+                    ))
+
+                    agent_data = []
+                    agent_frame = []
+
+
+                pred_state = tracklet_to_pred(frame_info)[idx]
+                agent_data.append(pred_state)
+                agent_frame.append(frame_idx)
+
+    agent_data_df = pd.DataFrame(
+        agent_data,
+        columns=["x", "y", "z", "vx", "vy", "ax", "ay", "heading"],
+        index=pd.MultiIndex.from_tuples(
+            [
+                (str(int(agent_info["id"])), idx)
+                for idx in agent_frame
+            ],
+            names=["agent_id", "scene_ts"],
+        ),
+    )
+
+    agent_metadata = AgentMetadata(
+        name=str(int(agent_info["id"])),
+        agent_type=carla_to_trajdata_object_type[agent_info["cls"]],
+        first_timestep=agent_frame[0],
+        last_timestep=agent_frame[-1],
+        extent=FixedExtent(
+            length=agent_info["size"][1], width=agent_info["size"][0], height=agent_info["size"][2]
+        ),
+    )
+    
+    Agent_list.append(Agent(
+                            metadata=agent_metadata,
+                            data=agent_data_df,
+                        ))
+    return Agent_list             
+
+
+
+class CarlaDataset(RawDataset):
+    def compute_metadata(self, env_name: str, data_dir: str) -> EnvMetadata:
+        # We're using the nuScenes prediction challenge split here.
+        # See https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/prediction/splits.py
+        # for full details on how the splits are obtained below.
+        all_scene_splits: Dict[str, List[str]] = create_splits_scenes(data_dir)
+
+        train_scenes: List[str] = deepcopy(all_scene_splits["train"])
+        NUM_IN_TRAIN_VAL = round(len(train_scenes)*0.25)
+        all_scene_splits["train"] = train_scenes[NUM_IN_TRAIN_VAL:]
+        all_scene_splits["train_val"] = train_scenes[:NUM_IN_TRAIN_VAL]
+
+        if env_name == 'carla':
+            carla_scene_splits: Dict[str, List[str]] = {
+                k: all_scene_splits[k] for k in ["train", "train_val", "val"]
+            }
+
+            # nuScenes possibilities are the Cartesian product of these
+            dataset_parts: List[Tuple[str, ...]] = [
+                ("train", "train_val", "val"),
+            ]
+        else:
+            raise ValueError(f"Unknown nuScenes environment name: {env_name}")
+
+        self.scene_splits = carla_scene_splits
+
+        # Inverting the dict from above, associating every scene with its data split.
+        carla_scene_split_map: Dict[str, str] = {
+            v_elem: k for k, v in carla_scene_splits.items() for v_elem in v
+        }
+        return EnvMetadata(
+            name=env_name,
+            data_dir=data_dir,
+            dt=nusc_utils.NUSC_DT,
+            parts=dataset_parts,
+            scene_split_map=carla_scene_split_map,
+            # The location names should match the map names used in
+            # the unified data cache.
+            map_locations=tuple([]),
+        )
+
+    def load_dataset_obj(self, verbose: bool = False) -> None:
+        if verbose:
+            print(f"Loading {self.name} dataset...", flush=True)
+
+        self.dataset_obj = CarlaTracking(
+            dataroot=self.metadata.data_dir
+        )
+
+    def _get_matching_scenes_from_obj(
+        self,
+        scene_tag: SceneTag,
+        scene_desc_contains: Optional[List[str]],
+        env_cache: EnvCache,
+    ) -> List[SceneMetadata]:
+        all_scenes_list: List[CarlaSceneRecord] = list()
+
+        scenes_list: List[SceneMetadata] = list()
+        for idx, scene_record in enumerate(self.dataset_obj):
+            scene_name: str = scene_record
+            scene_location: str = re.match('.*(Town\d+)', scene_record)[1]
+            scene_split: str = self.metadata.scene_split_map[scene_name]
+            scene_length: int = len(self.dataset_obj[scene_record])
+
+            # Saving all scene records for later caching.
+            all_scenes_list.append(
+                CarlaSceneRecord(
+                    scene_name, scene_location, scene_length, idx
+                )
+            )
+
+            if scene_split in scene_tag:
+
+                scene_metadata = SceneMetadata(
+                    env_name=self.metadata.name,
+                    name=scene_name,
+                    dt=self.metadata.dt,
+                    raw_data_idx=idx,
+                )
+                scenes_list.append(scene_metadata)
+
+        self.cache_all_scenes_list(env_cache, all_scenes_list)
+        return scenes_list
+
+    def _get_matching_scenes_from_cache(
+        self,
+        scene_tag: SceneTag,
+        scene_desc_contains: Optional[List[str]],
+        env_cache: EnvCache,
+    ) -> List[Scene]:
+        all_scenes_list: List[CarlaSceneRecord] = env_cache.load_env_scenes_list(
+            self.name
+        )
+
+        scenes_list: List[SceneMetadata] = list()
+        for scene_record in all_scenes_list:
+            (
+                scene_name,
+                scene_location,
+                scene_length,
+                data_idx,
+            ) = scene_record
+            scene_split: str = self.metadata.scene_split_map[scene_name]
+
+            if scene_split in scene_tag:
+                scene_metadata = Scene(
+                    self.metadata,
+                    scene_name,
+                    scene_location,
+                    scene_split,
+                    scene_length,
+                    data_idx,
+                    None,  # This isn't used if everything is already cached.
+                )
+                scenes_list.append(scene_metadata)
+
+        return scenes_list
+
+    def get_scene(self, scene_info: SceneMetadata) -> Scene:
+        _, route_name, _, data_idx = scene_info
+
+        scene_record = sorted(self.dataset_obj[route_name])
+        scene_name: str = route_name
+        scene_location: str = re.match('.*(Town\d+)', route_name)[1]
+        scene_split: str = self.metadata.scene_split_map[scene_name]
+        scene_length: int = len(self.dataset_obj[route_name]['ego'])
+
+        return Scene(
+            self.metadata,
+            scene_name,
+            scene_location,
+            scene_split,
+            scene_length,
+            data_idx,
+            scene_record,
+        )
+
+    def get_agent_info(
+        self, scene: Scene, cache_path: Path, cache_class: Type[SceneCache]
+    ) -> Tuple[List[AgentMetadata], List[List[AgentMetadata]]]:
+        ego_agent_info: AgentMetadata = AgentMetadata(
+            name="ego",
+            agent_type=AgentType.VEHICLE,
+            first_timestep=0,
+            last_timestep=scene.length_timesteps - 1,
+            extent=FixedExtent(length=4.084, width=1.730, height=1.562), #TODO: (yulong) replace with carla ego
+        )
+
+        agent_presence: List[List[AgentMetadata]] = [
+            [ego_agent_info] for _ in range(scene.length_timesteps)
+        ]
+
+        agent_data_list: List[pd.DataFrame] = list()
+        existing_agents: Dict[str, AgentMetadata] = dict()
+
+        all_frames = [self.dataset_obj[scene.name]["all"][key] for key in sorted(self.dataset_obj[scene.name]["all"])]
+        
+        # frame_idx_dict = {
+        #     frame_dict: idx for idx, frame_dict in enumerate(all_frames)
+        # }
+        for frame_idx, frame_info in enumerate(all_frames):
+            for idx in range(frame_info['id'].shape[1]):
+                if str(int(frame_info["id"][0,idx,0])) in existing_agents:
+                    continue
+
+                agent_info = {"id": frame_info["id"][0,idx,0],
+                              "cls": frame_info["cls"][0,idx,:].argmax(),
+                               "size": frame_info["size"][0,idx,:] }
+                # if not agent_info["next"]:
+                #     # There are some agents with only a single detection to them, we don't care about these.
+                #     continue
+
+                agent_list: List[Agent] = agg_agent_data(
+                    all_frames, agent_info, frame_idx
+                )
+                for agent in agent_list:
+                    for scene_ts in range(
+                        agent.metadata.first_timestep, agent.metadata.last_timestep + 1
+                    ):
+                        agent_presence[scene_ts].append(agent.metadata)
+
+                    existing_agents[agent.name] = agent.metadata
+
+                    agent_data_list.append(agent.data)
+
+        ego_all_frames = [self.dataset_obj[scene.name]["ego"][key] for key in sorted(self.dataset_obj[scene.name]["ego"])]
+
+        ego_agent: Agent = agg_ego_data(ego_all_frames)
+        agent_data_list.append(ego_agent.data)
+
+        agent_list: List[AgentMetadata] = [ego_agent_info] + list(
+            existing_agents.values()
+        )
+
+        cache_class.save_agent_data(pd.concat(agent_data_list), cache_path, scene)
+
+        return agent_list, agent_presence
+
+
+    def cache_maps(
+        self,
+        cache_path: Path,
+        map_cache_class: Type[SceneCache],
+        map_params: Dict[str, Any],
+    ) -> None:
+        
+        map_api = MapAPI(cache_path)
+        for carla_town in [f"Town0{x}" for x in range(1, 8)] + ["Town10", "Town10HD"]: # ["main"]:
+            vec_map = map_api.get_map(
+                f"drivesim:main" if carla_town == "main" else f"carla:{carla_town}",
+                incl_road_lanes=True,
+                incl_road_areas=True,
+                incl_ped_crosswalks=True,
+                incl_ped_walkways=True,
+            )
+            map_cache_class.finalize_and_cache_map(cache_path, vec_map, map_params)
diff --git a/src/trajdata/dataset_specific/drivesim/__init__.py b/src/trajdata/dataset_specific/drivesim/__init__.py
new file mode 100644
index 0000000..0167f39
--- /dev/null
+++ b/src/trajdata/dataset_specific/drivesim/__init__.py
@@ -0,0 +1 @@
+from .drivesim_dataset import DrivesimDataset
diff --git a/src/trajdata/dataset_specific/drivesim/drivesim_dataset.py b/src/trajdata/dataset_specific/drivesim/drivesim_dataset.py
new file mode 100644
index 0000000..865e451
--- /dev/null
+++ b/src/trajdata/dataset_specific/drivesim/drivesim_dataset.py
@@ -0,0 +1,116 @@
+import warnings
+from copy import deepcopy
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
+from collections import defaultdict
+
+import pandas as pd
+
+from tqdm import tqdm
+
+from trajdata.caching import EnvCache, SceneCache
+from trajdata.data_structures.agent import (
+    Agent,
+    AgentMetadata,
+    AgentType,
+    FixedExtent,
+    VariableExtent,
+)
+from trajdata.data_structures.environment import EnvMetadata
+from trajdata.data_structures.scene_metadata import Scene, SceneMetadata
+from trajdata.data_structures.scene_tag import SceneTag
+from trajdata.dataset_specific.raw_dataset import RawDataset
+from trajdata.dataset_specific.scene_records import DrivesimSceneRecord
+from trajdata.maps import VectorMap
+
+DRIVESIM_DT = 0.1
+
+class DrivesimDataset(RawDataset):
+    def compute_metadata(self, env_name: str, data_dir: str) -> EnvMetadata:
+
+        return EnvMetadata(
+            name="drivesim",
+            data_dir=data_dir,
+            dt=DRIVESIM_DT,
+            parts=[("train",),("main",)],
+            scene_split_map=defaultdict(lambda: "train"),
+            # The location names should match the map names used in
+            # the unified data cache.
+            map_locations=("main",),
+        )
+
+    def load_dataset_obj(self, verbose: bool = False) -> None:
+        pass
+
+    def _get_matching_scenes_from_obj(
+        self,
+        scene_tag: SceneTag,
+        scene_desc_contains: Optional[List[str]],
+        env_cache: EnvCache,
+    ) -> List[SceneMetadata]:
+        raise NotImplementedError()
+
+    def _get_matching_scenes_from_cache(
+        self,
+        scene_tag: SceneTag,
+        scene_desc_contains: Optional[List[str]],
+        env_cache: EnvCache,
+    ) -> List[Scene]:
+        all_scenes_list: List[DrivesimSceneRecord] = env_cache.load_env_scenes_list(
+            self.name
+        )
+        scenes_list: List[SceneMetadata] = list()
+        for scene_record in all_scenes_list:
+            (
+                scene_name,
+                scene_location,
+                scene_length,
+                scene_desc,
+                data_idx,
+            ) = scene_record
+            scene_split: str = self.metadata.scene_split_map[scene_name]
+
+            if scene_location.split("-")[0] in scene_tag and scene_split in scene_tag:
+                if scene_desc_contains is not None and not any(
+                    desc_query in scene_desc for desc_query in scene_desc_contains
+                ):
+                    continue
+
+                scene_metadata = Scene(
+                    self.metadata,
+                    scene_name,
+                    scene_location,
+                    scene_split,
+                    scene_length,
+                    data_idx,
+                    None,  # This isn't used if everything is already cached.
+                    scene_desc,
+                )
+                scenes_list.append(scene_metadata)
+
+        return scenes_list
+
+    def get_scene(self, scene_info: SceneMetadata) -> Scene:
+        raise NotImplementedError()
+
+    def get_agent_info(
+        self, scene: Scene, cache_path: Path, cache_class: Type[SceneCache]
+    ) -> Tuple[List[AgentMetadata], List[List[AgentMetadata]]]:
+        raise NotImplementedError()
+
+    def cache_map(
+        self,
+        map_name: str,
+        cache_path: Path,
+        map_cache_class: Type[SceneCache],
+        map_params: Dict[str, Any],
+    ) -> None:
+        raise NotImplementedError()
+
+    def cache_maps(
+        self,
+        cache_path: Path,
+        map_cache_class: Type[SceneCache],
+        map_params: Dict[str, Any],
+    ) -> None:
+        raise NotImplementedError()
diff --git a/src/trajdata/dataset_specific/drivesim/nvmap_utils.py b/src/trajdata/dataset_specific/drivesim/nvmap_utils.py
new file mode 100644
index 0000000..0f41cc2
--- /dev/null
+++ b/src/trajdata/dataset_specific/drivesim/nvmap_utils.py
@@ -0,0 +1,1792 @@
+
+import os, shutil, copy
+import glob
+import json
+from collections import OrderedDict, deque
+
+import numpy as np
+import cv2
+
+import matplotlib as mpl
+# mpl.use('Agg')
+import matplotlib.pyplot as plt
+from matplotlib.patches import Circle, Polygon, PathPatch, Ellipse
+from matplotlib.path import Path
+
+import sys
+
+# NOTE: NVMap documentation https://nvmapspec.drive.nvda.io/map_layer_format/index.html
+
+# transformation matrix of the base pose of the global cooridinate system (defined in ECEF)
+#       will be used to transform each road segment to global frame.
+# rivermark is used for autolabels
+GLOBAL_BASE_POSE_RIVERMARK = np.array([[ 0.7661314567952979, -0.47780431092164843, -0.42982046411659275, -2684537.8381108316],
+                                        [ 0.06759529959471867,  0.7249870917843614, -0.6854375188292178, -4305029.705512633],
+                                        [ 0.6391192896333319,  0.49608140179888094,  0.5877327423309361,  3852303.4349504006],
+                                        [ 0.0, 0.0,  0.0, 1.0]])
+# endeavor can be used for map if desired
+GLOBAL_BASE_POSE_ENDEAVOR = np.array([[0.892451945618358, 0.11765375230214345, -0.43553084773783024, -2686874.9549495797],
+                                        [-0.40233607907375113, 0.644307257120398, -0.6503797643665964, -4305568.426410184],
+                                        [0.2040960661981692, 0.7556624596942837, 0.6223496145826857, 3850100.0530229895],
+                                        [ 0.0, 0.0,  0.0, 1.0]])
+# this is the one that will be used for the map and trajectories
+GLOBAL_BASE_POSE = GLOBAL_BASE_POSE_ENDEAVOR
+# GLOBAL_BASE_POSE = GLOBAL_BASE_POSE_RIVERMARK
+
+MAX_ENDEAVOR_FRAME = 3613
+
+RIVERMARK_VIZ = False
+# 0 : drivable only
+# 1 : + ego
+# 2 : + lines
+# 3 : + other non-extra cars
+# 4 : + all cars
+RIVERMARK_VIZ_LAYER = 4
+NVCOLOR = (118.0/255, 185.0/255, 0.0/255)
+NVCOLOR2 = (55.0/255, 225.0/255, 0.0/255)
+# EXTRA_DYN_LWH = [5.087, 2.307, 1.856]
+EXTRA_DYN_LWH = [4.387, 1.907, 1.656]
+
+NUSC_EXPAND_LEN = 0.0 #2.0 # meters to expand drivable area outward
+
+AUTOLABEL_DT = 0.1 # sec
+
+NVMAP_LAYERS = {'lane_dividers_v1', 'lanes_v1', 'lane_channels_v1'}
+SEGMENT_GRAPH_LAYER = 'segment_graph_v1'
+
+# maps track ids (which occur first in time) to other tracks that are 
+#       actually the same object
+# NOTE: each association list must be sorted in temporal order
+TRACK_ASSOC = {
+    # frame 3000 - 3545 sequence
+    2486 : [2584, 2669],
+    2546 : [2618],
+    2489 : [2558],
+    2496 : [2553],
+    2515 : [2578],
+    2525 : [2615],
+    2555 : [2609],
+    2546 : [2618, 2673],
+    # 2491 : [2566, 2631],
+    2491 : [2631],
+    # 2566 : [2631],
+    2484 : [2692, 2729, 2745, 2766],
+    2686 : [2811],
+    2618 : [2673],
+    2592 : [2719],
+    2251 : [2847],
+    2269 : [2734, 2853],
+    1280 : [2768, 2790],
+    2631 : [2685],
+    2561 : [2699],
+    # frame 570 - 990 sequences
+    1298 : [1360],
+    1282 : [1362],
+    1241 : [1366],
+    1305 : [1398],
+    1286 : [1377],
+    1327 : [1409],
+    1444 : [1576],
+    1326 : [1455],
+    1376 : [1425, 1435, 1444],
+    1345 : [1445, 1456, 1476],
+    1346 : [1477, 1491],
+    # frame 270 - 600
+    1067 : [1193],
+    1109 : [1206],
+    1126 : [1229],
+    1127 : [1234],
+    1137 : [1255],
+    1150 : [1251],
+    1168 : [1246],
+    1184 : [1269],
+    1203 : [1225],
+    1196 : [1307],
+    1211 : [1315],
+    1191 : [1297],
+    1183 : [1276],
+    1188 : [1274],
+    1178 : [1250],
+    # frame 1440 - 1860
+    1717 : [1761],
+    1712 : [1778],
+    1704 : [1786],
+    1733 : [1803],
+    1708 : [1722,1811],
+    1674 : [1732,1740],
+    1760 : [1837],
+    1824 : [1836],
+    1793 : [1872],
+    1812 : [1869],
+    1748 : [1812],
+    1826 : [1864],
+    1723 : [1756],
+    # rivermark
+    9409 : [9517]
+}
+# false positive (or extrapolation is bad)
+# 2566
+TRACK_REMOVE = {2676, 2687, 2590, 2833, 1199, 9413, 9449, 9437, 9589, 2588, 2669, 2562, 2516, 2566} # 9449 is rivermark dynamic, and 9437 is rivermark trash can
+
+def check_single_veh_coll(traj_tgt, lw_tgt, traj_others, lw_others):
+    '''
+    Checks if the target trajectory collides with each of the given other trajectories.
+
+    Assumes all trajectories and attributes are UNNORMALIZED. Handles nan frames in traj_others by simply skipping.
+
+    :param traj_tgt: (T x 4)
+    :param lw_tgt: (2, )
+    :param traj_others: (N x T x 4)
+    :param lw_others: (N x 2)
+
+    :returns veh_coll: (N)
+    :returns coll_time: (N)
+    '''
+    from shapely.geometry import Polygon
+
+    NA, FT, _ = traj_others.shape
+
+    veh_coll = np.zeros((NA, FT), dtype=np.bool)
+    poly_cache = dict() # for the tgt polygons since used many times
+    for aj in range(NA):
+        for t in range(FT):
+            # compute iou
+            if t not in poly_cache:
+                ai_state = traj_tgt[t, :]
+                if np.sum(np.isnan(ai_state)) > 0:
+                    continue
+                ai_corners = get_corners(ai_state, lw_tgt)
+                ai_poly = Polygon(ai_corners)
+                poly_cache[t] = ai_poly
+            else:
+                ai_poly = poly_cache[t]
+
+            aj_state = traj_others[aj, t, :]
+            if np.sum(np.isnan(aj_state)) > 0:
+                continue
+            aj_corners = get_corners(aj_state, lw_others[aj])
+            aj_poly = Polygon(aj_corners)
+            cur_iou = ai_poly.intersection(aj_poly).area / ai_poly.union(aj_poly).area
+            if cur_iou > 0.02:
+                veh_coll[aj, t] = True
+
+    return veh_coll
+
+def plt_color(i):
+    clist = plt.rcParams['axes.prop_cycle'].by_key()['color']
+    return clist[i]
+
+def get_rot(h):
+    return np.array([
+        [np.cos(h), np.sin(h)],
+        [-np.sin(h), np.cos(h)],
+    ])
+
+def get_corners(box, lw):
+    l, w = lw
+    simple_box = np.array([
+        [-l/2., -w/2.],
+        [l/2., -w/2.],
+        [l/2., w/2.],
+        [-l/2., w/2.],
+    ])
+    h = np.arctan2(box[3], box[2])
+    rot = get_rot(h)
+    simple_box = np.dot(simple_box, rot)
+    simple_box += box[:2]
+    return simple_box
+
+def plot_box(box, lw, color='g', alpha=0.7, no_heading=False):
+    l, w = lw
+    h = np.arctan2(box[3], box[2])
+    simple_box = get_corners(box, lw)
+
+    arrow = np.array([
+        box[:2],
+        box[:2] + l/2.*np.array([np.cos(h), np.sin(h)]),
+    ])
+
+    plt.fill(simple_box[:, 0], simple_box[:, 1], color=color, edgecolor='k', alpha=alpha, linewidth=1.0, zorder=3)
+    if not no_heading:
+        # plt.plot(arrow[:, 0], arrow[:, 1], color, alpha=1.0)
+        plt.plot(arrow[:, 0], arrow[:, 1], 'k', alpha=alpha, zorder=3)
+
+def create_video(img_path_form, out_path, fps):
+    '''
+    Creates a video from a format for frame e.g. 'data_out/frame%04d.png'.
+    Saves in out_path.
+    '''
+    import subprocess
+    # if RIVERMARK_VIZ:
+    #     ffmpeg_cmd = ['ffmpeg', '-y', '-i', img_path_form,
+    #                 '-vf', 'transpose=2', img_path_form]
+    #     subprocess.run(ffmpeg_cmd)
+
+    ffmpeg_cmd = ['ffmpeg', '-y', '-r', str(fps), '-i', img_path_form,
+                    '-vcodec', 'libx264', '-crf', '18', '-pix_fmt', 'yuv420p', out_path]
+    subprocess.run(ffmpeg_cmd)
+
+def debug_viz_vid(seg_dict, prefix, poses, poses_valid, poses_lwh,
+                    comp_out_path='./out/dev_nvmap',
+                    fps=10,
+                    subsamp=3,
+                    pose_ids=None,
+                    **kwargs):
+    poses = poses[:,::subsamp] 
+    poses_valid = poses_valid[:,::subsamp]
+    T = poses.shape[1]
+    out_dir = os.path.join(comp_out_path, prefix)
+    if not os.path.exists(out_dir):
+        os.makedirs(out_dir)
+    for t in range(T):
+        print('rendering frame %d...' % (t))
+        debug_viz_segs(seg_dict, 'frame_%06d' % (t), poses[:,t:t+1], poses_valid[:,t:t+1], poses_lwh,
+                        comp_out_path=out_dir,
+                        pose_ids=pose_ids,
+                        ego_traj=poses[0],
+                        **kwargs)
+    create_video(os.path.join(out_dir, 'frame_%06d.jpg'), out_dir + '.mp4', fps)
+
+def debug_viz_segs(seg_dict, prefix, poses=None, poses_valid=None, poses_lwh=None, pose_ids=None,
+                    comp_out_path='./out/dev_nvmap',
+                    extent=80,
+                    grid=True,
+                    show_ticks=True,
+                    dpi=100,
+                    ego_traj=None):
+    '''
+    Visualize segments in the given dictionary.
+
+    :param seg_dict: segments dictionary
+    :param prefix: prefix to save figure
+    :param poses: NxTx4x4 trajectory for N vehicles that will be plotted if given.
+    '''
+    if not os.path.exists(comp_out_path):
+        os.makedirs(comp_out_path)
+
+    # fig = plt.figure()
+    fig = plt.figure(dpi=dpi)
+
+    origins = []
+    arr_len = 10.0
+    for _, seg in seg_dict.items():
+        if poses is not None:
+            dist2ego = np.linalg.norm(seg.local2world[:2, -1] - poses[0,0,:2,-1])
+            if dist2ego > 200:
+                continue
+        # plot layers
+        if 'drivable_area' in seg.layers:
+            # draw drivable area first so under everything else
+            for drivable_poly in seg.layers['drivable_area']:
+                polypatch = Polygon(drivable_poly[:,:2],
+                                    color='darkgray',
+                                    alpha=1.0,
+                                    linestyle='-')
+                                    # linewidth=2)
+                plt.gca().add_patch(polypatch)
+
+        if RIVERMARK_VIZ and RIVERMARK_VIZ_LAYER < 2:
+            # only drivable area
+            continue
+
+        for layer_k, layer_v in seg.layers.items():
+            if layer_k in {'lane_dividers', 'lane_divider'}:
+                for lane_div in layer_v:
+                    polyline = lane_div.polyline if isinstance(lane_div, LaneDivider) else lane_div
+                    linepatch = PathPatch(Path(polyline[:,:2]),
+                                        fill=False,
+                                        color='gold',
+                                        linestyle='-')
+                                        # linewidth=2)
+                    plt.gca().add_patch(linepatch)
+            elif layer_k in {'road_dividers', 'road_divider'}:
+                for road_div in layer_v:
+                    linepatch = PathPatch(Path(road_div[:,:2]),
+                                        fill=False,
+                                        color='orange',
+                                        linestyle='-')
+                                        # linewidth=2)
+                    plt.gca().add_patch(linepatch)
+            elif layer_k in {'road_boundaries'}:
+                for road_bound in layer_v:
+                    linepatch = PathPatch(Path(road_bound.polyline[:,:2]),
+                                        fill=False,
+                                        color='darkgray',
+                                        linestyle='-')
+                                        # linewidth=2)
+                    plt.gca().add_patch(linepatch)
+            # elif layer_k in {'lane_channels'}:
+            #     for lane_channel in layer_v:
+            #         linepatch = PathPatch(Path(lane_channel.left),
+            #                             fill=False,
+            #                             color='blue',
+            #                             linestyle='-')
+            #                             # linewidth=2)
+            #         plt.gca().add_patch(linepatch)
+            #         linepatch = PathPatch(Path(lane_channel.right),
+            #                             fill=False,
+            #                             color='red',
+            #                             linestyle='-')
+            #                             # linewidth=2)
+            #         plt.gca().add_patch(linepatch)
+
+        # plot local coordinate system origin
+        if poses is None:
+            local_coords = np.array([[0.0, 0.0, 0.0, 1.0], [arr_len, 0.0, 0.0, 1.0], [0.0, arr_len, 0.0, 1.0]])
+            world_coords = np.dot(seg.local2world, local_coords.T).T
+            world_coords = world_coords[:,:2] # only plot 2D coords
+            origins.append(world_coords[0])
+            xdelta = world_coords[1] - world_coords[0]
+            ydelta = world_coords[2] - world_coords[0]
+            plt.arrow(world_coords[0, 0], world_coords[0, 1], xdelta[0], xdelta[1], color='red')
+            plt.arrow(world_coords[0, 0], world_coords[0, 1], ydelta[0], ydelta[1], color='green')
+
+    if poses is not None and poses_valid is not None and poses_lwh is not None:
+        # center on ego
+        origin = poses[0,0,:2,-1]
+        if RIVERMARK_VIZ:
+            extent = 45
+            origin = origin + np.array([extent - 15.0, 0.0])
+
+        # if RIVERMARK_VIZ and RIVERMARK_VIZ_LAYER > 3:
+        #     # plot ego traj
+        #     ego_pos = ego_traj[::10,:2,3]
+        #     plt.plot(ego_pos[:,0], ego_pos[:,1], 'o-', c=NVCOLOR2, markersize=2.5) #, markersize=8), linewidth
+            
+        plt.xlim(origin[0]-extent, origin[0]+extent)
+        plt.ylim(origin[1]-extent, origin[1]+extent)
+        for n in range(poses.shape[0]):
+            if RIVERMARK_VIZ and RIVERMARK_VIZ_LAYER < 1:
+                continue
+            if RIVERMARK_VIZ and RIVERMARK_VIZ_LAYER < 3 and n != 0:
+                continue
+            if RIVERMARK_VIZ and RIVERMARK_VIZ_LAYER < 4 and pose_ids[n] == 'extra':
+                continue
+            # if RIVERMARK_VIZ and RIVERMARK_VIZ_LAYER == 3 and n == 0:
+            #     continue
+            cur_color = plt_color((n+2) % 9)
+            if RIVERMARK_VIZ and RIVERMARK_VIZ_LAYER >= 4 and pose_ids[n] == 'extra':
+                cur_color = '#ff00ff'
+            if RIVERMARK_VIZ:
+                print(n)
+                print(pose_ids[n])
+                print(cur_color)
+            cur_poses = poses[n] #, ::20]
+            xy = cur_poses[:,:2,3]
+            hvec = cur_poses[:,:2,0] # x axis
+            hvec = hvec / np.linalg.norm(hvec, axis=1, keepdims=True)
+            for t in range(cur_poses.shape[0]):
+                if poses_valid[n, t]:
+                    plot_box(np.array([xy[t,0], xy[t,1], hvec[t,0], hvec[t,1]]), poses_lwh[n,:2],
+                                    color=NVCOLOR if n ==0 else cur_color, alpha=1.0, no_heading=False)
+                    if pose_ids is not None and not RIVERMARK_VIZ:
+                        plt.text(xy[t,0] + 1.0, xy[t,1] + 1.0, pose_ids[n], c='red', fontsize='x-small')
+
+    plt.gca().set_aspect('equal')
+    plt.grid(grid)
+    if not show_ticks:
+        plt.xticks([])
+        plt.yticks([])
+        plt.gca().axis('off')
+    # plt.tight_layout()
+    cur_save_path = os.path.join(comp_out_path, prefix + '.jpg')
+    fig.savefig(cur_save_path)
+    # plt.show()
+    plt.close(fig)
+
+    if RIVERMARK_VIZ:
+        og_img = cv2.imread(cur_save_path)
+        rot_img = cv2.rotate(og_img, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)
+        cv2.imwrite(cur_save_path, rot_img)
+
+
+def get_tile_mask(tile, layer_name, local_box, canvas_size):
+    '''
+    Rasterizes a layer of the given tile object into a binary mask.
+    Assumes tile object has been converted to hold nuscenes-like layers.
+
+    :param tile: NVMapTile object holding the nuscenes layers
+    :param layer_name str: which layer to rasterize, currently supports ['drivable_area', 'carpark_area', 'road_divider', 'lane_divider']
+    :param local_box tuple: (center_x, center_y, H, W) in meters which patch of the map to rasterize
+    :param canvas_size tuple: (H, W) pixels tuple which determines the resolution at which the layer is rasterized
+    '''
+    # must transform each map element to pixel space
+    # https://github.com/nutonomy/nuscenes-devkit/blob/9b209638ef3dee6d0cdc5ac700c493747f5b35fe/python-sdk/nuscenes/map_expansion/map_api.py#L1894
+    patch_x, patch_y, patch_h, patch_w = local_box
+    canvas_h = canvas_size[0]
+    canvas_w = canvas_size[1]
+    scale_height = canvas_h/patch_h
+    scale_width = canvas_w/patch_w
+    trans_x = -patch_x + patch_w / 2.0
+    trans_y = -patch_y + patch_h / 2.0
+    trans = np.array([[trans_x, trans_y]])
+    scale = np.array([[scale_width, scale_height]])
+
+    map_mask = np.zeros(canvas_size, np.uint8)
+    for seg_id, seg in tile.segments.items():
+        for poly_pts in seg.layers[layer_name]:
+            # convert to pixel coords
+            poly_pts = (poly_pts + trans)*scale
+            # rasterize
+            if layer_name in {'drivable_area'}:
+                # polygon
+                coords = poly_pts.astype(np.int32)
+                cv2.fillPoly(map_mask, [coords], 1)
+            elif layer_name in {'lane_divider', 'road_divider'}:
+                # polyline
+                coords = poly_pts.astype(np.int32)
+                cv2.polylines(map_mask, [coords], False, 1, 2)
+            elif layer_name in {'carpark_area'}:
+                # empty
+                pass
+            else:
+                print('Unrecognized layer %d - cannot render mask' % (layer_name))
+
+    return map_mask
+    
+# https://www.dmv.ca.gov/portal/handbook/california-driver-handbook/lane-control/
+#   - solid yellow lines = center of road for two-way (road divider)
+#   - two, one solid one broken yellow = may pass but going opposite dir (road divider)
+#   - two solid yellow = road divider
+#   - solid white = edge of road going same way (lane divider)
+#   - broken white = two or more lanes same direction (lane divider)
+#   - double white = HOV (lane divider)
+#   - invisible should be ommitted (e.g. will just be in intersections)
+
+def convert_tile_to_nuscenes(tile):
+    '''
+    Given a tile, converts its layers into similar format as nuscenes.
+    This includes converting lane dividers and road boundaries to lane/road dividers and
+    drivable area.
+    returns an updated copy of the tile.
+    '''
+    print('Converting to nuscenes...')
+    tile = copy.deepcopy(tile)
+    for seg_id, seg in tile.segments.items():
+        if 'lane_dividers' in seg.layers:
+            nusc_lane_dividers = []
+            nusc_road_dividers = []
+            for div in seg.layers['lane_dividers']:
+                style = div.style
+                if len(style) > 0:
+                    div_color = style[0][2]
+                    div_pattern = style[0][0]
+                    if div_color in {'White'}: #, 'Green'}:
+                        # divides traffic in same direction
+                        nusc_lane_dividers.append(div.polyline[:,:2])
+                    elif div_color in {'Yellow'} or (len(style) == 2 and div_pattern == 'Botts Dots'): #, 'Blue', 'Red', 'Orange'}:
+                        # divides traffic in opposite direction
+                        nusc_road_dividers.append(div.polyline[:,:2])
+                # elif RIVERMARK_VIZ:
+                #     nusc_lane_dividers.append(div.polyline[:,:2])
+            # update segment
+            seg.layers['lane_divider'] = nusc_lane_dividers
+            seg.layers['road_divider'] = nusc_road_dividers
+
+            del seg.layers['lane_dividers']
+        
+        if 'road_boundaries' in seg.layers:
+            del seg.layers['road_boundaries']
+            pass # actually don't need this for now, it's just for reference
+
+        if 'lane_channels' in seg.layers:
+            # convert to drivable area polygon
+            expand_len = NUSC_EXPAND_LEN # meters
+            drivable_area_polys = []
+            for channel in seg.layers['lane_channels']:
+                left = channel.left[:,:2]
+                right = channel.right[:,:2]
+                if RIVERMARK_VIZ:
+                    seg.layers['lane_divider'].append(left)
+                    seg.layers['road_divider'].append(right)
+                if left.shape[0] > 1:
+                    # compute normals at each vertex to expand
+                    left_diff = left[1:] - left[:-1]
+                    left_diff = np.concatenate([left_diff, left_diff[-1:,:]], axis=0)
+                    left_diff = left_diff / np.linalg.norm(left_diff, axis=1, keepdims=True)
+                    left_norm = np.concatenate([-left_diff[:,1:2], left_diff[:,0:1]], axis=1)
+                    right_diff = right[1:] - right[:-1]
+                    right_diff = np.concatenate([right_diff, right_diff[-1:,:]], axis=0)
+                    right_diff = right_diff / np.linalg.norm(right_diff, axis=1, keepdims=True)
+                    right_norm = np.concatenate([right_diff[:,1:2], -right_diff[:,0:1]], axis=1)
+                    # expand channel
+                    left = left + (left_norm * expand_len)
+                    right = right + (right_norm * expand_len)
+
+                channel_poly = np.concatenate([right, np.flip(left, axis=0)], axis=0)
+                drivable_area_polys.append(channel_poly)
+            seg.layers['drivable_area'] = drivable_area_polys
+            del seg.layers['lane_channels']
+
+        # add empty carpark area for completeness
+        seg.layers['carpark_area'] = []
+
+    # compute extents of map
+    map_maxes = np.array([-float('inf'), -float('inf')]) # xmax, ymax
+    map_mins = np.array([float('inf'), float('inf')]) # xmin, ymin
+    for _, seg in tile.segments.items():
+        for k, v in seg.layers.items():
+            if len(v) > 0:
+                all_pts = np.concatenate(v, axis=0)
+                cur_maxes = np.amax(all_pts, axis=0)
+                cur_mins = np.amin(all_pts, axis=0)
+                map_maxes = np.where(cur_maxes > map_maxes, cur_maxes, map_maxes)
+                map_mins = np.where(cur_mins < map_mins, cur_mins, map_mins)
+    map_xlim = (map_mins[0] - 10, map_maxes[0] + 10) # buffer of 10m
+    map_ylim = (map_mins[1] - 10, map_maxes[1] + 10) # buffer of 10m
+    W = map_xlim[1] - map_xlim[0]
+    H = map_ylim[1] - map_ylim[0]
+    # translate so bottom left corner is at origin
+    trans_offset = np.array([[-map_xlim[0], -map_ylim[0]]])
+    tile.trans_offset = trans_offset
+    tile.H = H
+    tile.W = W
+    for _, seg in tile.segments.items():
+        seg.local2world[:2, -1] += trans_offset[0]
+        for k, v in seg.layers.items():
+            if len(v) > 0:
+                for pts in v:
+                    pts += trans_offset
+
+    return tile
+
+def load_tile(tile_path,
+              layers=['lane_dividers_v1', 'lane_channels_v1']):
+    # load in all road segment dicts
+    print('Parsing segment graph...')
+    tile_name = tile_path.split('/')[-1]
+    segs_path = os.path.join(tile_path, SEGMENT_GRAPH_LAYER)
+    assert os.path.exists(tile_path), 'cannot find segment graph layer, which is required to load any layers'
+    seg_json_list, _ = load_json_dir(segs_path)
+    road_segments = parse_road_segments(seg_json_list)
+    print('Found %d road segments:' % (len(road_segments)))
+
+    # fill road segments with other desired layers
+    print('Loading requested layers...')
+    for layer in layers:
+        assert layer in NVMAP_LAYERS, 'loading layer type %s is currently not supported!' % (layer)
+        layer_dir = os.path.join(tile_path, layer)
+        assert os.path.exists(layer_dir), 'could not find requested layer %s in tile directory!' % (layer)
+        layer_json_list, seg_names = load_json_dir(layer_dir)
+        if layer == 'lane_dividers_v1':
+            parse_lane_dividers(layer_json_list, seg_names, road_segments)
+        elif layer == 'lane_channels_v1':
+            parse_lane_channels(layer_json_list, seg_names, road_segments)
+        elif layer == 'lanes_v1':
+            raise NotImplementedError()
+
+    return NVMapTile(road_segments, name=tile_name)
+
+
+def load_json_dir(json_dir_path):
+    '''
+    Loads in all json files in the given directory and returns the resulting list of dicts along
+    with the names of the json files read from.
+    '''
+    json_files = sorted(glob.glob(os.path.join(json_dir_path, '*.json')))
+    file_names = ['.'.join(jf.split('/')[-1].split('.')[:-1]) for jf in json_files]
+    json_list = []
+    for jf in json_files:
+        with open(jf, 'r') as f:
+            json_list.append(json.load(f))
+    return json_list, file_names
+
+def parse_pt(pt_dict):
+    pt_entries = ['x', 'y', 'z', 'w']
+    if len(pt_dict) == 3:
+        pt = [pt_dict[pt_entries[0]], pt_dict[pt_entries[1]], pt_dict[pt_entries[2]]]
+    elif len(pt_dict) == 4:
+        pt = [pt_dict[pt_entries[0]], pt_dict[pt_entries[1]], pt_dict[pt_entries[2]], pt_dict[pt_entries[3]]]
+    else:
+        assert False, 'input point must be length 3 or 4'
+    return pt
+
+def parse_pt_list(pt_list):
+    return np.array([parse_pt(pt) for pt in pt_list])
+
+def parse_lane_channels(lane_channels_json_list, seg_name_list, road_segments):
+    '''
+    Parses lane channels in each segment to an object, and store in its respective segment.
+    Transforms channels into the global coordinate system.
+
+    :param lane_channels_json_list: list of lane channels json dicts
+    :param seg_name_list: the name of the segment corresponding to each json file in lane_div_json_list
+    :param road_segments: dict of all road segments in a tile
+    :return: updated road_segments (also updated in place)
+    '''
+    for channel_dict, seg_name in zip(lane_channels_json_list, seg_name_list):
+        cur_seg = road_segments[seg_name]
+        # load lane dividers
+        lane_channels = []
+        lane_channel_dicts = channel_dict['channels']
+        for channel in lane_channel_dicts:
+            # left
+            left_geom = channel['left_side']['geometry'][0]['chunk']['channel_edge_line']
+            left_polyline = parse_pt_list(left_geom['points'])
+            left_polyline = cur_seg.to_global(left_polyline)[:,:3]
+            # right
+            right_geom = channel['right_side']['geometry'][0]['chunk']['channel_edge_line']
+            right_polyline = parse_pt_list(right_geom['points'])
+            right_polyline = cur_seg.to_global(right_polyline)[:,:3]
+            assert left_polyline.shape[0] == right_polyline.shape[0], 'channel edges should be same length!'
+            # build lane channel object
+            lane_channels.append(LaneChannel(left_polyline, right_polyline))
+        cur_seg.layers['lane_channels'] = lane_channels
+
+    return road_segments
+
+def parse_lane_dividers(lane_div_json_list, seg_name_list, road_segments):
+    '''
+    Parses lane dividers in each segment to an object, and store in its respective segment.
+    Transforms dividers into the global coordinate system.
+
+    :param lane_div_json_list: list of lane divider json dicts
+    :param seg_name_list: the name of the segment corresponding to each json file in lane_div_json_list
+    :param road_segments: dict of all road segments in a tile
+    :return: updated road_segments (also updated in place)
+    '''
+    for div_dict, seg_name in zip(lane_div_json_list, seg_name_list):
+        cur_seg = road_segments[seg_name]
+        # load lane dividers
+        lane_divs = []
+        lane_div_dicts = div_dict['dividers']
+        for div in lane_div_dicts:
+            # NOTE: left/right/height lines also sometimes available - ignoring for now
+            # NOTE: lane divider styles (type/color) also available - ignoring for now
+            # center_line is only guaranteed
+            lane_geom = div['geometry']['center_line']
+            lane_polyline = parse_pt_list(lane_geom['points'])
+            lane_polyline = cur_seg.to_global(lane_polyline)[:,:3]
+            # parse divider style
+            if 'style' in div:
+                lane_styles = div['style']
+                lane_styles = [(style['pattern'], style['material'], style['color']) for style in lane_styles]
+            else:
+                lane_styles = []
+            # build lane div object
+            lane_divs.append(LaneDivider(lane_polyline, lane_styles))
+        cur_seg.layers['lane_dividers'] = lane_divs
+
+        # load road boundaries
+        road_bounds = []
+        road_bound_dicts = div_dict['road_boundaries']
+        for div in road_bound_dicts:
+            # NOTE: left/right/height lines also sometimes available - ignoring for now
+            # NOTE: road boundary type also available - ignoring for now
+            # center_line is only guaranteed
+            bound_geom = div['geometry']['center_line']
+            bound_polyline = parse_pt_list(bound_geom['points'])
+            bound_polyline = cur_seg.to_global(bound_polyline)[:,:3]
+            # parse boundary type
+            if 'type' in div:
+                bound_type = div['type']
+            else:
+                bound_type = None
+            # build object
+            road_bounds.append(RoadBoundary(bound_polyline, bound_type))
+        cur_seg.layers['road_boundaries'] = road_bounds
+    
+    return road_segments
+
+def parse_road_segments(seg_json_list):
+    '''
+    Parses road segments into objects, and transforms into a shared coordinate system.
+
+    :param seg_json_list: list of road segment json dicts
+
+    :return: dict of all road segments mapping id -> RoadSegment
+    '''
+    # build objects for all road segments
+    road_segments = OrderedDict()
+    for seg_dict in seg_json_list:
+        cur_seg = build_road_seg(seg_dict)
+        road_segments[cur_seg.id] = cur_seg
+
+    # go through and annotate local2world by converting GPS to the "global" coordinate system
+    for seg_id, seg in road_segments.items():
+        lat_lng_alt = np.array(seg.gps_origin).reshape((-1,3))
+        rot_axis = np.array([1.0, 0.0, 0.0]).reshape((-1,3))
+        rot_angle = np.array([0.0]).reshape((-1,1))
+        ecef_pose = lat_lng_alt_2_ecef(lat_lng_alt, rot_axis, rot_angle, 'WGS84')[0]
+        seg.local2world = np.linalg.inv(GLOBAL_BASE_POSE) @ ecef_pose
+    
+    return road_segments
+
+def build_road_seg(seg_dict):
+    '''
+    Parses road segment json dictionary into an object.
+    '''
+    segment = seg_dict['segment']
+    seg_id = segment['id']
+    seg_origin = [segment['origin']['lat'], segment['origin']['lon'], segment['origin']['height']]
+    connections = []
+    if 'connections' in segment:
+        conn_list = segment['connections']
+        for conn in conn_list:
+            source_id = conn['source_id']
+            source2tgt = []
+            for ci in range(4):
+                source2tgt.append(np.array(parse_pt(conn['source_to_target'][f'column_{ci}'])))
+            source2tgt = np.stack(source2tgt, axis=1)
+            connections.append((source_id, source2tgt))
+    return RoadSegment(seg_id, seg_origin, connections)
+
+def collect_seg_origins(road_segments):
+    '''
+    Returns np array of all road_segment origins (in order of dict).
+    :param road_segments: OrderedDict of RoadSegment objects
+    '''
+    return np.array([seg.origin for _, seg in road_segments.items()])
+
+class RoadSegment(object):
+    def __init__(self, seg_id, origin, connections,
+                 is_root=False,
+                 layers=None):
+        '''
+        :param seg_id str:
+        :param origin: list of [lat, lon, height]
+        :param connections list: list of tuples, each containing (neighbor_id, neighbor2local_transform) 
+                            where the transform is a np.array(4,4))
+        :param layers dict: layer objects within this road segment
+        '''
+        self.id = seg_id
+        self.gps_origin = origin # GPS
+        self.connections = connections
+        self.layers = layers if layers is not None else dict()
+        # transformation matrix from local to world frame
+        self.local2world = None
+
+    def to_global(self, pts):
+        '''
+        Transform an array of points from this segment's frame to global.
+
+        :param pts: np array (N x 3)
+        '''
+        pts = np.concatenate([pts, np.ones((pts.shape[0], 1))], axis=1)
+        pts = np.dot(self.local2world, pts.T).T
+        return pts
+
+    def transform(self, mat):
+        '''
+        Transforms this segment and all contained layers by the given 4x4 transformation matrix.
+        '''
+        self.local2world = mat @ self.local2world
+        for _, layer in self.layers.items():
+            for el in layer:
+                el.transform(mat)
+
+    def __repr__(self):
+        return '<RoadSegment -- %s>' % str('\n '.join('%s : %s' % (k, repr(v)) for (k, v) in self.__dict__.items()))
+
+class LaneDivider(object):
+    def __init__(self, polyline, style):
+        '''
+        :param polyline: np.array Nx3 defining the divider geometry
+        :param style: list of tuples of (pattern, matterial, color)
+        '''
+        self.polyline = polyline
+        self.style = style
+
+    def transform(self, mat):
+        '''
+        Transforms this map element by the given 4x4 transformation matrix.
+        '''
+        pts = np.concatenate([self.polyline, np.ones((self.polyline.shape[0], 1))], axis=1)
+        pts = np.dot(mat, pts.T).T
+        self.polyline = pts[:,:3]
+
+    def __repr__(self):
+        return '<LaneDivider -- %s>' % str('\n '.join('%s : %s' % (k, repr(v)) for (k, v) in self.__dict__.items()))
+
+class RoadBoundary(object):
+    def __init__(self, polyline, bound_type):
+        '''
+        :param polyline: np.array Nx3 defining the divider geometry
+        :param type str: type of the boundary
+        '''
+        self.polyline = polyline
+        self.bound_type = bound_type
+
+    def transform(self, mat):
+        '''
+        Transforms this map element by the given 4x4 transformation matrix.
+        '''
+        pts = np.concatenate([self.polyline, np.ones((self.polyline.shape[0], 1))], axis=1)
+        pts = np.dot(mat, pts.T).T
+        self.polyline = pts[:,:3]
+
+    def __repr__(self):
+        return '<RoadBoundary -- %s>' % str('\n '.join('%s : %s' % (k, repr(v)) for (k, v) in self.__dict__.items()))
+
+class LaneChannel(object):
+    def __init__(self, left_polyline, right_polyline):
+        '''
+        :param left_polyline: np.array Nx3 defining the channel left edge geometry
+        :param right_polyline: np.array Nx3 defining the channel right edge geometry
+        '''
+        self.left = left_polyline
+        self.right = right_polyline
+
+    def transform(self, mat):
+        '''
+        Transforms this map element by the given 4x4 transformation matrix.
+        '''
+        pts = np.concatenate([self.left, np.ones((self.left.shape[0], 1))], axis=1)
+        pts = np.dot(mat, pts.T).T
+        self.left = pts[:,:3]
+        pts = np.concatenate([self.right, np.ones((self.right.shape[0], 1))], axis=1)
+        pts = np.dot(mat, pts.T).T
+        self.right = pts[:,:3]
+
+    def __repr__(self):
+        return '<LaneChannel -- %s>' % str('\n '.join('%s : %s' % (k, repr(v)) for (k, v) in self.__dict__.items()))
+
+class NVMapTile(object):
+    def __init__(self, segments,
+                   name=None):
+        '''
+        :param segments: dict mapping seg_id -> RoadSegment objects for all road segments in the tile.
+        '''
+        self.segments = segments
+        self.name = name
+
+    def transform(self, mat):
+        '''
+        Multiply all elements of the map tile by the given 4x4 matrix
+        '''
+        for seg_id, seg in self.segments.items():
+            seg.transform(mat)
+
+    def __repr__(self):
+        return '<NVMapTile -- %s>' % str('\n '.join('%s : %s' % (k, repr(v)) for (k, v) in self.__dict__.items()))
+
+
+#######################################################################################################################
+
+#
+#  Utils from Zan for converting between GPS and world coordinates
+#
+
+from scipy.spatial.transform import Rotation as R
+
+
+def lat_lng_alt_2_ecef(lat_lng_alt, orientation_axis, orientation_angle, earth_model='WGS84'):
+    ''' Computes the transformation from the world pose coordiante system to the earth centered earth fixed (ECEF) one
+    Args:
+        lat_lng_alt (np.array): latitude, longitude and altitude coordinate (in degrees and meters) [n,3]
+        orientation_axis (np.array): orientation in the local ENU coordinate system [n,3]
+        orientation_angle (np.array): orientation angle of the local ENU coordinate system in degrees [n,1]
+        earth_model (string): earth model used for conversion (spheric will be unaccurate when maps are large)
+    Out:
+        trans (np.array): transformation parameters from world pose to ECEF coordinate system in se3 form (n, 4, 4)
+    '''
+    n = lat_lng_alt.shape[0]
+    trans = np.tile(np.eye(4).reshape(1,4,4),[n,1,1])
+
+    theta = (90. - lat_lng_alt[:, 0]) * np.pi/180
+    phi = lat_lng_alt[:, 1] * np.pi/180
+
+    R_enu_ecef = local_ENU_2_ECEF_orientation(theta, phi)
+
+    if earth_model == 'WGS84':
+        a = 6378137.0
+        flattening = 1.0 / 298.257223563
+        b = a * (1.0 - flattening)
+        translation = lat_lng_alt_2_translation_ellipsoidal(lat_lng_alt, a, b)
+
+    elif earth_model == 'sphere':
+        earth_radius = 6378137.0 # Earth radius in meters
+        z_dir =  np.concatenate([(np.sin(theta)*np.cos(phi))[:,None], 
+                            (np.sin(theta)*np.sin(phi))[:,None], 
+                            (np.cos(theta))[:,None] ],axis=1)
+
+        translation = (earth_radius + lat_lng_alt[:, -1])[:,None] * z_dir
+    
+    else:
+        raise ValueError ("Selected ellipsoid not implemented!")
+
+    world_pose_orientation = axis_angle_2_so3(orientation_axis, orientation_angle)
+
+    trans[:,:3,:3] =  R_enu_ecef @ world_pose_orientation
+    trans[:,:3,3] =  translation 
+
+    return trans
+
+def local_ENU_2_ECEF_orientation(theta, phi):
+    ''' Computes the rotation matrix between the world_pose and ECEF coordinate system
+    Args:
+        theta (np.array): theta coordinates in radians [n,1]
+        phi (np.array): phi coordinates in radians [n,1]
+    Out:
+        (np.array): rotation from world pose to ECEF in so3 representation [n,3,3]
+    '''
+    z_dir = np.concatenate([(np.sin(theta)*np.cos(phi))[:,None], 
+                            (np.sin(theta)*np.sin(phi))[:,None], 
+                            (np.cos(theta))[:,None] ],axis=1)
+    z_dir = z_dir/np.linalg.norm(z_dir, axis=-1, keepdims=True)
+
+    y_dir = np.concatenate([-(np.cos(theta)*np.cos(phi))[:,None], 
+                            -(np.cos(theta)*np.sin(phi))[:,None], 
+                            (np.sin(theta))[:,None] ],axis=1)
+    y_dir = y_dir/np.linalg.norm(y_dir, axis=-1, keepdims=True)
+
+    x_dir = np.cross(y_dir, z_dir)
+
+    return np.concatenate([x_dir[:,:,None], y_dir[:,:,None], z_dir[:,:,None]], axis = -1)
+
+
+def lat_lng_alt_2_translation_ellipsoidal(lat_lng_alt, a, b):
+    ''' Computes the translation based on the ellipsoidal earth model
+    Args:
+        lat_lng_alt (np.array): latitude, longitude and altitude coordinate (in degrees and meters) [n,3]
+        a (float/double): Semi-major axis of the ellipsoid
+        b (float/double): Semi-minor axis of the ellipsoid
+    Out:
+        (np.array): translation from world pose to ECEF [n,3]
+    '''
+
+    phi =  lat_lng_alt[:, 0] * np.pi/180
+    gamma =  lat_lng_alt[:, 1] * np.pi/180
+
+    cos_phi = np.cos(phi)
+    sin_phi = np.sin(phi)
+    cos_gamma = np.cos(gamma)
+    sin_gamma = np.sin(gamma)
+    e_square = (a * a - b * b) / (a * a)
+
+    N = a / np.sqrt(1 - e_square * sin_phi * sin_phi)
+
+
+    x = (N + lat_lng_alt[:, 2]) * cos_phi * cos_gamma
+    y = (N + lat_lng_alt[:, 2]) * cos_phi * sin_gamma
+    z = (N *  (b*b)/(a*a) + lat_lng_alt[:, 2]) * sin_phi
+
+    return np.concatenate([x[:,None] ,y[:,None], z[:,None]], axis=1 )
+
+def axis_angle_2_so3(axis, angle, degrees=True):
+    ''' Converts the axis angle representation of the so3 rotation matrix
+    Args:
+        axis (np.array): the rotation axes [n,3]
+        angle float/double: rotation angles either in degrees or radians [n]
+        degrees bool: True if angle is given in degrees else False
+
+    Out:
+        (np array): rotations given so3 matrix representation [n,3,3]
+    '''
+    # Treat angle (radians) below this as 0.
+    cutoff_angle = 1e-9 if not degrees else 1e-9*180/np.pi
+    angle[angle < cutoff_angle] = 0.0
+
+    # Scale the axis to have the norm representing the angle
+    if degrees:
+        angle = np.radians(angle)
+    axis_angle = (angle/np.linalg.norm(axis, axis=1, keepdims=True)) * axis
+
+    return R.from_rotvec(axis_angle).as_matrix()
+
+def ecef_2_lat_lng_alt(trans, earth_model='WGS84'):
+    ''' Converts the transformation from the earth centered earth fixed (ECEF) coordinate frame to the world pose
+    Args:
+        trans (np.array): transformation parameters in ECEF [n,4,4]
+        earth_model (string): earth model used for conversion (spheric will be unaccurate when maps are large)
+    Out:
+        lat_lng_alt (np.array): latitude, longitude and altitude coordinate (in degrees and meters) [n,3]
+        orientation_axis (np.array): orientation in the local ENU coordinate system [n,3]
+        orientation_angle (np.array): orientation angle of the local ENU coordinate system in degrees [n,1]
+    '''
+
+    translation = trans[:,:3,3]
+    rotation = trans[:,:3,:3]
+    
+    if earth_model == 'WGS84':
+        a = 6378137.0
+        flattening = 1.0 / 298.257223563
+        lat_lng_alt = translation_2_lat_lng_alt_ellipsoidal(translation, a, flattening)
+
+    elif earth_model == 'sphere':
+        earth_radius = 6378137.0 # Earth radius in meters
+        lat_lng_alt = translation_2_lat_lng_alt_spherical(translation, earth_radius)
+
+    else:
+        raise ValueError ("Selected ellipsoid not implemented!")
+
+
+    # Compute the orientation axis and angle
+    theta = (90. - lat_lng_alt[:, 0]) * np.pi/180
+    phi = lat_lng_alt[:, 1] * np.pi/180
+
+    R_ecef_enu = local_ENU_2_ECEF_orientation(theta, phi).transpose(0,2,1)
+
+    orientation = R_ecef_enu @ rotation
+    orientation_axis, orientation_angle = so3_2_axis_angle(orientation)
+
+
+    return lat_lng_alt, orientation_axis, orientation_angle
+
+def translation_2_lat_lng_alt_spherical(translation, earth_radius):
+    ''' Computes the translation in the ECEF to latitude, longitude, altitude based on the spherical earth model
+    Args:
+        translation (np.array): translation in the ECEF coordinate frame (in meters) [n,3]
+        earth_radius (float/double): earth radius
+    Out:
+        (np.array): latitude, longitude and altitude [n,3]
+    '''
+    altitude = np.linalg.norm(translation, axis=-1) - earth_radius
+    latitude = 90 - np.arccos(translation[:,2] / np.linalg.norm(translation, axis=-1, keepdims=True)) * 180/np.pi
+    longitude =  np.arctan2(translation[:,1],translation[:,0]) * 180/np.pi
+
+    return np.concatenate([latitude[:,None], longitude[:,None], altitude[:,None]], axis=1)
+
+def translation_2_lat_lng_alt_ellipsoidal(translation, a, f):
+    ''' Computes the translation in the ECEF to latitude, longitude, altitude based on the ellipsoidal earth model
+    Args:
+        translation (np.array): translation in the ECEF coordinate frame (in meters) [n,3]
+        a (float/double): Semi-major axis of the ellipsoid
+        f (float/double): flattening factor of the earth
+ radius
+    Out:
+        (np.array): latitude, longitude and altitude [n,3]
+    '''
+
+    # Compute support parameters
+    f0 = (1 - f) * (1 - f)
+    f1 = 1 - f0
+    f2 = 1 / f0 - 1
+
+    z_div_1_f =  translation[:,2] / (1 - f)
+    x2y2 = np.square(translation[:,0]) + np.square(translation[:,1])
+
+    x2y2z2 = x2y2 + z_div_1_f*z_div_1_f
+    x2y2z2_pow_3_2 = x2y2z2 * np.sqrt(x2y2z2)
+
+    gamma = (x2y2z2_pow_3_2 + a * f2 * z_div_1_f * z_div_1_f) / (x2y2z2_pow_3_2 - a * f1 * x2y2) *  translation[:,2] / np.sqrt(x2y2)
+
+    longitude = np.arctan2(translation[:,1], translation[:,0]) * 180/np.pi
+    latitude = np.arctan(gamma) * 180/np.pi
+    altitude = np.sqrt(1 + np.square(gamma)) * (np.sqrt(x2y2) - a / np.sqrt(1 + f0 * np.square(gamma)))
+
+    return np.concatenate([latitude[:,None], longitude[:,None], altitude[:,None]], axis=1)
+
+def so3_2_axis_angle(so3, degrees=True):
+    ''' Converts the so3 representation to axis_angle
+    Args:
+        so3 (np.array): the rotation matrices [n,3,3]
+        degrees bool: True if angle should be given in degrees
+
+    Out:
+        axis (np array): the rotation axis [n,3]
+        angle (np array): the rotation angles, either in degrees (if degrees=True) or radians [n,]
+    '''
+    rot_vec = R.from_matrix(so3).as_rotvec()
+
+    angle = np.linalg.norm(rot_vec, axis=-1, keepdims=True)
+    axis = rot_vec / angle
+    if degrees:
+        angle = np.degrees(angle)
+
+    return axis, angle
+
+#######################################################################################################################
+
+#
+# Utils for loading in ego and autolabel pose data for session
+#
+
+import datetime
+import pickle
+
+from scipy import spatial, interpolate
+
+# NV_EGO_LWH = [4.084, 1.73, 1.562] # this is the nuscenes measurements
+NV_EGO_LWH = [5.30119, 2.1133, 1.49455] # actual hyperion 8
+
+class PoseInterpolator:
+    ''' Interpolates the poses to the desired time stamps. The translation component is interpolated linearly,
+    while spherical linear interpolation (SLERP) is used for the rotations.
+    https://en.wikipedia.org/wiki/Slerp
+
+    Args:
+        poses (np.array): poses at given timestamps in a se3 representation [n,4,4]
+        timestamps (np.array): timestamps of the known poses [n]
+        ts_target (np.array): timestamps for which the poses will be interpolated [m,1]
+    Out:
+        (np.array): interpolated poses in se3 representation [m,4,4]
+    '''
+    def __init__(self, poses, timestamps):
+
+        self.slerp = spatial.transform.Slerp(timestamps, R.from_matrix(poses[:,:3,:3]))
+        self.f_x = interpolate.interp1d(timestamps, poses[:,0,3])
+        self.f_y = interpolate.interp1d(timestamps, poses[:,1,3])
+        self.f_z = interpolate.interp1d(timestamps, poses[:,2,3])
+
+        self.last_row = np.array([0,0,0,1]).reshape(1,1,-1)
+
+    def interpolate_to_timestamps(self, ts_target):
+        x_interp = self.f_x(ts_target).reshape(-1,1,1)
+        y_interp = self.f_y(ts_target).reshape(-1,1,1)
+        z_interp = self.f_z(ts_target).reshape(-1,1,1)
+        R_interp = self.slerp(ts_target).as_matrix().reshape(-1,3,3)
+
+        t_interp = np.concatenate([x_interp,y_interp,z_interp],axis=-2)
+
+        return np.concatenate((np.concatenate([R_interp,t_interp],axis=-1), np.tile(self.last_row,(R_interp.shape[0],1,1))), axis=1)
+
+def angle_diff(x, y, period=2*np.pi):
+    '''
+    Get the smallest angle difference between 2 angles: the angle from y to x.
+    :param x: angle 1 (B)
+    :param y: angle 2 (B)
+    :param period: periodicity in radians for assessing difference.
+    :return diff: smallest angle difference between to angles (B)
+    '''
+    # calculate angle difference, modulo to [0, 2*pi]
+    diff = (x - y + period / 2) % period - period / 2
+    diff[diff > np.pi] = diff[diff > np.pi] - (2 * np.pi)  # shift (pi, 2*pi] to (-pi, 0]
+    return diff
+
+def load_ego_pose_from_image_meta(images_path, map_tile=None):
+    '''
+    Loads in the SDC pose from the metadata attached to a session image stream.
+
+    :param images_path str: directory of the images/metadata. Should contain *.pkl for each frame and timestamps.npz
+    :param map_tile: Tile object, if given translates the ego trajectory so in the same frame as this map tile.
+    '''
+    frame_meta = sorted(glob.glob(os.path.join(images_path, '*.pkl')))
+    timestamps_pth = os.path.join(images_path, 'timestamps.npz')
+
+    # load in timesteps
+    # ego_t = np.load(timestamps_pth)['frame_t']
+
+    ego_poses = []
+    ego_t = []
+    for meta_file in frame_meta:
+        with open(meta_file, 'rb') as f:
+            cur_meta = pickle.load(f)
+            # ego_poses.append(cur_meta['ego_pose_s'])
+            # ego_poses.append(cur_meta['ego_pose_timestamps'][0])
+            ego_poses.append(cur_meta['ego_pose_e'])
+            ego_t.append(cur_meta['ego_pose_timestamps'][1])
+    ego_poses = np.stack(ego_poses, axis=0)
+    ego_t = np.array(ego_t)
+
+    # pose_sidx = int(frame_meta[0].split('/')[-1].split('.')[0])
+    # pose_eidx = int(frame_meta[-1].split('/')[-1].split('.')[0]) + 1
+    # ego_t = ego_t[pose_sidx:pose_eidx]
+
+    if map_tile is not None:
+        ego_poses[:, :2, -1] += map_tile.trans_offset 
+
+    return ego_poses, ego_t
+
+def check_time_overlap(s0, e0, s1, e1):
+    overlap = (s0 < s1 and e0 > s1) or \
+              (s0 > s1 and s0 < e1) or \
+              (s1 < s0 and e1 > s0) or \
+              (s1 > s0 and s1 < e0)
+    return overlap
+
+
+def load_trajectories(autolabels_path, ego_images_path, lidar_rig_path,
+                        map_tile=None,
+                        frame_range=None,
+                        postprocess=True,
+                        fill_first_n=None,
+                        mine_dups=False,
+                        extra_obj_path=None,
+                        load_ids=None,
+                        crop2valid=True):
+    '''
+    This only loads labeled trajectories that are available at the same section
+    as the ego labels.
+
+    :param autolabels_path str: pkl file to load autolabels from
+    :param ego_images_path str: directory containing image metadata to load ego poses from
+    :param lidar_rig_path str: npz containing lidar2rig transform for ego
+    :param map_tile: Tile object, if given translates the trajectories so in the same frame as this map tile.
+    :param frame_range tuple: If given (start, end) only loads in this frame range (wrt the ego sequence)
+    :param postprocess bool: If true, runs some post-processing to associate track and heuristically remove rotation flips.
+    :param extra_obj_path str: if given, load in an additional trajectory from here and includes in the data
+    :return traj_T: 
+    '''
+    # Load the autolabels
+    with open(autolabels_path, 'rb') as f:
+        labels = pickle.load(f)
+
+    # Load the poses and the timestamps
+    ego_poses, ego_pose_timestamps = load_ego_pose_from_image_meta(ego_images_path)
+    if frame_range is not None:
+        assert frame_range[1] > frame_range[0]
+        assert (frame_range[0] >= 0 and frame_range[0] <= ego_poses.shape[0])
+        ego_poses = ego_poses[frame_range[0]:frame_range[1]]
+        ego_pose_timestamps = ego_pose_timestamps[frame_range[0]:frame_range[1]]
+
+    # if ego_poses are not in rivermark frame, need to take them so can load autolabels
+    ego_poses_rivermark = ego_poses
+    if GLOBAL_BASE_POSE is not GLOBAL_BASE_POSE_RIVERMARK:
+        ego_poses_ecef = np.matmul(GLOBAL_BASE_POSE[np.newaxis], ego_poses)
+        ego_poses_rivermark = np.matmul(np.linalg.inv(GLOBAL_BASE_POSE_RIVERMARK)[np.newaxis], ego_poses_ecef)
+
+    # Load the lidar to rig transformation parameters and timestamps
+    T_lidar_rig = np.load(lidar_rig_path)['T_lidar_rig']
+
+    # first pass to break tracks into contiguous subsequences
+    #       and merge manually given missed associations
+    track_seqs = dict()
+    processed_ids = set()
+    updated_labels = dict()
+    for track_id, track in labels.items():
+        if load_ids is not None and track_id not in load_ids:
+            continue
+        if track_id in processed_ids or track_id in TRACK_REMOVE:
+            # already processed this through association
+            #       or should be removed
+            continue
+        
+        obj_ts = track['3D_bbox'][:,0]
+        if track_id in TRACK_ASSOC:
+            # stack all the data from all associated tracks
+            # NOTE: this assumes TRACK_ASSOC is sorted in temporal order already
+            assoc_data = [labels[assoc_track_id]['3D_bbox'] for assoc_track_id in TRACK_ASSOC[track_id]]
+            # if association is wrong, the tracks may overlap
+            valid_assoc = [not check_time_overlap(obj_ts[0], obj_ts[-1], assoc_label[0,0], assoc_label[-1,0]) for assoc_label in assoc_data]
+            if np.sum(valid_assoc) != len(valid_assoc):
+                print('Invalid associations for track_id %s!!' % (track_id))
+                print('Ignoring: ')
+                print(np.array(TRACK_ASSOC[track_id])[~np.array(valid_assoc)])
+            assoc_data = [assoc_label for aid, assoc_label in enumerate(assoc_data) if valid_assoc[aid]]
+            if len(assoc_data) > 0:
+                assoc_bbox = np.concatenate([track['3D_bbox']] + assoc_data, axis=0)
+                updated_labels[track_id] = {'3D_bbox' : assoc_bbox, 'type' : track['type']}
+                obj_ts = assoc_bbox[:,0]
+                processed_ids.update(TRACK_ASSOC[track_id])
+            else:
+                updated_labels[track_id] = track
+        else:
+            updated_labels[track_id] = track
+
+        if len(obj_ts) < 2:
+            # make sure track is longer than single frame
+            continue
+        track_seqs[track_id] = []
+        # larger than 3 timesteps considered a break, otherwise should be reasonable to interpolate
+        #       should we do even larger?
+        track_break = np.diff(1e-6*obj_ts) > (AUTOLABEL_DT*3 + AUTOLABEL_DT*0.5)
+        seq_sidx = 0
+        for tidx in range(1, obj_ts.shape[0]):
+            if track_break[tidx-1]:
+                track_seqs[track_id].append((seq_sidx, tidx))
+                seq_sidx = tidx
+        track_seqs[track_id].append((seq_sidx, obj_ts.shape[0]))
+        processed_ids.add(track_id)
+
+    # load extra object
+    if extra_obj_path is not None:
+        extra_obj_data = np.load(extra_obj_path)
+        extra_obj_poses = extra_obj_data['poses']
+        extra_obj_timestamps = extra_obj_data['pose_timestamps']
+        extra_obj_lwh = EXTRA_DYN_LWH
+        extra_track = {
+            'poses' : extra_obj_poses,
+            'timestamps' : extra_obj_timestamps,
+            'lwh' : extra_obj_lwh,
+            'type' : 'car'
+        }
+        updated_labels['extra'] = extra_track
+        track_seqs['extra'] = [(0,extra_obj_poses.shape[0])]
+
+    # collect all tracks that overlap with ego data
+    traj_poses = []
+    traj_valid = []
+    traj_lwh = []
+    traj_ids = []
+    for track_id, cur_track_seqs in track_seqs.items():
+        track = updated_labels[track_id]
+        if track_id == 'extra':
+            all_obj_ts = track['timestamps']
+            all_obj_dat = track['poses']
+            obj_lwh = track['lwh']
+        else:
+            all_obj_ts = track['3D_bbox'][:,0]
+            all_obj_dat = track['3D_bbox']
+            obj_lwh = np.median(all_obj_dat[:,4:7], axis=0) # use all timesteps for bbox size
+        # will fill these in as we go through each subseq
+        full_obj_traj = np.ones_like(ego_poses_rivermark)*np.nan
+        obj_valid = np.zeros((full_obj_traj.shape[0]), dtype=bool)
+        for seq_sidx, seq_eidx in cur_track_seqs:
+            obj_ts = all_obj_ts[seq_sidx:seq_eidx]
+            if (obj_ts[0] >= ego_pose_timestamps[0] and obj_ts[0] <= ego_pose_timestamps[-1]) or \
+                 (obj_ts[-1] >= ego_pose_timestamps[0] and obj_ts[-1] <= ego_pose_timestamps[-1]) or \
+                 (obj_ts[0] <= ego_pose_timestamps[0] and obj_ts[-1] >= ego_pose_timestamps[-1]):
+                obj_type = track['type']
+                # if obj_type != 'car':
+                #     continue
+                obj_dat = all_obj_dat[seq_sidx:seq_eidx]
+                # find steps overlapping the ego sequence
+                valid_ts = np.logical_and(obj_ts >= ego_pose_timestamps[0], obj_ts <= ego_pose_timestamps[-1])
+
+                overlap_inds = np.nonzero(valid_ts)[0]
+                if len(overlap_inds) < 2:
+                    continue # need more than 1 frame overlap
+                sidx = np.amin(overlap_inds)
+                eidx = np.amax(overlap_inds)+1
+
+                obj_ts = obj_ts[sidx:eidx]
+                # some poses have the same timestep -- drop these so we can interpolate
+                valid_t = np.diff(obj_ts) > 0
+                valid_t = np.append(valid_t, [True])
+                if not valid_t[0]:
+                    # want to keep the edge times in tact since these surround ego times
+                    valid_t[0] = True
+                    valid_t[1] = False
+                obj_ts = obj_ts[valid_t]
+
+                if track_id == 'extra':
+                    print(obj_ts)
+                    glob_obj_poses = obj_dat[sidx:eidx]
+                    print(glob_obj_poses.shape)
+                    # exit()
+                else:
+                    obj_pos = obj_dat[sidx:eidx,1:4][valid_t]
+                    # print(obj_pos)
+                    # print(obj_dat[sidx:eidx,4:7][valid_t])
+                    obj_rot_eulxyz = obj_dat[sidx:eidx,7:][valid_t]
+                    obj_rot_eulxyz[obj_rot_eulxyz[:,2] < -np.pi, 2] += (2 * np.pi)
+                    obj_rot_eulxyz[obj_rot_eulxyz[:,2] > np.pi, 2] -= (2 * np.pi)
+                    obj_R = R.from_euler('xyz', obj_rot_eulxyz, degrees=False).as_matrix()
+                    # build transformation matrix (pose sequence)
+                    obj_poses = np.repeat(np.eye(4)[np.newaxis], len(obj_ts), axis=0)
+                    obj_poses[:,:3,:3] = obj_R
+                    obj_poses[:,:3,-1] = obj_pos
+
+                    # need to interpolate the ego pose to transform from lidar frame to global
+                    overlap_ego_mask = np.logical_and(ego_pose_timestamps >= obj_ts[0] - 1e6, ego_pose_timestamps <= obj_ts[-1] + 1e6) # add 1 sec around so can interp first/last frames
+                    overlap_ego_t = ego_pose_timestamps[overlap_ego_mask]
+                    overlap_ego_poses = ego_poses_rivermark[overlap_ego_mask]
+                    ego_interp = PoseInterpolator(overlap_ego_poses, overlap_ego_t)
+                    T_rig_global = ego_interp.interpolate_to_timestamps(obj_ts)
+
+                    # transform to rig frame from lidar
+                    rig_obj_poses = np.matmul(T_lidar_rig[np.newaxis], obj_poses)
+
+                    # print('elev')
+                    # print(rig_obj_poses[:,2, 3])
+                    # print('height')
+                    # print(obj_dat[sidx:eidx,6][valid_t])
+
+                    # transform to global frame (w.r.t rivermark) from rig
+                    glob_obj_poses = np.matmul(T_rig_global, rig_obj_poses)
+                    # now to the desired global frame
+                    if GLOBAL_BASE_POSE is not GLOBAL_BASE_POSE_RIVERMARK:
+                        # to ECEF
+                        glob_obj_poses = np.matmul(GLOBAL_BASE_POSE_RIVERMARK[np.newaxis], glob_obj_poses)
+                        # to desired global pose
+                        glob_obj_poses = np.matmul(np.linalg.inv(GLOBAL_BASE_POSE)[np.newaxis], glob_obj_poses)
+
+                if postprocess and track_id != 'extra':
+                    # we're going to collect frames with "correct" orientations
+                    #       by first looking at dynamic frames where can use motion to infer correct orientation, 
+                    #       then using dynamic to determine correctness of static frames.
+                    #       then we can interpolate between all these correct frames.
+                    glob_hvec = glob_obj_poses[:,:2,0] # x-axis
+                    glob_hvec = glob_hvec / np.linalg.norm(glob_hvec, axis=-1, keepdims=True)
+                    glob_yaw = np.arctan2(glob_hvec[:,1], glob_hvec[:,0])
+                    glob_pos = glob_obj_poses[:,:2,3] # 2d
+
+                    # TODO add smoothing to the position to avoid noisy velocities
+                    obj_vel = np.diff(glob_pos[:,:2], axis=0) / np.diff(obj_ts*1e-6)[:,np.newaxis]
+                    obj_vel = np.concatenate([obj_vel, obj_vel[-1:,:]], axis=0)
+                    # is_dynamic = np.median(np.linalg.norm(obj_vel, axis=1)) > 2.0 # m/s
+                    is_dynamic = np.linalg.norm(obj_vel, axis=1) > 2.0 # m/s
+
+                    is_correct_mask = np.zeros((glob_pos.shape[0]), dtype=bool)
+
+                    # dynamic first
+                    if np.sum(is_dynamic) > 0:
+                        dynamic_vel = obj_vel[is_dynamic]
+                        dynamic_yaw = glob_yaw[is_dynamic]
+                        dynamic_vel_norm = np.linalg.norm(dynamic_vel, axis=1, keepdims=True)
+                        vel_dir = dynamic_vel / (dynamic_vel_norm + 1e-9)
+                        head_dir = np.concatenate([np.cos(dynamic_yaw[:,np.newaxis]), np.sin(dynamic_yaw[:,np.newaxis])], axis=1)
+                        vel_head_dot = np.sum(vel_dir * head_dir, axis=1)
+                        dynamic_correct = vel_head_dot > 0
+                        is_correct_mask[is_dynamic] = dynamic_correct
+
+                    # now static, by referencing closest correct dynamic
+                    dynamic_inds = np.nonzero(np.logical_and(is_dynamic, is_correct_mask))[0]
+                    static_inds = np.nonzero(~is_dynamic)[0]
+                    if len(static_inds) > 0:
+                        # if no dynamic frames
+                        # assume correct orientation has the most frequent sign
+                        num_pos = np.sum(glob_yaw[~is_dynamic] >= 0)
+                        num_neg = np.sum(glob_yaw[~is_dynamic] < 0)
+                        for static_ind in static_inds:
+                            if len(dynamic_inds) > 0:
+                                closest_dyn_ind = np.argmin(np.abs(static_ind - dynamic_inds))
+                                dyn_stat_dot = np.sum(glob_hvec[closest_dyn_ind]*glob_hvec[static_ind])
+                                if dyn_stat_dot > 0: # going in same direction
+                                    is_correct_mask[static_ind] = True
+                            else:
+                                is_wrong = glob_yaw[static_ind] >= 0 if num_neg > num_pos else glob_yaw[static_ind] < 0
+                                is_correct_mask[static_ind] = not is_wrong
+
+                    if np.sum(is_correct_mask) > 0:
+                        fix_interp_poses = glob_obj_poses[is_correct_mask]
+                        fix_interp_t = obj_ts[is_correct_mask]
+                        # what if edges are not correct?
+                        if not is_correct_mask[0]:
+                            # just pad first correct to beginning
+                            fix_interp_poses = np.concatenate([fix_interp_poses[0:1], fix_interp_poses], axis=0)
+                            fix_interp_t = np.concatenate([[obj_ts[0]], fix_interp_t], axis=0)
+                        if not is_correct_mask[-1]:
+                            # just pad first correct to beginning
+                            fix_interp_poses = np.concatenate([fix_interp_poses, fix_interp_poses[-1:]], axis=0)
+                            fix_interp_t = np.concatenate([fix_interp_t, [obj_ts[-1]]], axis=0)
+
+                        # now interpolate between correct frames
+                        fix_flip_interp = PoseInterpolator(fix_interp_poses, fix_interp_t)
+                        fixed_rot_poses = fix_flip_interp.interpolate_to_timestamps(obj_ts)
+                        glob_obj_poses[:,:3,:3] = fixed_rot_poses[:,:3,:3] # don't want to update translation                    
+
+                # after processing interpolate to the relevant ego timestamps (upsample from 10Hz to 30Hz)
+                obj_interp = PoseInterpolator(glob_obj_poses, obj_ts)
+                overlap_ego_mask = np.logical_and(ego_pose_timestamps >= obj_ts[0], ego_pose_timestamps <= obj_ts[-1])
+                overlap_ego_t = ego_pose_timestamps[overlap_ego_mask]
+                glob_obj_poses = obj_interp.interpolate_to_timestamps(overlap_ego_t)
+
+                # update full seq information
+                full_obj_traj[overlap_ego_mask] = glob_obj_poses
+                obj_valid[overlap_ego_mask] = True
+
+        if np.sum(obj_valid) > 0:
+            traj_poses.append(full_obj_traj)
+            traj_valid.append(obj_valid)
+            traj_lwh.append(obj_lwh)
+            traj_ids.append(track_id)
+
+    traj_poses = np.stack(traj_poses, axis=0)
+    traj_valid = np.stack(traj_valid, axis=0)
+    traj_lwh = np.stack(traj_lwh, axis=0)
+    traj_ids = np.array(traj_ids)
+
+    if crop2valid:
+        # we interpolated inside ego timestamp maximum, so have to crop a bit
+        val_inds = np.nonzero(np.sum(traj_valid, axis=0) > 0)[0]
+        start_valid = np.amin(val_inds)
+        end_valid = np.amax(val_inds)+1
+        traj_poses = traj_poses[:,start_valid:end_valid]
+        traj_valid = traj_valid[:,start_valid:end_valid]
+        ego_poses = ego_poses[start_valid:end_valid]
+        ego_pose_timestamps = ego_pose_timestamps[start_valid:end_valid]
+
+        print(start_valid)
+        print(end_valid)
+
+    if fill_first_n is not None:
+        # for each trajectory, make sure the first n steps are
+        #       all valid either by interpolation or extrapolation.
+        all_ts = ego_pose_timestamps*1e-6
+        for ai in range(traj_poses.shape[0]):
+            cur_poses = traj_poses[ai]
+            cur_trans = cur_poses[:,:3,3]
+            cur_R = cur_poses[:,:3,:3]
+            cur_valid = traj_valid[ai]
+
+            if np.sum(cur_valid) < 30:
+                # if does't show up for at least a second throughout, don't be extrapolating
+                continue
+
+            first_n_valid = cur_valid[:fill_first_n]
+            if np.sum(~first_n_valid) == 0:
+                continue
+
+            first_n_timestamps = all_ts[:fill_first_n]
+
+            all_val_inds = sorted(np.nonzero(cur_valid)[0])
+            first_val_idx = all_val_inds[0]
+            last_val_idx = all_val_inds[-1]
+
+            # interp steps are those between the first and last valid steps
+            first_n_steps = np.arange(min(fill_first_n, all_ts.shape[0]))
+            interp_steps = np.logical_and(first_n_steps >= first_val_idx, first_n_steps <= last_val_idx)
+            first_n_interp = None
+            if np.sum(interp_steps) > 0:
+                first_n_interp = PoseInterpolator(cur_poses[cur_valid], all_ts[cur_valid])
+                first_n_interp = first_n_interp.interpolate_to_timestamps(first_n_timestamps[interp_steps])
+
+            # extrap fw are those past last valid step (disappear)
+            extrap_fw_steps = first_n_steps > last_val_idx
+            first_n_extrap_fw = None
+            if np.sum(extrap_fw_steps) > 0:
+                if last_val_idx > 0 and cur_valid[last_val_idx-1]:
+                    # need to compute velocity to extrapolate
+                    dt = first_n_timestamps[extrap_fw_steps] - all_ts[last_val_idx]
+                    dt = dt[:,np.newaxis]
+                    last_pos = np.repeat(cur_trans[last_val_idx][np.newaxis], dt.shape[0], axis=0)
+                    # translation
+                    last_lin_vel = (cur_trans[last_val_idx] - cur_trans[last_val_idx-1]) / (all_ts[last_val_idx] - all_ts[last_val_idx-1])
+                    last_lin_vel = np.repeat(last_lin_vel[np.newaxis], dt.shape[0], axis=0)
+                    extrap_trans = last_pos + last_lin_vel*dt
+                    # copy rotation
+                    extrap_R = np.repeat(cur_R[last_val_idx:last_val_idx+1], dt.shape[0], axis=0)
+                    # # extrapolate rotation
+                    # last_delta_R = np.dot(cur_R[last_val_idx], cur_R[last_val_idx-1].T)
+                    # last_delta_rotvec = R.from_matrix(last_delta_R).as_rotvec()
+                    # last_delta_angle = np.linalg.norm(last_delta_rotvec)
+                    # last_delta_axis = last_delta_rotvec / (last_delta_angle + 1e-9)
+                    # last_ang_vel = last_delta_angle / (all_ts[last_val_idx] - all_ts[last_val_idx-1])
+                    # last_ang_vel = np.repeat(last_ang_vel[np.newaxis,np.newaxis], dt.shape[0], axis=0)
+                    # extrap_angle = last_ang_vel*dt
+                    # last_delta_axis = np.repeat(last_delta_axis[np.newaxis], dt.shape[0], axis=0)
+                    # extrap_rotvec = extrap_angle*last_delta_axis
+                    # extrap_delta_R = R.from_rotvec(extrap_rotvec).as_matrix()
+                    # extrap_R = np.matmul(extrap_delta_R, cur_R[last_val_idx:last_val_idx+1])
+
+                    # put together
+                    extrap_poses = np.repeat(np.eye(4)[np.newaxis], dt.shape[0], axis=0)
+                    extrap_poses[:,:3,:3] = extrap_R
+                    extrap_poses[:,:3,3] = extrap_trans
+                    first_n_extrap_fw = extrap_poses
+
+            # extrap bw are those before first valid step (appear)
+            extrap_bw_steps = first_n_steps < first_val_idx
+            first_n_extrap_bw = None
+            if np.sum(extrap_bw_steps) > 0:
+                if first_val_idx < (cur_valid.shape[0]-1) and cur_valid[first_val_idx+1]:
+                    # need to compute velocity to extrapolate
+                    dt = first_n_timestamps[extrap_bw_steps] - all_ts[first_val_idx] # note will be < 0
+                    dt = dt[:,np.newaxis]
+                    first_pos = np.repeat(cur_trans[first_val_idx][np.newaxis], dt.shape[0], axis=0)
+                    # translation
+                    first_lin_vel = (cur_trans[first_val_idx+1] - cur_trans[first_val_idx]) / (all_ts[first_val_idx+1] - all_ts[first_val_idx])
+                    first_lin_vel = np.repeat(first_lin_vel[np.newaxis], dt.shape[0], axis=0)
+                    extrap_trans = first_pos + first_lin_vel*dt
+                    # copy rotation
+                    extrap_R = np.repeat(cur_R[first_val_idx:first_val_idx+1], dt.shape[0], axis=0)
+                    # # extrapolate rotation
+                    # first_delta_R = np.dot(cur_R[first_val_idx+1], cur_R[first_val_idx].T)
+                    # first_delta_rotvec = R.from_matrix(first_delta_R).as_rotvec()
+                    # first_delta_angle = np.linalg.norm(first_delta_rotvec)
+                    # first_delta_axis = first_delta_rotvec / (first_delta_angle+1e-9)
+                    # first_ang_vel = first_delta_angle / (all_ts[first_val_idx+1] - all_ts[first_val_idx])
+                    # first_ang_vel = np.repeat(first_ang_vel[np.newaxis,np.newaxis], dt.shape[0], axis=0)
+                    # extrap_angle = first_ang_vel*dt
+                    # first_delta_axis = np.repeat(first_delta_axis[np.newaxis], dt.shape[0], axis=0)
+                    # extrap_rotvec = extrap_angle*first_delta_axis
+                    # extrap_delta_R = R.from_rotvec(extrap_rotvec).as_matrix()
+                    # extrap_R = np.matmul(extrap_delta_R, cur_R[first_val_idx:first_val_idx+1])
+                    # put together
+                    extrap_poses = np.repeat(np.eye(4)[np.newaxis], dt.shape[0], axis=0)
+                    extrap_poses[:,:3,:3] = extrap_R
+                    extrap_poses[:,:3,3] = extrap_trans
+                    first_n_extrap_bw = extrap_poses
+
+            first_n_poses = cur_poses[:fill_first_n]
+            if first_n_interp is not None:
+                first_n_poses[interp_steps] = first_n_interp
+            if first_n_extrap_fw is not None:
+                first_n_poses[extrap_fw_steps] = first_n_extrap_fw
+            if first_n_extrap_bw is not None:
+                first_n_poses[extrap_bw_steps] = first_n_extrap_bw
+
+            first_n_valid = np.sum(np.isnan(first_n_poses.reshape((first_n_poses.shape[0], 16))), axis=1) == 0
+
+            traj_poses[ai, :fill_first_n] = first_n_poses
+            traj_valid[ai, :fill_first_n] = first_n_valid
+
+            # if traj_ids[ai] == 2588:
+            #     print(extrap_fw_steps)
+            #     print(first_n_extrap_fw)
+            #     exit()
+
+        # based on fill-in can tell if there are duplicated (mis-associated tracks) if they collide
+        if mine_dups:
+            print('Mining possible duplicates...')
+            first_n_xy = traj_poses[:, :fill_first_n, :2, 3]
+            first_n_hvec = traj_poses[:, :fill_first_n, :2, 0]
+            first_n_hvec = first_n_hvec / np.linalg.norm(first_n_hvec, axis=-1, keepdims=True)
+            for ai in range(traj_poses.shape[0]):
+                ai_mask = np.zeros((traj_poses.shape[0]), dtype=bool)
+                ai_mask[ai] = True
+                cur_id = traj_ids[ai]
+                other_ids = traj_ids[~ai_mask]
+
+                traj_tgt = np.concatenate([first_n_xy[ai], first_n_hvec[ai]], axis=1)
+                lw_tgt = traj_lwh[ai, :2]
+                traj_others = np.concatenate([first_n_xy[~ai_mask], first_n_hvec[~ai_mask]], axis=2)
+                lw_others = traj_lwh[~ai_mask, :2]
+
+                # if they collide more than 75% of the time
+                veh_coll = check_single_veh_coll(traj_tgt, lw_tgt, traj_others, lw_others)
+                dup_mask = np.sum(veh_coll, axis=1) > int(0.75*veh_coll.shape[1])
+
+                if np.sum(dup_mask) > 0:
+                    dup_ids = sorted([otid for otid in other_ids[dup_mask].tolist() if otid > cur_id])
+                    dup_ids = [str(otid) for otid in dup_ids]
+                    if len(dup_ids) > 0:
+                        dup_str = ','.join(dup_ids)
+                        dup_str = str(cur_id) + ' : [' + dup_str + '],'
+                        print(dup_str)
+
+    # add ego at index 0
+    ego_poses = ego_poses[np.newaxis]
+    traj_poses = np.concatenate([ego_poses, traj_poses], axis=0)
+    traj_valid = np.concatenate([np.ones((1, ego_poses.shape[1]), dtype=bool), traj_valid], axis=0)
+    traj_lwh = np.concatenate([np.array([NV_EGO_LWH]), traj_lwh], axis=0)
+    traj_ids = np.array(['ego'] + traj_ids.tolist())
+
+    if map_tile is not None:
+        traj_poses[:, :, :2, -1] += map_tile.trans_offset 
+
+    return traj_poses, traj_valid, traj_lwh, ego_pose_timestamps*1e-6, traj_ids
+
+
+if __name__ == '__main__':
+    tile_path = './data/nvidia/nvmaps/92d651e5-21d2-4816-b16d-0feace622aa1/jsv3/92d651e5-21d2-4816-b16d-0feace622aa1/tile/4bd02829-cab6-435d-8ebd-679c96787f8b_json'
+    tile = load_tile(tile_path, layers=['lane_dividers_v1', 'lane_channels_v1'])
+    # convert to nuscenes-like map format if desired
+    nusc_tile = convert_tile_to_nuscenes(tile)
+
+    # dynamic_obj_path = './data/nvidia/dynamic_object_poses.npz'
+    # dyn_obj_data = np.load(dynamic_obj_path)
+    # dyn_obj_poses = dyn_obj_data['poses']
+    # dyn_obj_timestamps = dyn_obj_data['pose_timestamps']
+
+    # print(dyn_obj_poses.shape)
+
+    # # debug_viz_vid(tile.segments, 'dyn_obj', 
+    # #                 comp_out_path='./out/dev_gtc_demo/dev_preprocess',
+    # #                 poses=dyn_obj_poses[np.newaxis],
+    # #                 poses_valid=np.ones((1, dyn_obj_poses.shape[0])),
+    # #                 poses_lwh=np.array([[5.087, 2.307, 1.856]]),
+    # #                 # pose_ids=traj_ids,
+    # #                 subsamp=1,
+    # #                 fps=30)
+
+    # # TODO option to just load specific ids?
+
+    # # rivermark
+    # autolabels_path = './data/nvidia/endeavor/labels/autolabels.pkl'
+    # ego_images_path = './data/nvidia/ego_session/processed/44093/images/image_00'
+    # lidar_rig_path = './data/nvidia/endeavor/poses/T_lidar_rig.npz'
+    # frame_range = (510, 880)
+    # traj_poses, traj_valid, traj_lwh, traj_t, traj_ids = load_trajectories(autolabels_path, ego_images_path, lidar_rig_path,
+    #                                                                         frame_range=frame_range,
+    #                                                                         postprocess=True,
+    #                                                                         # extra_obj_path=dynamic_obj_path,
+    #                                                                         # load_ids=['ego', 'extra'],
+    #                                                                         crop2valid=False,
+    #                                                                         fill_first_n=160)
+
+    # print(traj_poses.shape)
+    # debug_viz_vid(tile.segments, 'rivermark_fill', 
+    #                 comp_out_path='./out/dev_gtc_demo/dev_preprocess',
+    #                 poses=traj_poses,
+    #                 poses_valid=traj_valid,
+    #                 poses_lwh=traj_lwh,
+    #                 pose_ids=traj_ids,
+    #                 subsamp=1,
+    #                 fps=30)
+
+    # exit()
+
+
+    autolabels_path = './data/nvidia/endeavor/labels/autolabels.pkl'
+    ego_images_path = './data/nvidia/endeavor/images/image_00'
+    lidar_rig_path = './data/nvidia/endeavor/poses/T_lidar_rig.npz'
+
+    # this just filters which frames to process, can be None
+    # frame_range = (3170, 3590) # (3200, 3545) # (0, 900), (1000, 2400), None
+    # frame_range = (60, 3590)
+    frame_range = (3000, 3590)
+    # frame_range = (2000, 2400)
+    # frame_range = (570, 870) # merge
+    # frame_range = (240, 660) # merge
+    # frame_range = (1440, 1860) # merge
+
+    # process on the original nvmap
+    traj_poses, traj_valid, traj_lwh, traj_t, traj_ids = load_trajectories(autolabels_path, ego_images_path, lidar_rig_path,
+                                                                            frame_range=frame_range,
+                                                                            postprocess=True,
+                                                                            mine_dups=False,
+                                                                            fill_first_n=300)
+    print(traj_poses.shape)
+    debug_viz_vid(tile.segments, 'extrap_proc_ids_sframe_00003000_eframe_00003300',
+                    comp_out_path='./out/dev_gtc_demo/dev_preprocess',
+                    poses=traj_poses,
+                    poses_valid=traj_valid,
+                    poses_lwh=traj_lwh,
+                    pose_ids=traj_ids,
+                    subsamp=1,
+                    fps=30)
+
+    # # process with the nuscenes map version
+    # # (the only difference is the coordinate system is offset such that the origin is at bottom left)
+    # traj_poses, traj_valid, traj_lwh, traj_t, traj_ids = load_trajectories(autolabels_path, ego_images_path, lidar_rig_path,
+    #                                                                     frame_range=frame_range,
+    #                                                                     map_tile=nusc_tile,
+    #                                                                     postprocess=True)
+    # debug_viz_vid(nusc_tile.segments, 'processed_sframe_00003200_eframe_00003545_nusc', 
+    #                 comp_out_path='./out/gtc_demo/dev_preprocess',
+    #                 poses=traj_poses,
+    #                 poses_valid=traj_valid,
+    #                 poses_lwh=traj_lwh,
+    #                 pose_ids=traj_ids,
+    #                 subsamp=3,
+    #                 fps=10)
+
+    #
+    # output single pose for Amlan
+    #
+
+    # single_frame_idx = 3500
+    # last_pose = traj_poses[:,single_frame_idx:single_frame_idx+1]
+    # last_valid = traj_valid[:,single_frame_idx]
+    # last_step_valid_poses = last_pose[last_valid]
+    # print(last_step_valid_poses.shape)
+    # last_step_lwh = traj_lwh[last_valid]
+
+    # debug_viz_segs(nusc_tile.segments, 'endeavor_nusc_step%d' % (single_frame_idx), poses=last_step_valid_poses, poses_valid=traj_valid[:,single_frame_idx:single_frame_idx+1][last_valid], poses_lwh=last_step_lwh)
+
+    # np.savez('./out/dev_nvmap/endeavor_poses_frame%06d.npz' % (single_frame_idx),
+    #             poses=last_step_valid_poses,
+    #             lwh=last_step_lwh)
+    # exit()
+
+    #
+    # Output GPS trajectories for Jeremy
+    #
+
+    # #  convert ego poses back to global ECEF coordinate system
+    # N, T, _, _ = traj_poses.shape
+    # ecef_traj_poses = np.matmul(GLOBAL_BASE_POSE[np.newaxis,np.newaxis], traj_poses)
+    # print(ecef_traj_poses.shape)
+    # # convert to GPS
+    # gps_traj_poses = ecef_2_lat_lng_alt(ecef_traj_poses.reshape((N*T, 4, 4)), earth_model='WGS84')
+    # lat_lng_alt, orientation_axis, orientation_angle = gps_traj_poses
+
+    # save_path = os.path.join('./out/dev_nvmap/endeavor_trajectory_track_ids.npz')
+    # out_dict = {
+    #     'timestamps' : traj_t,
+    #     'track_ids' : traj_ids,
+    #     'ecef_poses' : ecef_traj_poses,
+    #     'pose_valid' : traj_valid,
+    #     'gps_lat_lng_alt'  : lat_lng_alt.reshape((N, T, 3)),
+    #     'gps_orientation_axis' : orientation_axis.reshape((N, T, 3)),
+    #     'gps_orientation_angle_degrees' : orientation_angle.reshape((N, T, 1))
+    # }
+    # for k, v in out_dict.items():
+    #     print(k)
+    #     print(v.shape)
+    #     # if k != 'timestamps':
+    #     #     print(np.sum(np.isnan(v), axis=1))
+    # np.savez(save_path, **out_dict)
+
+    # exit()
+                                                                            
+    # debug_viz_vid(tile.segments, 'gt_sframe_00003200_eframe_00003545', 
+    #                 comp_out_path='./out/gtc_demo/dev_preprocess',
+    #                 poses=traj_poses,
+    #                 poses_valid=traj_valid,
+    #                 poses_lwh=traj_lwh,
+    #                 subsamp=3,
+    #                 fps=10)
diff --git a/src/trajdata/dataset_specific/drivesim/test.py b/src/trajdata/dataset_specific/drivesim/test.py
new file mode 100644
index 0000000..ee6b58e
--- /dev/null
+++ b/src/trajdata/dataset_specific/drivesim/test.py
@@ -0,0 +1,79 @@
+import numpy as np
+from typing import Dict
+import trajdata.dataset_specific.drivesim.nvmap_utils as nvutils
+
+# This is from the <geoReference> tag from the .xodr file for Endeavor.
+LATLONALT_ORIGIN_ENDEAVOR = np.array([[37.37852062996696, -121.9596180846297, 0.0]])
+
+
+def convert_to_DS(poses: np.ndarray, track_ids:list,fps:float):
+    """_summary_
+
+    Args:
+        poses (np.ndarray): x, y, h states of each agent at each time, relative to the ego vehicle's state at time t=0. (N, T, 3)
+    """
+    # wgs84 = coutils.LocalToWGS84((0, 0, 0), (37.37852062996696, -121.9596180846297, 0.0))
+    # wgs84_2 = coutils.ECEFtoWGS84(nvutils.GLOBAL_BASE_POSE_ENDEAVOR[:3, -1])
+    
+    world_from_map_ft = nvutils.lat_lng_alt_2_ecef(
+        LATLONALT_ORIGIN_ENDEAVOR,
+        np.array([[1, 0, 0]]),
+        np.array([[0]])
+    )
+    
+    N, T = poses.shape[:2]
+    x = poses[..., 0]
+    y = poses[..., 1]
+    heading = poses[..., 2]
+
+    c = np.cos(heading)
+    s = np.sin(heading)
+    T_mat = np.tile(np.eye(4), (N, T, 1, 1))
+    T_mat[..., 0, 0] = c
+    T_mat[..., 0, 1] = -s
+    T_mat[..., 1, 0] = s
+    T_mat[..., 1, 1] = c
+    T_mat[..., 0, 3] = x
+    T_mat[..., 1, 3] = y
+    # TODO: Some height for ray-casting down to road?
+    # T_mat[..., 2, 3] = 0
+
+    ecef_traj_poses = np.matmul(world_from_map_ft[:, np.newaxis], T_mat)
+    gps_traj_poses = nvutils.ecef_2_lat_lng_alt(ecef_traj_poses.reshape((N*T, 4, 4)), earth_model='WGS84')
+    lat_lng_alt, orientation_axis, orientation_angle = gps_traj_poses
+    breakpoint()
+    out_dict = {
+        'timestamps' : np.linspace(0, T/fps, T),
+        'track_ids' : np.array(track_ids),
+        # 'bbox_lwh' : np.array([[4.387, 1.907, 1.656], [4.387, 1.907, 1.656]]),
+        # 'ecef_poses' : ecef_traj_poses,
+        'pose_valid' : np.ones((ecef_traj_poses.shape[0], ecef_traj_poses.shape[1]), dtype=bool),
+        'gps_lat_lng_alt'  : lat_lng_alt.reshape((N, T, 3)),
+        'gps_orientation_axis' : orientation_axis.reshape((N, T, 3)),
+        'gps_orientation_angle_degrees' : orientation_angle.reshape((N, T, 1))
+    }
+
+    return out_dict
+
+
+def main():
+    poses = np.zeros((2, 315, 3))
+    poses[0, :, 0] = np.linspace(-505, -519, 315)
+    poses[0, :, 1] = np.linspace(-1019, -866, 315)
+    poses[0, :, 2] = np.linspace(np.pi/2, 5*np.pi/8, 315)
+
+    fps = 30
+    out_dict = convert_to_DS(poses,["ego","769"],fps)
+
+    # with np.load("/home/bivanovic/projects/drivesim-ov/source/extensions/omni.drivesim.dl_traffic_model/data/example_trajectories.npz") as data:
+    #     out_dict["gps_lat_lng_alt"][1] = data["gps_lat_lng_alt"][1]
+    #     out_dict["gps_orientation_axis"][1] = data["gps_orientation_axis"][1]
+    #     out_dict["gps_orientation_angle_degrees"][1] = data["gps_orientation_angle_degrees"][1]
+
+    np.savez(
+        "/home/bivanovic/projects/drivesim-ov/source/extensions/omni.drivesim.dl_traffic_model/data/test.npz",
+        **out_dict
+    )
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
diff --git a/src/trajdata/dataset_specific/nuplan/nuplan_dataset.py b/src/trajdata/dataset_specific/nuplan/nuplan_dataset.py
index 1c4df3f..08f9977 100644
--- a/src/trajdata/dataset_specific/nuplan/nuplan_dataset.py
+++ b/src/trajdata/dataset_specific/nuplan/nuplan_dataset.py
@@ -360,38 +360,6 @@ class NuplanDataset(RawDataset):
 
         return agent_list, agent_presence
 
-    def cache_map(
-        self,
-        map_name: str,
-        cache_path: Path,
-        map_cache_class: Type[SceneCache],
-        map_params: Dict[str, Any],
-    ) -> None:
-        nuplan_map: NuPlanMap = map_factory.get_maps_api(
-            map_root=str(self.metadata.data_dir.parent / "maps"),
-            map_version=nuplan_utils.NUPLAN_MAP_VERSION,
-            map_name=nuplan_utils.NUPLAN_FULL_MAP_NAME_DICT[map_name],
-        )
-
-        # Loading all layer geometries.
-        nuplan_map.initialize_all_layers()
-
-        # This df has the normal lane_connectors with additional boundary information,
-        # which we want to use, however the default index is not the lane_connector_fid,
-        # although it is a 1:1 mapping so we instead create another index with the
-        # lane_connector_fids as the key and the resulting integer indices as the value.
-        lane_connector_fids: pd.Series = nuplan_map._vector_map[
-            "gen_lane_connectors_scaled_width_polygons"
-        ]["lane_connector_fid"]
-        lane_connector_idxs: pd.Series = pd.Series(
-            index=lane_connector_fids, data=range(len(lane_connector_fids))
-        )
-
-        vector_map = VectorMap(map_id=f"{self.name}:{map_name}")
-        nuplan_utils.populate_vector_map(vector_map, nuplan_map, lane_connector_idxs)
-
-        map_cache_class.finalize_and_cache_map(cache_path, vector_map, map_params)
-
     def cache_maps(
         self,
         cache_path: Path,
@@ -406,4 +374,45 @@ class NuplanDataset(RawDataset):
             desc=f"Caching {self.name} Maps at {map_params['px_per_m']:.2f} px/m",
             position=0,
         ):
-            self.cache_map(map_name, cache_path, map_cache_class, map_params)
+            cache_map(
+                map_root=str(self.metadata.data_dir.parent / "maps"),
+                env_name=self.name,
+                map_name=map_name,
+                cache_path=cache_path,
+                map_cache_class=map_cache_class,
+                map_params=map_params,
+            )                    
+
+
+def cache_map(
+        map_root: str, 
+        env_name: str, 
+        map_name: str,
+        cache_path: Path,
+        map_cache_class: Type[SceneCache],
+        map_params: Dict[str, Any],              
+) -> None:
+    nuplan_map: NuPlanMap = map_factory.get_maps_api(
+        map_root=map_root,
+        map_version=nuplan_utils.NUPLAN_MAP_VERSION,
+        map_name=nuplan_utils.NUPLAN_FULL_MAP_NAME_DICT[map_name],
+    )
+
+    # Loading all layer geometries.
+    nuplan_map.initialize_all_layers()
+
+    # This df has the normal lane_connectors with additional boundary information,
+    # which we want to use, however the default index is not the lane_connector_fid,
+    # although it is a 1:1 mapping so we instead create another index with the
+    # lane_connector_fids as the key and the resulting integer indices as the value.
+    lane_connector_fids: pd.Series = nuplan_map._vector_map[
+        "gen_lane_connectors_scaled_width_polygons"
+    ]["lane_connector_fid"]
+    lane_connector_idxs: pd.Series = pd.Series(
+        index=lane_connector_fids, data=range(len(lane_connector_fids))
+    )
+
+    vector_map = VectorMap(map_id=f"{env_name}:{map_name}")
+    nuplan_utils.populate_vector_map(vector_map, nuplan_map, lane_connector_idxs)
+
+    map_cache_class.finalize_and_cache_map(cache_path, vector_map, map_params)    
\ No newline at end of file
diff --git a/src/trajdata/dataset_specific/nuplan/nuplan_utils.py b/src/trajdata/dataset_specific/nuplan/nuplan_utils.py
index 5007ace..dea194c 100644
--- a/src/trajdata/dataset_specific/nuplan/nuplan_utils.py
+++ b/src/trajdata/dataset_specific/nuplan/nuplan_utils.py
@@ -189,6 +189,7 @@ class NuPlanObject:
 
 
 def nuplan_type_to_unified_type(nuplan_type: str) -> AgentType:
+    # TODO (pkarkus) map traffic cones, barriers to static; generic_object to pedestrian
     if nuplan_type == "pedestrian":
         return AgentType.PEDESTRIAN
     elif nuplan_type == "bicycle":
@@ -327,12 +328,20 @@ def populate_vector_map(
         # The right boundary of Lane A has Lane A to its left.
         boundary_connectivity_dict[right_boundary_id]["left"].append(lane_id)
 
+        # Find road areas that this lane intersects for faster lane-based lookup later.
+        intersect_filt = nuplan_map._vector_map["drivable_area"].intersects(lane_info["geometry"])
+        isnear_filt = (nuplan_map._vector_map["drivable_area"].distance(lane_info["geometry"]) < 3.)
+        road_area_ids = set(nuplan_map._vector_map["drivable_area"][intersect_filt | isnear_filt]["fid"].values)
+        if not road_area_ids:
+            print (f"Warning: no road lane associated with lane {lane_id}")
+
         # "partial" because we aren't adding lane connectivity until later.
         partial_new_lane = RoadLane(
             id=lane_id,
             center=Polyline(center_pts),
             left_edge=Polyline(left_pts),
             right_edge=Polyline(right_pts),
+            road_area_ids=road_area_ids,
         )
         vector_map.add_map_element(partial_new_lane)
         overall_pbar.update()
diff --git a/src/trajdata/dataset_specific/scene_records.py b/src/trajdata/dataset_specific/scene_records.py
index 68bd5b9..0c6f29b 100644
--- a/src/trajdata/dataset_specific/scene_records.py
+++ b/src/trajdata/dataset_specific/scene_records.py
@@ -28,6 +28,12 @@ class NuscSceneRecord(NamedTuple):
     desc: str
     data_idx: int
 
+class CarlaSceneRecord(NamedTuple):
+    name: str
+    location: str
+    length: str
+    data_idx: int
+
 
 class LyftSceneRecord(NamedTuple):
     name: str
@@ -48,3 +54,11 @@ class NuPlanSceneRecord(NamedTuple):
     split: str
     # desc: str
     data_idx: int
+
+class DrivesimSceneRecord(NamedTuple):
+    name: str
+    location: str
+    length: str
+    split: str
+    # desc: str
+    data_idx: int
diff --git a/src/trajdata/maps/map_api.py b/src/trajdata/maps/map_api.py
index 36a1245..e6e4a6d 100644
--- a/src/trajdata/maps/map_api.py
+++ b/src/trajdata/maps/map_api.py
@@ -1,6 +1,6 @@
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Optional
+from typing import TYPE_CHECKING, Optional, Union
 
 if TYPE_CHECKING:
     from trajdata.maps.map_kdtree import MapElementKDTree
@@ -15,9 +15,20 @@ from trajdata.utils import map_utils
 
 
 class MapAPI:
-    def __init__(self, unified_cache_path: Path) -> None:
+    def __init__(self, unified_cache_path: Path, keep_in_memory: bool = False) -> None:
+        """A simple interface for loading trajdata's vector maps which does not require
+        instantiation of a `UnifiedDataset` object.
+
+        Args:
+            unified_cache_path (Path): Path to trajdata's local cache on disk.
+            keep_in_memory (bool): Whether loaded maps should be stored
+            in memory (memoized) for later re-use. For most cases (e.g., batched dataloading),
+            this is a good idea. However, this can cause rapid memory usage growth for some
+            datasets (e.g., Waymo) and it can be better to disable this. Defaults to False.
+        """
         self.unified_cache_path: Path = unified_cache_path
         self.maps: Dict[str, VectorMap] = dict()
+        self._keep_in_memory = keep_in_memory
 
     def get_map(
         self, map_id: str, scene_cache: Optional[SceneCache] = None, **kwargs
@@ -25,22 +36,43 @@ class MapAPI:
         if map_id not in self.maps:
             env_name, map_name = map_id.split(":")
             env_maps_path: Path = self.unified_cache_path / env_name / "maps"
-            stored_vec_map: VectorizedMap = map_utils.load_vector_map(
-                env_maps_path / f"{map_name}.pb"
-            )
+            vec_map_path: Path = env_maps_path / f"{map_name}.pb"
+
+            if not Path.exists(vec_map_path):
+                if self.data_dirs is None:
+                    raise ValueError(
+                        f"There is no cached map at {vec_map_path} and there was no " + 
+                        "`data_dirs` provided to rebuild cache.")
+
+                # Rebuild maps by creating a dummy dataset object.
+                # TODO(pkarkus) We need support for rebuilding map files only, without creating dataset and building agent data.
+                from trajdata.dataset import UnifiedDataset
+                dataset = UnifiedDataset(
+                    desired_data=[env_name],
+                    rebuild_maps=True,
+                    data_dirs=self.data_dirs,
+                    cache_location=self.unified_cache_path,
+                    verbose=True,
+                )
+                # Hopefully we successfully created map cache.
+
+            stored_vec_map: VectorizedMap = map_utils.load_vector_map(vec_map_path)
 
             vec_map: VectorMap = VectorMap.from_proto(stored_vec_map, **kwargs)
             vec_map.search_kdtrees: Dict[
                 str, MapElementKDTree
             ] = map_utils.load_kdtrees(env_maps_path / f"{map_name}_kdtrees.dill")
 
-            self.maps[map_id] = vec_map
+            if self._keep_in_memory:
+                self.maps[map_id] = vec_map
+        else:
+            vec_map = self.maps[map_id]
 
         if scene_cache is not None:
-            self.maps[map_id].associate_scene_data(
+            vec_map.associate_scene_data(
                 scene_cache.get_traffic_light_status_dict(
                     kwargs.get("desired_dt", None)
                 )
             )
 
-        return self.maps[map_id]
+        return vec_map
diff --git a/src/trajdata/maps/map_kdtree.py b/src/trajdata/maps/map_kdtree.py
index 59abdc2..a978a90 100644
--- a/src/trajdata/maps/map_kdtree.py
+++ b/src/trajdata/maps/map_kdtree.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 from collections import defaultdict
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Iterator
 
 if TYPE_CHECKING:
     from trajdata.maps.vec_map import VectorMap
@@ -43,8 +43,7 @@ class MapElementKDTree:
             total=len(vector_map),
             disable=not verbose,
         ):
-            result = self._extract_points(map_elem)
-            if result is not None:
+            for result in self._extract_points_and_metadata(map_elem):
                 points, extras = result
                 polyline_inds.extend([len(polylines)] * points.shape[0])
 
@@ -54,6 +53,7 @@ class MapElementKDTree:
 
                 for k, v in extras.items():
                     metadata[k].append(v)
+                metadata["map_elem_id"].append(np.array([map_elem.id]))
 
         points = np.concatenate(polylines, axis=0)
         polyline_inds = np.array(polyline_inds)
@@ -64,7 +64,7 @@ class MapElementKDTree:
 
     def _extract_points_and_metadata(
         self, map_element: MapElement
-    ) -> Optional[Tuple[np.ndarray, dict[str, np.ndarray]]]:
+    ) -> Iterator[Tuple[np.ndarray, dict[str, np.ndarray]]]:
         """Defines the coordinates we want to store in the KDTree for a MapElement.
         Args:
             map_element (MapElement): the MapElement to store in the KDTree.
@@ -116,16 +116,16 @@ class LaneCenterKDTree(MapElementKDTree):
         self.max_segment_len = max_segment_len
         super().__init__(vector_map)
 
-    def _extract_points(self, map_element: MapElement) -> Optional[np.ndarray]:
+    def _extract_points_and_metadata(
+        self, map_element: MapElement
+    ) -> Iterator[Tuple[np.ndarray, dict[str, np.ndarray]]]:
         if map_element.elem_type == MapElementType.ROAD_LANE:
             pts: Polyline = map_element.center
             if self.max_segment_len is not None:
                 pts = pts.interpolate(max_dist=self.max_segment_len)
 
             # We only want to store xyz in the kdtree, not heading.
-            return pts.xyz, {"heading": pts.h}
-        else:
-            return None
+            yield pts.xyz, {"heading": pts.h}
 
     def current_lane_inds(
         self,
@@ -181,3 +181,42 @@ class LaneCenterKDTree(MapElementKDTree):
         min_costs = [np.min(costs[lane_inds == ind]) for ind in unique_lane_inds]
 
         return unique_lane_inds[np.argsort(min_costs)]
+
+
+class RoadAreaKDTree(MapElementKDTree):
+    """KDTree for road area polygons.
+    The polygons may have holes. We will simply store points along both the 
+    exterior_polygon and all interior_holes. Finding a nearest point in this KDTree will
+    correspond to finding any 
+    """
+
+    def __init__(
+        self, vector_map: VectorMap, max_segment_len: Optional[float] = None
+    ) -> None:
+        """
+        Args:
+            vec_map: the VectorizedMap object to build the KDTree for
+            max_segment_len (float, optional): if specified, we will insert extra points into the KDTree
+                such that all polyline segments are shorter then max_segment_len.
+        """
+        self.max_segment_len = max_segment_len
+        super().__init__(vector_map)
+
+    def _extract_points_and_metadata(
+        self, map_element: MapElement
+    ) -> Iterator[Tuple[np.ndarray, dict[str, np.ndarray]]]:
+        if map_element.elem_type == MapElementType.ROAD_AREA:
+            # Exterior polygon
+            pts: Polyline = map_element.exterior_polygon
+            if self.max_segment_len is not None:
+                pts = pts.interpolate(max_dist=self.max_segment_len)
+            # We only want to store xyz in the kdtree, not heading.
+            yield pts.xyz, {"exterior": np.array([True])}
+
+            # Interior holes
+            for pts in map_element.interior_holes:
+                if self.max_segment_len is not None:
+                    pts = pts.interpolate(max_dist=self.max_segment_len)
+                # We only want to store xyz in the kdtree, not heading.
+                yield pts.xyz, {"exterior": np.array([False])}
+
diff --git a/src/trajdata/maps/vec_map.py b/src/trajdata/maps/vec_map.py
index 95b299d..52a225f 100644
--- a/src/trajdata/maps/vec_map.py
+++ b/src/trajdata/maps/vec_map.py
@@ -25,9 +25,10 @@ import matplotlib.pyplot as plt
 import numpy as np
 from matplotlib.axes import Axes
 from tqdm import tqdm
+from shapely.geometry import Polygon
 
 import trajdata.proto.vectorized_map_pb2 as map_proto
-from trajdata.maps.map_kdtree import LaneCenterKDTree
+from trajdata.maps.map_kdtree import LaneCenterKDTree, RoadAreaKDTree
 from trajdata.maps.traffic_light_status import TrafficLightStatus
 from trajdata.maps.vec_map_elements import (
     MapElement,
@@ -52,6 +53,7 @@ class VectorMap:
     )
     search_kdtrees: Optional[Dict[MapElementType, MapElementKDTree]] = None
     traffic_light_status: Optional[Dict[Tuple[int, int], TrafficLightStatus]] = None
+    online_metadict: Optional[Dict[Tuple[str, int], Dict]] = None
 
     def __post_init__(self) -> None:
         self.env_name, self.map_name = self.map_id.split(":")
@@ -60,11 +62,16 @@ class VectorMap:
         if MapElementType.ROAD_LANE in self.elements:
             self.lanes = list(self.elements[MapElementType.ROAD_LANE].values())
 
+        # self._road_area_polygons: Dict[str, Polygon] = {}
+
     def add_map_element(self, map_elem: MapElement) -> None:
         self.elements[map_elem.elem_type][map_elem.id] = map_elem
 
     def compute_search_indices(self) -> None:
-        self.search_kdtrees = {MapElementType.ROAD_LANE: LaneCenterKDTree(self)}
+        self.search_kdtrees = {
+            MapElementType.ROAD_LANE: LaneCenterKDTree(self),
+            # MapElementType.ROAD_AREA: RoadAreaKDTree(self),
+        }
 
     def iter_elems(self) -> Iterator[MapElement]:
         for elems_dict in self.elements.values():
@@ -101,6 +108,9 @@ class VectorMap:
             new_lane.adjacent_lanes_right.extend(
                 [lane_id.encode() for lane_id in road_lane.adj_lanes_right]
             )
+            # new_lane.road_area_ids.extend(
+            #     [road_area_id.encode() for road_area_id in road_lane.road_area_ids]
+            # )
 
     def _write_road_areas(
         self, vectorized_map: map_proto.VectorizedMap, shifted_origin: np.ndarray
@@ -250,6 +260,9 @@ class VectorMap:
                 prev_lanes: Set[str] = set(
                     [iden.decode() for iden in road_lane_obj.entry_lanes]
                 )
+                # road_area_ids: Set[str] = set(
+                #     [iden.decode() for iden in road_lane_obj.road_area_ids]
+                # )
 
                 # Double-using the connectivity attributes for lane IDs now (will
                 # replace them with Lane objects after all Lane objects have been created).
@@ -262,6 +275,7 @@ class VectorMap:
                     adj_lanes_right,
                     next_lanes,
                     prev_lanes,
+                    # road_area_ids=road_area_ids,
                 )
                 map_elem_dict[MapElementType.ROAD_LANE][elem_id] = curr_lane
 
@@ -369,6 +383,33 @@ class VectorMap:
             self.lanes[idx] for idx in lane_kdtree.polyline_inds_in_range(xyz, dist)
         ]
 
+    def get_road_areas_within(self, xyz: np.ndarray, dist: float) -> List[RoadArea]:
+        road_area_kdtree: RoadAreaKDTree = self.search_kdtrees[MapElementType.ROAD_AREA]
+        polyline_inds = road_area_kdtree.polyline_inds_in_range(xyz, dist)
+        element_ids = set([
+            road_area_kdtree.metadata["map_elem_id"][ind] for ind in polyline_inds
+        ])
+        if MapElementType.ROAD_AREA not in self.elements:
+            raise ValueError(
+                "Road areas are not loaded. Use map_api.get_map(..., incl_road_areas=True)."
+            )
+        return [
+            self.elements[MapElementType.ROAD_AREA][id] for id in element_ids
+        ]
+    
+    def get_road_area_polygon_2d(self, id: str) -> Polygon:
+        if id not in self._road_area_polygons:
+            road_area: RoadArea = self.elements[MapElementType.ROAD_AREA][id]
+            road_area_polygon = Polygon(
+                shell=[(pt[0], pt[1]) for pt in road_area.exterior_polygon.points],
+                holes=[
+                    [(pt[0], pt[1]) for pt in polyline.points]
+                    for polyline in road_area.interior_holes
+                ]
+            )
+            self._road_area_polygons[id] = road_area_polygon
+        return self._road_area_polygons[id]
+
     def get_traffic_light_status(
         self, lane_id: str, scene_ts: int
     ) -> TrafficLightStatus:
@@ -380,6 +421,15 @@ class VectorMap:
             else TrafficLightStatus.NO_DATA
         )
 
+    def get_online_metadict(
+        self, lane_id: str, scene_ts: int = 0
+    ) -> Dict:
+        return (
+            self.online_metadict[(str(lane_id), scene_ts)]
+            if self.online_metadict is not None
+            else {}
+        )
+
     def rasterize(
         self, resolution: float = 2, **kwargs
     ) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
diff --git a/src/trajdata/maps/vec_map_elements.py b/src/trajdata/maps/vec_map_elements.py
index fd7a9f1..d1e34b6 100644
--- a/src/trajdata/maps/vec_map_elements.py
+++ b/src/trajdata/maps/vec_map_elements.py
@@ -1,10 +1,11 @@
 from dataclasses import dataclass, field
 from enum import IntEnum
-from typing import List, Optional, Set
+from typing import List, Optional, Set, Union
 
 import numpy as np
 
 from trajdata.utils import map_utils
+from trajdata.utils.arr_utils import angle_wrap
 
 
 class MapElementType(IntEnum):
@@ -47,6 +48,15 @@ class Polyline:
     def xyz(self) -> np.ndarray:
         return self.points[..., :3]
 
+    @property
+    def xyh(self) -> np.ndarray:
+        if self.has_heading:
+            return self.points[..., (0, 1, 3)]
+        else:
+            raise ValueError(
+                f"This Polyline only has {self.points.shape[-1]} coordinates, expected 4."
+            )
+
     @property
     def xyzh(self) -> np.ndarray:
         if self.has_heading:
@@ -67,14 +77,17 @@ class Polyline:
             map_utils.interpolate(self.points, num_pts=num_pts, max_dist=max_dist)
         )
 
-    def project_onto(self, xyz_or_xyzh: np.ndarray) -> np.ndarray:
+    def project_onto(self, xyz_or_xyzh: np.ndarray, return_index: bool = False) -> Union[np.ndarray, List]:
         """Project the given points onto this Polyline.
 
         Args:
             xyzh (np.ndarray): Points to project, of shape (M, D)
+            return_indices (bool): Return the index of starting point of the line segment
+                on which the projected points lies on.
 
         Returns:
             np.ndarray: The projected points, of shape (M, D)
+            np.ndarray: The index of previous polyline points if return_indices == True. 
 
         Note:
             D = 4 if this Polyline has headings, otherwise D = 3
@@ -94,7 +107,8 @@ class Polyline:
         dot_products: np.ndarray = (point_seg_diffs * line_seg_diffs).sum(
             axis=-1, keepdims=True
         )
-        norms: np.ndarray = np.linalg.norm(line_seg_diffs, axis=-1, keepdims=True) ** 2
+        # norms: np.ndarray = np.linalg.norm(line_seg_diffs, axis=-1, keepdims=True) ** 2
+        norms: np.ndarray = np.square(line_seg_diffs).sum(axis=-1, keepdims=True)
 
         # Clip ensures that the projected point stays within the line segment boundaries.
         projs: np.ndarray = (
@@ -102,20 +116,114 @@ class Polyline:
         )
 
         # 2. Find the nearest projections to the original points.
-        closest_proj_idxs: int = np.linalg.norm(xyz - projs, axis=-1).argmin(axis=-1)
+        # We have nan values when two consecutive points are equal. This will never be
+        # the closest projection point, so we replace nans with a large number.
+        point_to_proj_dist = np.nan_to_num(np.linalg.norm(xyz - projs, axis=-1), nan=1e6)
+        closest_proj_idxs: int = point_to_proj_dist.argmin(axis=-1)
+
+        proj_points = projs[range(xyz.shape[0]), closest_proj_idxs]
 
         if self.has_heading:
             # Adding in the heading of the corresponding p0 point (which makes
             # sense as p0 to p1 is a line => same heading along it).
-            return np.concatenate(
+            proj_points = np.concatenate(
                 [
-                    projs[range(xyz.shape[0]), closest_proj_idxs],
+                    proj_points,
                     np.expand_dims(self.points[closest_proj_idxs, -1], axis=-1),
                 ],
                 axis=-1,
             )
+        
+        if return_index:
+            return proj_points, closest_proj_idxs
         else:
-            return projs[range(xyz.shape[0]), closest_proj_idxs]
+            return proj_points
+
+    def distance_to_point(self, xyz: np.ndarray):
+        assert xyz.ndim == 2
+        xyz_proj = self.project_onto(xyz)
+        return np.linalg.norm(xyz[..., :3] - xyz_proj[..., :3], axis=-1)
+
+    def get_length(self):
+        # TODO(pkarkus) we could store cummulative distances to speed this up
+        dists = np.linalg.norm(self.xyz[1:, :3] - self.xyz[:-1, :3], axis=-1)
+        length = dists.sum()
+        return length
+
+
+    def get_length_from(self, start_ind: np.ndarray):
+        # TODO(pkarkus) we could store cummulative distances to speed this up
+        assert start_ind.ndim == 1
+        dists = np.linalg.norm(self.xyz[1:, :3] - self.xyz[:-1, :3], axis=-1)
+        length_upto = np.cumsum(np.pad(dists, (1, 0)))
+        length_from = length_upto[-1][None] - length_upto[start_ind]
+        return length_from
+
+
+    def traverse_along(self, dist: np.ndarray, start_ind: Optional[np.ndarray] = None) -> np.ndarray:
+        """
+        Interpolated endpoint of traversing `dist` distance along polyline from a starting point. 
+
+        Returns nan if the end point is not inside the polyline.
+        TODO(pkarkus) we could store cummulative distances to speed this up
+
+        Args:
+            dist (np.ndarray): distances, any shape [...]
+            start_ind (np.ndarray): index of point along polyline to calcualte distance from. 
+                Optional. Shape must match dist. [...]
+        
+        Returns:
+            endpoint_xyzh (np.ndarray): points along polyline `dist` distance from the 
+                starting point. Nan if endpoint would require extrapolation. [..., 4]
+
+        """
+        assert self.has_heading
+
+        # Add up distances from beginning of polyline
+        segment_lens = np.linalg.norm(self.xyz[1:] - self.xyz[:-1], axis=-1)   # n-1
+        cum_len = np.pad(np.cumsum(segment_lens, axis=0), (1, 0))  # n
+
+        # Increase dist with the length of lane up to start_ind
+        if start_ind is not None:
+            assert start_ind.ndim == dist.ndim    
+            dist = dist + cum_len[start_ind]
+        
+        # Find the first index where cummulative length is larger or equal than `dist`
+        inds = np.searchsorted(cum_len, dist, side='right')
+        # Invalidate inds == 0 and inds == len(cum_len), which means endpoint is outside the polyline.
+        invalid = np.logical_or(inds == 0, inds == len(cum_len))
+        # Replace invalid indices so we can easily carry out computation below, and invalidate output eventually.
+        inds[invalid] = 1
+
+        # Remaining distance from last point
+        remaining_dist = dist - cum_len[inds-1]
+
+        # Invalidate negative remaining dist (this should only happen when dist < 0)
+        invalid = np.logical_or(invalid, remaining_dist < 0.)
+        
+        # Interpolate between the previous and next points.
+        segment_vect_xyz = self.xyz[inds] - self.xyz[inds-1]
+        segment_len = np.linalg.norm(segment_vect_xyz, axis=-1)
+        assert (segment_len > 0.).all(), "Polyline segment has zero length"
+
+        proportion = (remaining_dist / segment_len)
+        endpoint_xyz = segment_vect_xyz * proportion[..., np.newaxis] + self.xyz[inds]
+        endpoint_h = angle_wrap(angle_wrap(self.h[inds] - self.h[inds-1]) * proportion + self.h[inds-1])
+        endpoint_xyzh = np.concatenate((endpoint_xyz, endpoint_h[..., np.newaxis]), axis=-1)
+
+        # Invalidate dummy output
+        endpoint_xyzh[invalid] = np.nan
+
+        return endpoint_xyzh
+
+    def concatenate_with(self, other: "Polyline") -> "Polyline":
+        return self.concatenate([self, other])
+
+    @staticmethod
+    def concatenate(polylines: List["Polyline"]) -> "Polyline":
+        # Assumes no overlap between consecutive polylines, i.e. next lane starts after current lane ends.
+        points = np.concatenate([polyline.points for polyline in polylines], axis=0)
+        return Polyline(points)
 
 
 @dataclass
@@ -132,6 +240,7 @@ class RoadLane(MapElement):
     adj_lanes_right: Set[str] = field(default_factory=lambda: set())
     next_lanes: Set[str] = field(default_factory=lambda: set())
     prev_lanes: Set[str] = field(default_factory=lambda: set())
+    road_area_ids: Set[str] = field(default_factory=lambda: set())
     elem_type: MapElementType = MapElementType.ROAD_LANE
 
     def __post_init__(self) -> None:
@@ -150,7 +259,35 @@ class RoadLane(MapElement):
     @property
     def reachable_lanes(self) -> Set[str]:
         return self.adj_lanes_left | self.adj_lanes_right | self.next_lanes
-
+    
+
+    def combine_next(self, next_lane):
+        assert next_lane.id in self.next_lanes
+        self.next_lanes.remove(next_lane.id)
+        self.next_lanes = self.next_lanes.union(next_lane.next_lanes)
+        self.center = self.center.concatenate_with(next_lane.center)
+        if self.left_edge is not None and next_lane.left_edge is not None:
+            self.left_edge = self.left_edge.concatenate_with(next_lane.left_edge)
+        if self.right_edge is not None and next_lane.right_edge is not None:
+            self.right_edge = self.right_edge.concatenate_with(next_lane.right_edge)
+        self.adj_lanes_right = self.adj_lanes_right.union(next_lane.adj_lanes_right)
+        self.adj_lanes_left = self.adj_lanes_left.union(next_lane.adj_lanes_left)
+        self.road_area_ids = self.road_area_ids.union(next_lane.road_area_ids)
+    
+    def combine_prev(self,prev_lane):
+        assert prev_lane.id in self.prev_lanes
+        self.prev_lanes.remove(prev_lane.id)
+        self.prev_lanes = self.prev_lanes.union(prev_lane.prev_lanes)
+        self.center = prev_lane.center.concatenate_with(self.center)
+        if self.left_edge is not None and prev_lane.left_edge is not None:
+            self.left_edge = prev_lane.left_edge.concatenate_with(self.left_edge)
+        if self.right_edge is not None and prev_lane.right_edge is not None:
+            self.right_edge = prev_lane.right_edge.concatenate_with(self.right_edge)
+        self.adj_lanes_right = self.adj_lanes_right.union(prev_lane.adj_lanes_right)
+        self.adj_lanes_left = self.adj_lanes_left.union(prev_lane.adj_lanes_left)
+        self.road_area_ids = self.road_area_ids.union(prev_lane.road_area_ids)
+        
+        
 
 @dataclass
 class RoadArea(MapElement):
diff --git a/src/trajdata/maps/vec_map_remote.py b/src/trajdata/maps/vec_map_remote.py
new file mode 100644
index 0000000..77f2375
--- /dev/null
+++ b/src/trajdata/maps/vec_map_remote.py
@@ -0,0 +1,582 @@
+from __future__ import annotations
+
+from typing import TYPE_CHECKING
+
+if TYPE_CHECKING:
+    from trajdata.maps.map_kdtree import MapElementKDTree, LaneCenterKDTree
+
+from collections import defaultdict
+from dataclasses import dataclass, field
+from math import ceil
+from typing import (
+    DefaultDict,
+    Dict,
+    Iterator,
+    List,
+    Optional,
+    Set,
+    Tuple,
+    Union,
+    overload,
+)
+
+import matplotlib as mpl
+import matplotlib.pyplot as plt
+import numpy as np
+from matplotlib.axes import Axes
+from tqdm import tqdm
+
+import trajdata.proto.vectorized_map_pb2 as map_proto
+from trajdata.maps.map_kdtree import LaneCenterKDTree
+from trajdata.maps.traffic_light_status import TrafficLightStatus
+from trajdata.maps.vec_map_elements import (
+    MapElement,
+    MapElementType,
+    PedCrosswalk,
+    PedWalkway,
+    Polyline,
+    RoadArea,
+    RoadLane,
+)
+from trajdata.utils import map_utils, raster_utils
+
+
+@dataclass(repr=False)
+class VectorMap:
+    map_id: str
+    extent: Optional[
+        np.ndarray
+    ] = None  # extent is [min_x, min_y, min_z, max_x, max_y, max_z]
+    elements: DefaultDict[MapElementType, Dict[str, MapElement]] = field(
+        default_factory=lambda: defaultdict(dict)
+    )
+    search_kdtrees: Optional[Dict[MapElementType, MapElementKDTree]] = None
+    traffic_light_status: Optional[Dict[Tuple[int, int], TrafficLightStatus]] = None
+
+    def __post_init__(self) -> None:
+        self.env_name, self.map_name = self.map_id.split(":")
+
+        self.lanes: Optional[List[RoadLane]] = None
+        if MapElementType.ROAD_LANE in self.elements:
+            self.lanes = list(self.elements[MapElementType.ROAD_LANE].values())
+
+    def add_map_element(self, map_elem: MapElement) -> None:
+        self.elements[map_elem.elem_type][map_elem.id] = map_elem
+
+    def compute_search_indices(self) -> None:
+        self.search_kdtrees = {MapElementType.ROAD_LANE: LaneCenterKDTree(self)}
+
+    def iter_elems(self) -> Iterator[MapElement]:
+        for elems_dict in self.elements.values():
+            for elem in elems_dict.values():
+                yield elem
+
+    def get_road_lane(self, lane_id: str) -> RoadLane:
+        return self.elements[MapElementType.ROAD_LANE][lane_id]
+
+    def __len__(self) -> int:
+        return sum(len(elems_dict) for elems_dict in self.elements.values())
+
+    def _write_road_lanes(
+        self, vectorized_map: map_proto.VectorizedMap, shifted_origin: np.ndarray
+    ) -> None:
+        road_lane: RoadLane
+        for elem_id, road_lane in self.elements[MapElementType.ROAD_LANE].items():
+            new_element: map_proto.MapElement = vectorized_map.elements.add()
+            new_element.id = elem_id.encode()
+
+            new_lane: map_proto.RoadLane = new_element.road_lane
+            map_utils.populate_lane_polylines(new_lane, road_lane, shifted_origin)
+
+            new_lane.entry_lanes.extend(
+                [lane_id.encode() for lane_id in road_lane.prev_lanes]
+            )
+            new_lane.exit_lanes.extend(
+                [lane_id.encode() for lane_id in road_lane.next_lanes]
+            )
+
+            new_lane.adjacent_lanes_left.extend(
+                [lane_id.encode() for lane_id in road_lane.adj_lanes_left]
+            )
+            new_lane.adjacent_lanes_right.extend(
+                [lane_id.encode() for lane_id in road_lane.adj_lanes_right]
+            )
+
+    def _write_road_areas(
+        self, vectorized_map: map_proto.VectorizedMap, shifted_origin: np.ndarray
+    ) -> None:
+        road_area: RoadArea
+        for elem_id, road_area in self.elements[MapElementType.ROAD_AREA].items():
+            new_element: map_proto.MapElement = vectorized_map.elements.add()
+            new_element.id = elem_id.encode()
+
+            new_area: map_proto.RoadArea = new_element.road_area
+            map_utils.populate_polygon(
+                new_area.exterior_polygon,
+                road_area.exterior_polygon.xyz,
+                shifted_origin,
+            )
+
+            hole: Polyline
+            for hole in road_area.interior_holes:
+                new_hole: map_proto.Polyline = new_area.interior_holes.add()
+                map_utils.populate_polygon(
+                    new_hole,
+                    hole.xyz,
+                    shifted_origin,
+                )
+
+    def _write_ped_crosswalks(
+        self, vectorized_map: map_proto.VectorizedMap, shifted_origin: np.ndarray
+    ) -> None:
+        ped_crosswalk: PedCrosswalk
+        for elem_id, ped_crosswalk in self.elements[
+            MapElementType.PED_CROSSWALK
+        ].items():
+            new_element: map_proto.MapElement = vectorized_map.elements.add()
+            new_element.id = elem_id.encode()
+
+            new_crosswalk: map_proto.PedCrosswalk = new_element.ped_crosswalk
+            map_utils.populate_polygon(
+                new_crosswalk.polygon,
+                ped_crosswalk.polygon.xyz,
+                shifted_origin,
+            )
+
+    def _write_ped_walkways(
+        self, vectorized_map: map_proto.VectorizedMap, shifted_origin: np.ndarray
+    ) -> None:
+        ped_walkway: PedWalkway
+        for elem_id, ped_walkway in self.elements[MapElementType.PED_WALKWAY].items():
+            new_element: map_proto.MapElement = vectorized_map.elements.add()
+            new_element.id = elem_id.encode()
+
+            new_walkway: map_proto.PedWalkway = new_element.ped_walkway
+            map_utils.populate_polygon(
+                new_walkway.polygon,
+                ped_walkway.polygon.xyz,
+                shifted_origin,
+            )
+
+    def to_proto(self) -> map_proto.VectorizedMap:
+        output_map = map_proto.VectorizedMap()
+        output_map.name = self.map_id
+
+        (
+            output_map.min_pt.x,
+            output_map.min_pt.y,
+            output_map.min_pt.z,
+            output_map.max_pt.x,
+            output_map.max_pt.y,
+            output_map.max_pt.z,
+        ) = self.extent
+
+        shifted_origin: np.ndarray = self.extent[:3]
+        (
+            output_map.shifted_origin.x,
+            output_map.shifted_origin.y,
+            output_map.shifted_origin.z,
+        ) = shifted_origin
+
+        # Populating the elements in the vectorized map protobuf.
+        self._write_road_lanes(output_map, shifted_origin)
+        self._write_road_areas(output_map, shifted_origin)
+        self._write_ped_crosswalks(output_map, shifted_origin)
+        self._write_ped_walkways(output_map, shifted_origin)
+
+        return output_map
+
+    @classmethod
+    def from_proto(cls, vec_map: map_proto.VectorizedMap, **kwargs):
+        # Options for which map elements to include.
+        incl_road_lanes: bool = kwargs.get("incl_road_lanes", True)
+        incl_road_areas: bool = kwargs.get("incl_road_areas", False)
+        incl_ped_crosswalks: bool = kwargs.get("incl_ped_crosswalks", False)
+        incl_ped_walkways: bool = kwargs.get("incl_ped_walkways", False)
+
+        # Add any map offset in case the map origin was shifted for storage efficiency.
+        shifted_origin: np.ndarray = np.array(
+            [
+                vec_map.shifted_origin.x,
+                vec_map.shifted_origin.y,
+                vec_map.shifted_origin.z,
+                0.0,  # Some polylines also have heading so we're adding
+                # this (zero) coordinate to account for that.
+            ]
+        )
+
+        map_elem_dict: Dict[str, Dict[str, MapElement]] = defaultdict(dict)
+
+        map_elem: MapElement
+        for map_elem in vec_map.elements:
+            elem_id: str = map_elem.id.decode()
+            if incl_road_lanes and map_elem.HasField("road_lane"):
+                road_lane_obj: map_proto.RoadLane = map_elem.road_lane
+
+                center_pl: Polyline = Polyline(
+                    map_utils.proto_to_np(road_lane_obj.center) + shifted_origin
+                )
+
+                # We do not care for the heading of the left and right edges
+                # (only the center matters).
+                left_pl: Optional[Polyline] = None
+                if road_lane_obj.HasField("left_boundary"):
+                    left_pl = Polyline(
+                        map_utils.proto_to_np(
+                            road_lane_obj.left_boundary, incl_heading=False
+                        )
+                        + shifted_origin[:3]
+                    )
+
+                right_pl: Optional[Polyline] = None
+                if road_lane_obj.HasField("right_boundary"):
+                    right_pl = Polyline(
+                        map_utils.proto_to_np(
+                            road_lane_obj.right_boundary, incl_heading=False
+                        )
+                        + shifted_origin[:3]
+                    )
+
+                adj_lanes_left: Set[str] = set(
+                    [iden.decode() for iden in road_lane_obj.adjacent_lanes_left]
+                )
+                adj_lanes_right: Set[str] = set(
+                    [iden.decode() for iden in road_lane_obj.adjacent_lanes_right]
+                )
+
+                next_lanes: Set[str] = set(
+                    [iden.decode() for iden in road_lane_obj.exit_lanes]
+                )
+                prev_lanes: Set[str] = set(
+                    [iden.decode() for iden in road_lane_obj.entry_lanes]
+                )
+
+                # Double-using the connectivity attributes for lane IDs now (will
+                # replace them with Lane objects after all Lane objects have been created).
+                curr_lane = RoadLane(
+                    elem_id,
+                    center_pl,
+                    left_pl,
+                    right_pl,
+                    adj_lanes_left,
+                    adj_lanes_right,
+                    next_lanes,
+                    prev_lanes,
+                )
+                map_elem_dict[MapElementType.ROAD_LANE][elem_id] = curr_lane
+
+            elif incl_road_areas and map_elem.HasField("road_area"):
+                road_area_obj: map_proto.RoadArea = map_elem.road_area
+
+                exterior: Polyline = Polyline(
+                    map_utils.proto_to_np(
+                        road_area_obj.exterior_polygon, incl_heading=False
+                    )
+                    + shifted_origin[:3]
+                )
+
+                interior_holes: List[Polyline] = list()
+                interior_hole: map_proto.Polyline
+                for interior_hole in road_area_obj.interior_holes:
+                    interior_holes.append(
+                        Polyline(
+                            map_utils.proto_to_np(interior_hole, incl_heading=False)
+                            + shifted_origin[:3]
+                        )
+                    )
+
+                curr_area = RoadArea(elem_id, exterior, interior_holes)
+                map_elem_dict[MapElementType.ROAD_AREA][elem_id] = curr_area
+
+            elif incl_ped_crosswalks and map_elem.HasField("ped_crosswalk"):
+                ped_crosswalk_obj: map_proto.PedCrosswalk = map_elem.ped_crosswalk
+
+                polygon_vertices: Polyline = Polyline(
+                    map_utils.proto_to_np(ped_crosswalk_obj.polygon, incl_heading=False)
+                    + shifted_origin[:3]
+                )
+
+                curr_area = PedCrosswalk(elem_id, polygon_vertices)
+                map_elem_dict[MapElementType.PED_CROSSWALK][elem_id] = curr_area
+
+            elif incl_ped_walkways and map_elem.HasField("ped_walkway"):
+                ped_walkway_obj: map_proto.PedCrosswalk = map_elem.ped_walkway
+
+                polygon_vertices: Polyline = Polyline(
+                    map_utils.proto_to_np(ped_walkway_obj.polygon, incl_heading=False)
+                    + shifted_origin[:3]
+                )
+
+                curr_area = PedWalkway(elem_id, polygon_vertices)
+                map_elem_dict[MapElementType.PED_WALKWAY][elem_id] = curr_area
+
+        return cls(
+            map_id=vec_map.name,
+            extent=np.array(
+                [
+                    vec_map.min_pt.x,
+                    vec_map.min_pt.y,
+                    vec_map.min_pt.z,
+                    vec_map.max_pt.x,
+                    vec_map.max_pt.y,
+                    vec_map.max_pt.z,
+                ]
+            ),
+            elements=map_elem_dict,
+            search_kdtrees=None,
+            traffic_light_status=None,
+        )
+
+    def associate_scene_data(
+        self, traffic_light_status_dict: Dict[Tuple[int, int], TrafficLightStatus]
+    ) -> None:
+        """Associates vector map with scene-specific data like traffic light information"""
+        self.traffic_light_status = traffic_light_status_dict
+
+    def get_current_lane(
+        self,
+        xyzh: np.ndarray,
+        max_dist: float = 2.0,
+        max_heading_error: float = np.pi / 8,
+    ) -> List[RoadLane]:
+        """
+        Args:
+            xyzh (np.ndarray): 3d position and heading of agent in world coordinates
+
+        Returns:
+            List[RoadLane]: List of possible road lanes that agent could be on
+        """
+        lane_kdtree: LaneCenterKDTree = self.search_kdtrees[MapElementType.ROAD_LANE]
+        return [
+            self.lanes[idx]
+            for idx in lane_kdtree.current_lane_inds(xyzh, max_dist, max_heading_error)
+        ]
+
+    def get_closest_lane(self, xyz: np.ndarray) -> RoadLane:
+        lane_kdtree: LaneCenterKDTree = self.search_kdtrees[MapElementType.ROAD_LANE]
+        return self.lanes[lane_kdtree.closest_polyline_ind(xyz)]
+
+    def get_closest_unique_lanes(self, xyz_vec: np.ndarray) -> List[RoadLane]:
+        assert xyz_vec.ndim == 2  # xyz_vec is assumed to be (*, 3)
+        lane_kdtree: LaneCenterKDTree = self.search_kdtrees[MapElementType.ROAD_LANE]
+        closest_inds = lane_kdtree.closest_polyline_ind(xyz_vec)
+        unique_inds = np.unique(closest_inds)
+        return [self.lanes[ind] for ind in unique_inds]
+
+    def get_lanes_within(self, xyz: np.ndarray, dist: float) -> List[RoadLane]:
+        lane_kdtree: LaneCenterKDTree = self.search_kdtrees[MapElementType.ROAD_LANE]
+        return [
+            self.lanes[idx] for idx in lane_kdtree.polyline_inds_in_range(xyz, dist)
+        ]
+
+    def get_traffic_light_status(
+        self, lane_id: str, scene_ts: int
+    ) -> TrafficLightStatus:
+        return (
+            self.traffic_light_status.get(
+                (int(lane_id), scene_ts), TrafficLightStatus.NO_DATA
+            )
+            if self.traffic_light_status is not None
+            else TrafficLightStatus.NO_DATA
+        )
+
+    def rasterize(
+        self, resolution: float = 2, **kwargs
+    ) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
+        """Renders this vector map at the specified resolution.
+
+        Args:
+            resolution (float): The rasterized image's resolution in pixels per meter.
+
+        Returns:
+            np.ndarray: The rasterized RGB image.
+        """
+        return_tf_mat: bool = kwargs.get("return_tf_mat", False)
+        incl_centerlines: bool = kwargs.get("incl_centerlines", True)
+        incl_lane_edges: bool = kwargs.get("incl_lane_edges", True)
+        incl_lane_area: bool = kwargs.get("incl_lane_area", True)
+
+        scene_ts: Optional[int] = kwargs.get("scene_ts", None)
+
+        # (255, 102, 99) also looks nice.
+        center_color: Tuple[int, int, int] = kwargs.get("center_color", (129, 51, 255))
+        # (86, 203, 249) also looks nice.
+        edge_color: Tuple[int, int, int] = kwargs.get("edge_color", (118, 185, 0))
+        # (191, 215, 234) also looks nice.
+        area_color: Tuple[int, int, int] = kwargs.get("area_color", (214, 232, 181))
+
+        min_x, min_y, _, max_x, max_y, _ = self.extent
+
+        world_center_m: Tuple[float, float] = (
+            (max_x + min_x) / 2,
+            (max_y + min_y) / 2,
+        )
+
+        raster_size_x: int = ceil((max_x - min_x) * resolution)
+        raster_size_y: int = ceil((max_y - min_y) * resolution)
+
+        raster_from_local: np.ndarray = np.array(
+            [
+                [resolution, 0, raster_size_x / 2],
+                [0, resolution, raster_size_y / 2],
+                [0, 0, 1],
+            ]
+        )
+
+        # Compute pose from its position and rotation.
+        pose_from_world: np.ndarray = np.array(
+            [
+                [1, 0, -world_center_m[0]],
+                [0, 1, -world_center_m[1]],
+                [0, 0, 1],
+            ]
+        )
+
+        raster_from_world: np.ndarray = raster_from_local @ pose_from_world
+
+        map_img: np.ndarray = np.zeros(
+            shape=(raster_size_y, raster_size_x, 3), dtype=np.uint8
+        )
+
+        lane_edges: List[np.ndarray] = list()
+        centerlines: List[np.ndarray] = list()
+        lane: RoadLane
+        for lane in tqdm(
+            self.elements[MapElementType.ROAD_LANE].values(),
+            desc=f"Rasterizing Map at {resolution:.2f} px/m",
+            leave=False,
+        ):
+            centerlines.append(
+                raster_utils.world_to_subpixel(
+                    lane.center.points[:, :2], raster_from_world
+                )
+            )
+            if lane.left_edge is not None and lane.right_edge is not None:
+                left_pts: np.ndarray = lane.left_edge.points[:, :2]
+                right_pts: np.ndarray = lane.right_edge.points[:, :2]
+
+                lane_edges += [
+                    raster_utils.world_to_subpixel(left_pts, raster_from_world),
+                    raster_utils.world_to_subpixel(right_pts, raster_from_world),
+                ]
+
+                lane_color = area_color
+                status = self.get_traffic_light_status(lane.id, scene_ts)
+                if status == TrafficLightStatus.GREEN:
+                    lane_color = [0, 200, 0]
+                elif status == TrafficLightStatus.RED:
+                    lane_color = [200, 0, 0]
+                elif status == TrafficLightStatus.UNKNOWN:
+                    lane_color = [150, 150, 0]
+
+                # Drawing lane areas. Need to do per loop because doing it all at once can
+                # create lots of wonky holes in the image.
+                # See https://stackoverflow.com/questions/69768620/cv2-fillpoly-failing-for-intersecting-polygons
+                if incl_lane_area:
+                    lane_area: np.ndarray = np.concatenate(
+                        [left_pts, right_pts[::-1]], axis=0
+                    )
+                    raster_utils.rasterize_world_polygon(
+                        lane_area,
+                        map_img,
+                        raster_from_world,
+                        color=lane_color,
+                    )
+
+        # Drawing all lane edge lines at the same time.
+        if incl_lane_edges:
+            raster_utils.cv2_draw_polylines(lane_edges, map_img, color=edge_color)
+
+        # Drawing centerlines last (on top of everything else).
+        if incl_centerlines:
+            raster_utils.cv2_draw_polylines(centerlines, map_img, color=center_color)
+
+        if return_tf_mat:
+            return map_img.astype(float) / 255, raster_from_world
+        else:
+            return map_img.astype(float) / 255
+
+    @overload
+    def visualize_lane_graph(
+        self,
+        origin_lane: RoadLane,
+        num_hops: int,
+        **kwargs,
+    ) -> Axes:
+        ...
+
+    @overload
+    def visualize_lane_graph(self, origin_lane: str, num_hops: int, **kwargs) -> Axes:
+        ...
+
+    @overload
+    def visualize_lane_graph(self, origin_lane: int, num_hops: int, **kwargs) -> Axes:
+        ...
+
+    def visualize_lane_graph(
+        self, origin_lane: Union[RoadLane, str, int], num_hops: int, **kwargs
+    ) -> Axes:
+        ax = kwargs.get("ax", None)
+        if ax is None:
+            fig, ax = plt.subplots()
+
+        origin: str
+        if isinstance(origin_lane, RoadLane):
+            origin = origin_lane.id
+        elif isinstance(origin_lane, str):
+            origin = origin_lane
+        elif isinstance(origin_lane, int):
+            origin = self.lanes[origin_lane].id
+
+        viridis = mpl.colormaps[kwargs.get("cmap", "rainbow")].resampled(num_hops + 1)
+
+        already_seen: Set[str] = set()
+        lanes_to_plot: List[Tuple[str, int]] = [(origin, 0)]
+
+        if kwargs.get("legend", True):
+            ax.scatter([], [], label=f"Lane Endpoints", color="k")
+            ax.plot([], [], label=f"Origin Lane ({origin})", color=viridis(0))
+            for h in range(1, num_hops + 1):
+                ax.plot(
+                    [],
+                    [],
+                    label=f"{h} Lane{'s' if h > 1 else ''} Away",
+                    color=viridis(h),
+                )
+
+        raster_from_world = kwargs.get("raster_from_world", None)
+        while len(lanes_to_plot) > 0:
+            lane_id, curr_hops = lanes_to_plot.pop(0)
+            already_seen.add(lane_id)
+            lane: RoadLane = self.get_road_lane(lane_id)
+
+            center: np.ndarray = lane.center.points[..., :2]
+            first_pt_heading: float = lane.center.points[0, -1]
+            mdpt: np.ndarray = lane.center.midpoint[..., :2]
+
+            if raster_from_world is not None:
+                center = map_utils.transform_points(center, raster_from_world)
+                mdpt = map_utils.transform_points(mdpt[None, :], raster_from_world)[0]
+
+            ax.plot(center[:, 0], center[:, 1], color=viridis(curr_hops))
+            ax.scatter(center[[0, -1], 0], center[[0, -1], 1], color=viridis(curr_hops))
+            ax.quiver(
+                [center[0, 0]],
+                [center[0, 1]],
+                [np.cos(first_pt_heading)],
+                [np.sin(first_pt_heading)],
+                color=viridis(curr_hops),
+            )
+            ax.text(mdpt[0], mdpt[1], s=lane_id)
+
+            if curr_hops < num_hops:
+                lanes_to_plot += [
+                    (l, curr_hops + 1)
+                    for l in lane.reachable_lanes
+                    if l not in already_seen
+                ]
+
+        if kwargs.get("legend", True):
+            ax.legend(loc="best", frameon=True)
+
+        return ax
\ No newline at end of file
diff --git a/src/trajdata/proto/vectorized_map.proto b/src/trajdata/proto/vectorized_map.proto
index e1cd502..8f68281 100644
--- a/src/trajdata/proto/vectorized_map.proto
+++ b/src/trajdata/proto/vectorized_map.proto
@@ -39,14 +39,14 @@ message Point {
 }
 
 message Polyline {
-  // Position deltas in millimeters. The origin is an arbitrary location.
-  // From https://github.com/woven-planet/l5kit/blob/master/l5kit/l5kit/data/proto/road_network.proto#L446
+  // Position deltas in 10^-5 meters. The origin is an arbitrary location.
+  // Inspired by https://github.com/woven-planet/l5kit/blob/master/l5kit/l5kit/data/proto/road_network.proto#L446
   // The delta for the first point is just its coordinates tuple, i.e. it is a "delta" from
   // the origin. For subsequent points, this field stores the difference between the point's
   // coordinates and the previous point's coordinates. This is for representation efficiency.
-  repeated sint32 dx_mm = 1;
-  repeated sint32 dy_mm = 2;
-  repeated sint32 dz_mm = 3;
+  repeated sint64 dx_mm = 1;
+  repeated sint64 dy_mm = 2;
+  repeated sint64 dz_mm = 3;
   repeated double h_rad = 4;
 }
 
@@ -74,6 +74,9 @@ message RoadLane {
   // A list of neighbors to the right of this lane. Neighbor lanes
   // include only adjacent lanes going the same direction.
   repeated bytes adjacent_lanes_right = 7;
+
+  // A list of associated road area ids.
+  repeated bytes road_area_ids = 8;
 }
 
 message RoadArea {
diff --git a/src/trajdata/proto/vectorized_map_pb2.py b/src/trajdata/proto/vectorized_map_pb2.py
index 7352109..c5910d9 100644
--- a/src/trajdata/proto/vectorized_map_pb2.py
+++ b/src/trajdata/proto/vectorized_map_pb2.py
@@ -1,136 +1,545 @@
 # -*- coding: utf-8 -*-
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: vectorized_map.proto
-"""Generated protocol buffer code."""
+
 from google.protobuf import descriptor as _descriptor
-from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import message as _message
 from google.protobuf import reflection as _reflection
 from google.protobuf import symbol_database as _symbol_database
-
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
-    b'\n\x14vectorized_map.proto\x12\x08trajdata"\xb0\x01\n\rVectorizedMap\x12\x0c\n\x04name\x18\x01 \x01(\t\x12&\n\x08\x65lements\x18\x02 \x03(\x0b\x32\x14.trajdata.MapElement\x12\x1f\n\x06max_pt\x18\x03 \x01(\x0b\x32\x0f.trajdata.Point\x12\x1f\n\x06min_pt\x18\x04 \x01(\x0b\x32\x0f.trajdata.Point\x12\'\n\x0eshifted_origin\x18\x05 \x01(\x0b\x32\x0f.trajdata.Point"\xd8\x01\n\nMapElement\x12\n\n\x02id\x18\x01 \x01(\x0c\x12\'\n\troad_lane\x18\x02 \x01(\x0b\x32\x12.trajdata.RoadLaneH\x00\x12\'\n\troad_area\x18\x03 \x01(\x0b\x32\x12.trajdata.RoadAreaH\x00\x12/\n\rped_crosswalk\x18\x04 \x01(\x0b\x32\x16.trajdata.PedCrosswalkH\x00\x12+\n\x0bped_walkway\x18\x05 \x01(\x0b\x32\x14.trajdata.PedWalkwayH\x00\x42\x0e\n\x0c\x65lement_data"(\n\x05Point\x12\t\n\x01x\x18\x01 \x01(\x01\x12\t\n\x01y\x18\x02 \x01(\x01\x12\t\n\x01z\x18\x03 \x01(\x01"F\n\x08Polyline\x12\r\n\x05\x64x_mm\x18\x01 \x03(\x11\x12\r\n\x05\x64y_mm\x18\x02 \x03(\x11\x12\r\n\x05\x64z_mm\x18\x03 \x03(\x11\x12\r\n\x05h_rad\x18\x04 \x03(\x01"\x98\x02\n\x08RoadLane\x12"\n\x06\x63\x65nter\x18\x01 \x01(\x0b\x32\x12.trajdata.Polyline\x12.\n\rleft_boundary\x18\x02 \x01(\x0b\x32\x12.trajdata.PolylineH\x00\x88\x01\x01\x12/\n\x0eright_boundary\x18\x03 \x01(\x0b\x32\x12.trajdata.PolylineH\x01\x88\x01\x01\x12\x13\n\x0b\x65ntry_lanes\x18\x04 \x03(\x0c\x12\x12\n\nexit_lanes\x18\x05 \x03(\x0c\x12\x1b\n\x13\x61\x64jacent_lanes_left\x18\x06 \x03(\x0c\x12\x1c\n\x14\x61\x64jacent_lanes_right\x18\x07 \x03(\x0c\x42\x10\n\x0e_left_boundaryB\x11\n\x0f_right_boundary"d\n\x08RoadArea\x12,\n\x10\x65xterior_polygon\x18\x01 \x01(\x0b\x32\x12.trajdata.Polyline\x12*\n\x0einterior_holes\x18\x02 \x03(\x0b\x32\x12.trajdata.Polyline"3\n\x0cPedCrosswalk\x12#\n\x07polygon\x18\x01 \x01(\x0b\x32\x12.trajdata.Polyline"1\n\nPedWalkway\x12#\n\x07polygon\x18\x01 \x01(\x0b\x32\x12.trajdata.Polylineb\x06proto3'
+
+
+DESCRIPTOR = _descriptor.FileDescriptor(
+  name='vectorized_map.proto',
+  package='trajdata',
+  syntax='proto3',
+  serialized_options=None,
+  create_key=_descriptor._internal_create_key,
+  serialized_pb=b'\n\x14vectorized_map.proto\x12\x08trajdata\"\xb0\x01\n\rVectorizedMap\x12\x0c\n\x04name\x18\x01 \x01(\t\x12&\n\x08\x65lements\x18\x02 \x03(\x0b\x32\x14.trajdata.MapElement\x12\x1f\n\x06max_pt\x18\x03 \x01(\x0b\x32\x0f.trajdata.Point\x12\x1f\n\x06min_pt\x18\x04 \x01(\x0b\x32\x0f.trajdata.Point\x12\'\n\x0eshifted_origin\x18\x05 \x01(\x0b\x32\x0f.trajdata.Point\"\xd8\x01\n\nMapElement\x12\n\n\x02id\x18\x01 \x01(\x0c\x12\'\n\troad_lane\x18\x02 \x01(\x0b\x32\x12.trajdata.RoadLaneH\x00\x12\'\n\troad_area\x18\x03 \x01(\x0b\x32\x12.trajdata.RoadAreaH\x00\x12/\n\rped_crosswalk\x18\x04 \x01(\x0b\x32\x16.trajdata.PedCrosswalkH\x00\x12+\n\x0bped_walkway\x18\x05 \x01(\x0b\x32\x14.trajdata.PedWalkwayH\x00\x42\x0e\n\x0c\x65lement_data\"(\n\x05Point\x12\t\n\x01x\x18\x01 \x01(\x01\x12\t\n\x01y\x18\x02 \x01(\x01\x12\t\n\x01z\x18\x03 \x01(\x01\"F\n\x08Polyline\x12\r\n\x05\x64x_mm\x18\x01 \x03(\x12\x12\r\n\x05\x64y_mm\x18\x02 \x03(\x12\x12\r\n\x05\x64z_mm\x18\x03 \x03(\x12\x12\r\n\x05h_rad\x18\x04 \x03(\x01\"\xaf\x02\n\x08RoadLane\x12\"\n\x06\x63\x65nter\x18\x01 \x01(\x0b\x32\x12.trajdata.Polyline\x12.\n\rleft_boundary\x18\x02 \x01(\x0b\x32\x12.trajdata.PolylineH\x00\x88\x01\x01\x12/\n\x0eright_boundary\x18\x03 \x01(\x0b\x32\x12.trajdata.PolylineH\x01\x88\x01\x01\x12\x13\n\x0b\x65ntry_lanes\x18\x04 \x03(\x0c\x12\x12\n\nexit_lanes\x18\x05 \x03(\x0c\x12\x1b\n\x13\x61\x64jacent_lanes_left\x18\x06 \x03(\x0c\x12\x1c\n\x14\x61\x64jacent_lanes_right\x18\x07 \x03(\x0c\x12\x15\n\rroad_area_ids\x18\x08 \x03(\x0c\x42\x10\n\x0e_left_boundaryB\x11\n\x0f_right_boundary\"d\n\x08RoadArea\x12,\n\x10\x65xterior_polygon\x18\x01 \x01(\x0b\x32\x12.trajdata.Polyline\x12*\n\x0einterior_holes\x18\x02 \x03(\x0b\x32\x12.trajdata.Polyline\"3\n\x0cPedCrosswalk\x12#\n\x07polygon\x18\x01 \x01(\x0b\x32\x12.trajdata.Polyline\"1\n\nPedWalkway\x12#\n\x07polygon\x18\x01 \x01(\x0b\x32\x12.trajdata.Polylineb\x06proto3'
 )
 
 
-_VECTORIZEDMAP = DESCRIPTOR.message_types_by_name["VectorizedMap"]
-_MAPELEMENT = DESCRIPTOR.message_types_by_name["MapElement"]
-_POINT = DESCRIPTOR.message_types_by_name["Point"]
-_POLYLINE = DESCRIPTOR.message_types_by_name["Polyline"]
-_ROADLANE = DESCRIPTOR.message_types_by_name["RoadLane"]
-_ROADAREA = DESCRIPTOR.message_types_by_name["RoadArea"]
-_PEDCROSSWALK = DESCRIPTOR.message_types_by_name["PedCrosswalk"]
-_PEDWALKWAY = DESCRIPTOR.message_types_by_name["PedWalkway"]
-VectorizedMap = _reflection.GeneratedProtocolMessageType(
-    "VectorizedMap",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _VECTORIZEDMAP,
-        "__module__": "vectorized_map_pb2"
-        # @@protoc_insertion_point(class_scope:trajdata.VectorizedMap)
-    },
+
+
+_VECTORIZEDMAP = _descriptor.Descriptor(
+  name='VectorizedMap',
+  full_name='trajdata.VectorizedMap',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='name', full_name='trajdata.VectorizedMap.name', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='elements', full_name='trajdata.VectorizedMap.elements', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='max_pt', full_name='trajdata.VectorizedMap.max_pt', index=2,
+      number=3, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='min_pt', full_name='trajdata.VectorizedMap.min_pt', index=3,
+      number=4, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='shifted_origin', full_name='trajdata.VectorizedMap.shifted_origin', index=4,
+      number=5, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=35,
+  serialized_end=211,
 )
-_sym_db.RegisterMessage(VectorizedMap)
 
-MapElement = _reflection.GeneratedProtocolMessageType(
-    "MapElement",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _MAPELEMENT,
-        "__module__": "vectorized_map_pb2"
-        # @@protoc_insertion_point(class_scope:trajdata.MapElement)
-    },
+
+_MAPELEMENT = _descriptor.Descriptor(
+  name='MapElement',
+  full_name='trajdata.MapElement',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='id', full_name='trajdata.MapElement.id', index=0,
+      number=1, type=12, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"",
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='road_lane', full_name='trajdata.MapElement.road_lane', index=1,
+      number=2, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='road_area', full_name='trajdata.MapElement.road_area', index=2,
+      number=3, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='ped_crosswalk', full_name='trajdata.MapElement.ped_crosswalk', index=3,
+      number=4, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='ped_walkway', full_name='trajdata.MapElement.ped_walkway', index=4,
+      number=5, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+    _descriptor.OneofDescriptor(
+      name='element_data', full_name='trajdata.MapElement.element_data',
+      index=0, containing_type=None,
+      create_key=_descriptor._internal_create_key,
+    fields=[]),
+  ],
+  serialized_start=214,
+  serialized_end=430,
 )
-_sym_db.RegisterMessage(MapElement)
 
-Point = _reflection.GeneratedProtocolMessageType(
-    "Point",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _POINT,
-        "__module__": "vectorized_map_pb2"
-        # @@protoc_insertion_point(class_scope:trajdata.Point)
-    },
+
+_POINT = _descriptor.Descriptor(
+  name='Point',
+  full_name='trajdata.Point',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='x', full_name='trajdata.Point.x', index=0,
+      number=1, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='y', full_name='trajdata.Point.y', index=1,
+      number=2, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='z', full_name='trajdata.Point.z', index=2,
+      number=3, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=432,
+  serialized_end=472,
 )
-_sym_db.RegisterMessage(Point)
 
-Polyline = _reflection.GeneratedProtocolMessageType(
-    "Polyline",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _POLYLINE,
-        "__module__": "vectorized_map_pb2"
-        # @@protoc_insertion_point(class_scope:trajdata.Polyline)
-    },
+
+_POLYLINE = _descriptor.Descriptor(
+  name='Polyline',
+  full_name='trajdata.Polyline',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='dx_mm', full_name='trajdata.Polyline.dx_mm', index=0,
+      number=1, type=18, cpp_type=2, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='dy_mm', full_name='trajdata.Polyline.dy_mm', index=1,
+      number=2, type=18, cpp_type=2, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='dz_mm', full_name='trajdata.Polyline.dz_mm', index=2,
+      number=3, type=18, cpp_type=2, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='h_rad', full_name='trajdata.Polyline.h_rad', index=3,
+      number=4, type=1, cpp_type=5, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=474,
+  serialized_end=544,
 )
-_sym_db.RegisterMessage(Polyline)
 
-RoadLane = _reflection.GeneratedProtocolMessageType(
-    "RoadLane",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _ROADLANE,
-        "__module__": "vectorized_map_pb2"
-        # @@protoc_insertion_point(class_scope:trajdata.RoadLane)
-    },
+
+_ROADLANE = _descriptor.Descriptor(
+  name='RoadLane',
+  full_name='trajdata.RoadLane',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='center', full_name='trajdata.RoadLane.center', index=0,
+      number=1, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='left_boundary', full_name='trajdata.RoadLane.left_boundary', index=1,
+      number=2, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='right_boundary', full_name='trajdata.RoadLane.right_boundary', index=2,
+      number=3, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='entry_lanes', full_name='trajdata.RoadLane.entry_lanes', index=3,
+      number=4, type=12, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='exit_lanes', full_name='trajdata.RoadLane.exit_lanes', index=4,
+      number=5, type=12, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='adjacent_lanes_left', full_name='trajdata.RoadLane.adjacent_lanes_left', index=5,
+      number=6, type=12, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='adjacent_lanes_right', full_name='trajdata.RoadLane.adjacent_lanes_right', index=6,
+      number=7, type=12, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='road_area_ids', full_name='trajdata.RoadLane.road_area_ids', index=7,
+      number=8, type=12, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+    _descriptor.OneofDescriptor(
+      name='_left_boundary', full_name='trajdata.RoadLane._left_boundary',
+      index=0, containing_type=None,
+      create_key=_descriptor._internal_create_key,
+    fields=[]),
+    _descriptor.OneofDescriptor(
+      name='_right_boundary', full_name='trajdata.RoadLane._right_boundary',
+      index=1, containing_type=None,
+      create_key=_descriptor._internal_create_key,
+    fields=[]),
+  ],
+  serialized_start=547,
+  serialized_end=850,
 )
-_sym_db.RegisterMessage(RoadLane)
 
-RoadArea = _reflection.GeneratedProtocolMessageType(
-    "RoadArea",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _ROADAREA,
-        "__module__": "vectorized_map_pb2"
-        # @@protoc_insertion_point(class_scope:trajdata.RoadArea)
-    },
+
+_ROADAREA = _descriptor.Descriptor(
+  name='RoadArea',
+  full_name='trajdata.RoadArea',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='exterior_polygon', full_name='trajdata.RoadArea.exterior_polygon', index=0,
+      number=1, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='interior_holes', full_name='trajdata.RoadArea.interior_holes', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=852,
+  serialized_end=952,
 )
-_sym_db.RegisterMessage(RoadArea)
 
-PedCrosswalk = _reflection.GeneratedProtocolMessageType(
-    "PedCrosswalk",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _PEDCROSSWALK,
-        "__module__": "vectorized_map_pb2"
-        # @@protoc_insertion_point(class_scope:trajdata.PedCrosswalk)
-    },
+
+_PEDCROSSWALK = _descriptor.Descriptor(
+  name='PedCrosswalk',
+  full_name='trajdata.PedCrosswalk',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='polygon', full_name='trajdata.PedCrosswalk.polygon', index=0,
+      number=1, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=954,
+  serialized_end=1005,
 )
-_sym_db.RegisterMessage(PedCrosswalk)
 
-PedWalkway = _reflection.GeneratedProtocolMessageType(
-    "PedWalkway",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _PEDWALKWAY,
-        "__module__": "vectorized_map_pb2"
-        # @@protoc_insertion_point(class_scope:trajdata.PedWalkway)
-    },
+
+_PEDWALKWAY = _descriptor.Descriptor(
+  name='PedWalkway',
+  full_name='trajdata.PedWalkway',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='polygon', full_name='trajdata.PedWalkway.polygon', index=0,
+      number=1, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=1007,
+  serialized_end=1056,
 )
+
+_VECTORIZEDMAP.fields_by_name['elements'].message_type = _MAPELEMENT
+_VECTORIZEDMAP.fields_by_name['max_pt'].message_type = _POINT
+_VECTORIZEDMAP.fields_by_name['min_pt'].message_type = _POINT
+_VECTORIZEDMAP.fields_by_name['shifted_origin'].message_type = _POINT
+_MAPELEMENT.fields_by_name['road_lane'].message_type = _ROADLANE
+_MAPELEMENT.fields_by_name['road_area'].message_type = _ROADAREA
+_MAPELEMENT.fields_by_name['ped_crosswalk'].message_type = _PEDCROSSWALK
+_MAPELEMENT.fields_by_name['ped_walkway'].message_type = _PEDWALKWAY
+_MAPELEMENT.oneofs_by_name['element_data'].fields.append(
+  _MAPELEMENT.fields_by_name['road_lane'])
+_MAPELEMENT.fields_by_name['road_lane'].containing_oneof = _MAPELEMENT.oneofs_by_name['element_data']
+_MAPELEMENT.oneofs_by_name['element_data'].fields.append(
+  _MAPELEMENT.fields_by_name['road_area'])
+_MAPELEMENT.fields_by_name['road_area'].containing_oneof = _MAPELEMENT.oneofs_by_name['element_data']
+_MAPELEMENT.oneofs_by_name['element_data'].fields.append(
+  _MAPELEMENT.fields_by_name['ped_crosswalk'])
+_MAPELEMENT.fields_by_name['ped_crosswalk'].containing_oneof = _MAPELEMENT.oneofs_by_name['element_data']
+_MAPELEMENT.oneofs_by_name['element_data'].fields.append(
+  _MAPELEMENT.fields_by_name['ped_walkway'])
+_MAPELEMENT.fields_by_name['ped_walkway'].containing_oneof = _MAPELEMENT.oneofs_by_name['element_data']
+_ROADLANE.fields_by_name['center'].message_type = _POLYLINE
+_ROADLANE.fields_by_name['left_boundary'].message_type = _POLYLINE
+_ROADLANE.fields_by_name['right_boundary'].message_type = _POLYLINE
+_ROADLANE.oneofs_by_name['_left_boundary'].fields.append(
+  _ROADLANE.fields_by_name['left_boundary'])
+_ROADLANE.fields_by_name['left_boundary'].containing_oneof = _ROADLANE.oneofs_by_name['_left_boundary']
+_ROADLANE.oneofs_by_name['_right_boundary'].fields.append(
+  _ROADLANE.fields_by_name['right_boundary'])
+_ROADLANE.fields_by_name['right_boundary'].containing_oneof = _ROADLANE.oneofs_by_name['_right_boundary']
+_ROADAREA.fields_by_name['exterior_polygon'].message_type = _POLYLINE
+_ROADAREA.fields_by_name['interior_holes'].message_type = _POLYLINE
+_PEDCROSSWALK.fields_by_name['polygon'].message_type = _POLYLINE
+_PEDWALKWAY.fields_by_name['polygon'].message_type = _POLYLINE
+DESCRIPTOR.message_types_by_name['VectorizedMap'] = _VECTORIZEDMAP
+DESCRIPTOR.message_types_by_name['MapElement'] = _MAPELEMENT
+DESCRIPTOR.message_types_by_name['Point'] = _POINT
+DESCRIPTOR.message_types_by_name['Polyline'] = _POLYLINE
+DESCRIPTOR.message_types_by_name['RoadLane'] = _ROADLANE
+DESCRIPTOR.message_types_by_name['RoadArea'] = _ROADAREA
+DESCRIPTOR.message_types_by_name['PedCrosswalk'] = _PEDCROSSWALK
+DESCRIPTOR.message_types_by_name['PedWalkway'] = _PEDWALKWAY
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
+
+VectorizedMap = _reflection.GeneratedProtocolMessageType('VectorizedMap', (_message.Message,), {
+  'DESCRIPTOR' : _VECTORIZEDMAP,
+  '__module__' : 'vectorized_map_pb2'
+  # @@protoc_insertion_point(class_scope:trajdata.VectorizedMap)
+  })
+_sym_db.RegisterMessage(VectorizedMap)
+
+MapElement = _reflection.GeneratedProtocolMessageType('MapElement', (_message.Message,), {
+  'DESCRIPTOR' : _MAPELEMENT,
+  '__module__' : 'vectorized_map_pb2'
+  # @@protoc_insertion_point(class_scope:trajdata.MapElement)
+  })
+_sym_db.RegisterMessage(MapElement)
+
+Point = _reflection.GeneratedProtocolMessageType('Point', (_message.Message,), {
+  'DESCRIPTOR' : _POINT,
+  '__module__' : 'vectorized_map_pb2'
+  # @@protoc_insertion_point(class_scope:trajdata.Point)
+  })
+_sym_db.RegisterMessage(Point)
+
+Polyline = _reflection.GeneratedProtocolMessageType('Polyline', (_message.Message,), {
+  'DESCRIPTOR' : _POLYLINE,
+  '__module__' : 'vectorized_map_pb2'
+  # @@protoc_insertion_point(class_scope:trajdata.Polyline)
+  })
+_sym_db.RegisterMessage(Polyline)
+
+RoadLane = _reflection.GeneratedProtocolMessageType('RoadLane', (_message.Message,), {
+  'DESCRIPTOR' : _ROADLANE,
+  '__module__' : 'vectorized_map_pb2'
+  # @@protoc_insertion_point(class_scope:trajdata.RoadLane)
+  })
+_sym_db.RegisterMessage(RoadLane)
+
+RoadArea = _reflection.GeneratedProtocolMessageType('RoadArea', (_message.Message,), {
+  'DESCRIPTOR' : _ROADAREA,
+  '__module__' : 'vectorized_map_pb2'
+  # @@protoc_insertion_point(class_scope:trajdata.RoadArea)
+  })
+_sym_db.RegisterMessage(RoadArea)
+
+PedCrosswalk = _reflection.GeneratedProtocolMessageType('PedCrosswalk', (_message.Message,), {
+  'DESCRIPTOR' : _PEDCROSSWALK,
+  '__module__' : 'vectorized_map_pb2'
+  # @@protoc_insertion_point(class_scope:trajdata.PedCrosswalk)
+  })
+_sym_db.RegisterMessage(PedCrosswalk)
+
+PedWalkway = _reflection.GeneratedProtocolMessageType('PedWalkway', (_message.Message,), {
+  'DESCRIPTOR' : _PEDWALKWAY,
+  '__module__' : 'vectorized_map_pb2'
+  # @@protoc_insertion_point(class_scope:trajdata.PedWalkway)
+  })
 _sym_db.RegisterMessage(PedWalkway)
 
-if _descriptor._USE_C_DESCRIPTORS == False:
-
-    DESCRIPTOR._options = None
-    _VECTORIZEDMAP._serialized_start = 35
-    _VECTORIZEDMAP._serialized_end = 211
-    _MAPELEMENT._serialized_start = 214
-    _MAPELEMENT._serialized_end = 430
-    _POINT._serialized_start = 432
-    _POINT._serialized_end = 472
-    _POLYLINE._serialized_start = 474
-    _POLYLINE._serialized_end = 544
-    _ROADLANE._serialized_start = 547
-    _ROADLANE._serialized_end = 827
-    _ROADAREA._serialized_start = 829
-    _ROADAREA._serialized_end = 929
-    _PEDCROSSWALK._serialized_start = 931
-    _PEDCROSSWALK._serialized_end = 982
-    _PEDWALKWAY._serialized_start = 984
-    _PEDWALKWAY._serialized_end = 1033
+
 # @@protoc_insertion_point(module_scope)
diff --git a/src/trajdata/utils/arr_utils.py b/src/trajdata/utils/arr_utils.py
index e76a678..6f10bb4 100644
--- a/src/trajdata/utils/arr_utils.py
+++ b/src/trajdata/utils/arr_utils.py
@@ -93,7 +93,9 @@ def vrange(starts: np.ndarray, stops: np.ndarray) -> np.ndarray:
     return np.repeat(stops - lens.cumsum(), lens) + np.arange(lens.sum())
 
 
-def angle_wrap(radians: np.ndarray) -> np.ndarray:
+def angle_wrap(
+    radians: Union[np.ndarray, torch.Tensor]
+) -> Union[np.ndarray, torch.Tensor]:
     """This function wraps angles to lie within [-pi, pi).
 
     Args:
@@ -130,12 +132,12 @@ def rotation_matrix(angle: Union[float, np.ndarray]) -> np.ndarray:
     return rotmat.transpose(*np.arange(2, batch_dims + 2), 0, 1)
 
 
-def transform_matrices(angles: Tensor, translations: Tensor) -> Tensor:
+def transform_matrices(angles: Tensor, translations: Optional[Tensor]) -> Tensor:
     """Creates a 3x3 transformation matrix for each angle and translation in the input.
 
     Args:
-        angles (Tensor): The (N,)-shaped angles tensor to rotate points by (in radians).
-        translations (Tensor): The (N,2)-shaped translations to shift points by.
+        angles (Tensor): The (...)-shaped angles tensor to rotate points by (in radians).
+        translations (Tensor): The (...,2)-shaped translations to shift points by.
 
     Returns:
         Tensor: The Nx3x3 transformation matrices.
@@ -143,12 +145,19 @@ def transform_matrices(angles: Tensor, translations: Tensor) -> Tensor:
     cos_vals = torch.cos(angles)
     sin_vals = torch.sin(angles)
     last_rows = torch.tensor(
-        [[0.0, 0.0, 1.0]], dtype=angles.dtype, device=angles.device
-    ).expand((angles.shape[0], -1))
+        [0.0, 0.0, 1.0], dtype=angles.dtype, device=angles.device
+    ).view([1] * angles.ndim + [3]).expand(list(angles.shape) + [-1])
+
+    if translations is None:
+        trans_x = torch.zeros_like(angles)
+        trans_y = trans_x
+    else:
+        trans_x, trans_y = torch.unbind(translations, dim=-1)
+
     return torch.stack(
         [
-            torch.stack([cos_vals, -sin_vals, translations[:, 0]], dim=-1),
-            torch.stack([sin_vals, cos_vals, translations[:, 1]], dim=-1),
+            torch.stack([cos_vals, -sin_vals, trans_x], dim=-1),
+            torch.stack([sin_vals, cos_vals, trans_y], dim=-1),
             last_rows,
         ],
         dim=-2,
@@ -243,6 +252,125 @@ def transform_xyh_np(xyh: np.ndarray, tf_mat: np.ndarray) -> np.ndarray:
     transformed_angles = transform_angles_np(xyh[..., 2], tf_mat)
     return np.concatenate([transformed_xy, transformed_angles[..., None]], axis=-1)
 
+def transform_xyh_torch(xyh: torch.Tensor, tf_mat: torch.Tensor) -> torch.Tensor:
+    """
+    Returns transformed set of xyh points
+
+    Args:
+        xyh (torch.Tensor): shape [...,3]
+        tf_mat (torch.Tensor): shape [...,3,3]
+    """
+    transformed_xy = batch_nd_transform_points_pt(xyh[..., :2], tf_mat)
+    transformed_angles = batch_nd_transform_angles_pt(xyh[..., 2], tf_mat)
+    return torch.cat([transformed_xy, transformed_angles[..., None]], dim=-1)
+
+# -------- TODO(pkarkus) redundant transforms, remove them
+
+
+def batch_nd_transform_points_np(points: np.ndarray, Mat: np.ndarray) -> np.ndarray:
+    ndim = Mat.shape[-1] - 1
+    batch = list(range(Mat.ndim - 2)) + [Mat.ndim - 1] + [Mat.ndim - 2]
+    Mat = np.transpose(Mat, batch)
+    if points.ndim == Mat.ndim - 1:
+        return (points[..., np.newaxis, :] @ Mat[..., :ndim, :ndim]).squeeze(-2) + Mat[
+            ..., -1:, :ndim
+        ].squeeze(-2)
+    elif points.ndim == Mat.ndim:
+        return (
+            (points[..., np.newaxis, :] @ Mat[..., np.newaxis, :ndim, :ndim])
+            + Mat[..., np.newaxis, -1:, :ndim]
+        ).squeeze(-2)
+    else:
+        raise Exception("wrong shape")
+
+def batch_nd_transform_points_pt(
+    points: torch.Tensor, Mat: torch.Tensor
+) -> torch.Tensor:
+    ndim = Mat.shape[-1] - 1
+    Mat = torch.transpose(Mat, -1, -2)
+    if points.ndim == Mat.ndim - 1:
+        return (points[..., np.newaxis, :] @ Mat[..., :ndim, :ndim]).squeeze(-2) + Mat[
+            ..., -1:, :ndim
+        ].squeeze(-2)
+    elif points.ndim == Mat.ndim:
+        return (
+            (points[..., np.newaxis, :] @ Mat[..., np.newaxis, :ndim, :ndim])
+            + Mat[..., np.newaxis, -1:, :ndim]
+        ).squeeze(-2)
+    elif points.ndim == Mat.ndim + 1:
+        return (
+            (
+                points[..., np.newaxis, :]
+                @ Mat[..., np.newaxis, np.newaxis, :ndim, :ndim]
+            )
+            + Mat[..., np.newaxis, np.newaxis, -1:, :ndim]
+        ).squeeze(-2)
+    else:
+        raise Exception("wrong shape")
+
+
+def batch_nd_transform_angles_np(angles: np.ndarray, Mat: np.ndarray) -> np.ndarray:
+    cos_vals, sin_vals = Mat[..., 0, 0], Mat[..., 1, 0]
+    rot_angle = np.arctan2(sin_vals, cos_vals)
+    angles = angles + rot_angle
+    angles = angle_wrap(angles)
+    return angles
+
+
+def batch_nd_transform_angles_pt(
+    angles: torch.Tensor, Mat: torch.Tensor
+) -> torch.Tensor:
+    cos_vals, sin_vals = Mat[..., 0, 0], Mat[..., 1, 0]
+    rot_angle = torch.arctan2(sin_vals, cos_vals)
+    if rot_angle.ndim > angles.ndim:
+        raise ValueError("wrong shape")
+    while rot_angle.ndim < angles.ndim:
+        rot_angle = rot_angle.unsqueeze(-1)
+    angles = angles + rot_angle
+    angles = angle_wrap(angles)
+    return angles
+
+
+def batch_nd_transform_points_angles_np(
+    points_angles: np.ndarray, Mat: np.ndarray
+) -> np.ndarray:
+    assert points_angles.shape[-1] == 3
+    points = batch_nd_transform_points_np(points_angles[..., :2], Mat)
+    angles = batch_nd_transform_angles_np(points_angles[..., 2:3], Mat)
+    points_angles = np.concatenate([points, angles], axis=-1)
+    return points_angles
+
+
+def batch_nd_transform_points_angles_pt(
+    points_angles: torch.Tensor, Mat: torch.Tensor
+) -> torch.Tensor:
+    assert points_angles.shape[-1] == 3
+    points = batch_nd_transform_points_pt(points_angles[..., :2], Mat)
+    angles = batch_nd_transform_angles_pt(points_angles[..., 2:3], Mat)
+    points_angles = torch.concat([points, angles], axis=-1)
+    return points_angles
+
+
+def batch_nd_transform_xyvvaahh_pt(traj_xyvvaahh: torch.Tensor, tf: torch.Tensor) -> torch.Tensor:
+    """
+    traj_xyvvaahh: [..., state_dim] where state_dim = [x, y, vx, vy, ax, ay, sinh, cosh]
+    This is the state representation used in AgentBatch and SceneBatch.
+    """
+    rot_only_tf = tf.clone()
+    rot_only_tf[..., :2, -1] = 0.
+
+    xy, vv, aa, hh = torch.split(traj_xyvvaahh, (2, 2, 2, 2), dim=-1)
+    xy = batch_nd_transform_points_pt(xy, tf)
+    vv = batch_nd_transform_points_pt(vv, rot_only_tf)
+    aa = batch_nd_transform_points_pt(aa, rot_only_tf)
+    # hh: sinh, cosh instead of cosh, sinh, so we use flip
+    hh = batch_nd_transform_points_pt(hh.flip(-1), rot_only_tf).flip(-1)
+
+    return torch.concat((xy, vv, aa, hh), dim=-1)
+
+
+# -------- end of redundant transforms
+
 
 def agent_aware_diff(values: np.ndarray, agent_ids: np.ndarray) -> np.ndarray:
     values_diff: np.ndarray = np.diff(
@@ -325,3 +453,120 @@ def quaternion_to_yaw(q: np.ndarray):
         2 * (q[..., 0] * q[..., 3] - q[..., 1] * q[..., 2]),
         1 - 2 * (q[..., 2] ** 2 + q[..., 3] ** 2),
     )
+
+
+def batch_select(
+    x: torch.Tensor, 
+    index: torch.Tensor, 
+    batch_dims: int
+) -> torch.Tensor:
+    # Indexing into tensor, treating the first `batch_dims` dimensions as batch.
+    # Kind of: output[..., k] = x[..., index[...]]
+
+    assert index.ndim >= batch_dims
+    assert index.ndim <= x.ndim
+    assert x.shape[:batch_dims] == index.shape[:batch_dims]
+
+    batch_shape = x.shape[:batch_dims]
+    x_flat = x.reshape(-1, *x.shape[batch_dims:])
+    index_flat = index.reshape(-1, *index.shape[batch_dims:])
+    x_flat = x_flat[torch.arange(x_flat.shape[0]), index_flat]
+    x = x_flat.reshape(*batch_shape, *x_flat.shape[1:])
+    
+    return x
+
+
+def roll_with_tensor(mat: torch.Tensor, shifts: torch.LongTensor, dim: int):
+    if dim < 0:
+        dim = mat.ndim + dim
+    arange1 = torch.arange(mat.shape[dim], device=shifts.device)
+    expanded_shape = [1] * dim + [-1] + [1] * (mat.ndim-dim-1)
+    arange1 = arange1.view(expanded_shape).expand(mat.shape)
+    if shifts.ndim == 1:
+        shifts = shifts.view([1] * (dim-1) + [-1])
+    # TODO(pkarkus) assert that shift dimenesions either match mat or 1
+    shifts = shifts.view(list(shifts.shape) + [1] * (mat.ndim-dim))
+
+    arange2 = (arange1 - shifts) % mat.shape[dim]
+    # print(arange2)
+    return torch.gather(mat, dim, arange2)
+
+def round_2pi(x):
+    return (x + np.pi) % (2 * np.pi) - np.pi
+
+def batch_proj(x, line):
+    # x:[batch,3], line:[batch,N,3]
+    line_length = line.shape[-2]
+    batch_dim = x.ndim - 1
+    if isinstance(x, torch.Tensor):
+        delta = line[..., 0:2] - torch.unsqueeze(x[..., 0:2], dim=-2).repeat(
+            *([1] * batch_dim), line_length, 1
+        )
+        dis = torch.linalg.norm(delta, axis=-1)
+        idx0 = torch.argmin(dis, dim=-1)
+        idx = idx0.view(*line.shape[:-2], 1, 1).repeat(
+            *([1] * (batch_dim + 1)), line.shape[-1]
+        )
+        line_min = torch.squeeze(torch.gather(line, -2, idx), dim=-2)
+        dx = x[..., None, 0] - line[..., 0]
+        dy = x[..., None, 1] - line[..., 1]
+        delta_y = -dx * torch.sin(line_min[..., None, 2]) + dy * torch.cos(
+            line_min[..., None, 2]
+        )
+        delta_x = dx * torch.cos(line_min[..., None, 2]) + dy * torch.sin(
+            line_min[..., None, 2]
+        )
+
+        delta_psi = round_2pi(x[..., 2] - line_min[..., 2])
+
+        return (
+            delta_x,
+            delta_y,
+            torch.unsqueeze(delta_psi, dim=-1),
+        )
+    elif isinstance(x, np.ndarray):
+        delta = line[..., 0:2] - np.repeat(
+            x[..., np.newaxis, 0:2], line_length, axis=-2
+        )
+        dis = np.linalg.norm(delta, axis=-1)
+        idx0 = np.argmin(dis, axis=-1)
+        idx = idx0.reshape(*line.shape[:-2], 1, 1).repeat(line.shape[-1], axis=-1)
+        line_min = np.squeeze(np.take_along_axis(line, idx, axis=-2), axis=-2)
+        dx = x[..., None, 0] - line[..., 0]
+        dy = x[..., None, 1] - line[..., 1]
+        delta_y = -dx * np.sin(line_min[..., None, 2]) + dy * np.cos(
+            line_min[..., None, 2]
+        )
+        delta_x = dx * np.cos(line_min[..., None, 2]) + dy * np.sin(
+            line_min[..., None, 2]
+        )
+
+        delta_psi = round_2pi(x[..., 2] - line_min[..., 2])
+        return (
+            delta_x,
+            delta_y,
+            np.expand_dims(delta_psi, axis=-1),
+        )
+
+def get_close_lanes(radius,ego_xyh,vec_map,num_pts):
+    # obtain close lanes, their distance to the ego
+    close_lanes = []
+    while len(close_lanes)==0:
+        close_lanes=vec_map.get_lanes_within(ego_xyh,radius)
+        radius+=20
+    dis = list()
+    lane_pts = np.stack([lane.center.interpolate(num_pts).points[:,[0,1,3]] for lane in close_lanes],0)
+    dx,dy,dh = batch_proj(ego_xyh[None].repeat(lane_pts.shape[0],0),lane_pts)
+    
+    idx = np.abs(dx).argmin(axis=1)
+    # hausdorff distance to the lane (longitudinal)
+    x_dis = np.take_along_axis(np.abs(dx),idx[:,None],axis=1).squeeze(1)
+    x_dis[(dx.min(1)<0) & (dx.max(1)>0)] = 0
+    
+    y_dis = np.take_along_axis(np.abs(dy),idx[:,None],axis=1).squeeze(1)
+
+    # distance metric to the lane (combining x,y)
+    dis = x_dis+y_dis
+            
+                
+    return close_lanes,dis
diff --git a/src/trajdata/utils/batch_utils.py b/src/trajdata/utils/batch_utils.py
index cf63009..555e790 100644
--- a/src/trajdata/utils/batch_utils.py
+++ b/src/trajdata/utils/batch_utils.py
@@ -1,5 +1,9 @@
 from collections import defaultdict
-from typing import Any, Dict, Iterator, List, Optional, Tuple
+
+import torch
+
+from pathlib import Path
+from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
 
 import numpy as np
 from torch.utils.data import Sampler
@@ -10,10 +14,15 @@ from trajdata.data_structures import (
     AgentBatchElement,
     AgentDataIndex,
     AgentType,
+    SceneBatch,
     SceneBatchElement,
     SceneTimeAgent,
 )
-from trajdata.data_structures.collation import agent_collate_fn
+from trajdata.data_structures.collation import agent_collate_fn, batch_rotate_raster_maps_for_agents_in_scene
+from trajdata.maps import RasterizedMapPatch
+from trajdata.utils.map_utils import load_map_patch
+from trajdata.utils.arr_utils import batch_nd_transform_xyvvaahh_pt, batch_select, PadDirection
+from trajdata.caching.df_cache import DataFrameCache
 
 
 class SceneTimeBatcher(Sampler):
@@ -173,3 +182,107 @@ def convert_to_agent_batch(
         )
 
     return agent_collate_fn(batch_elems, return_dict=False, pad_format=pad_format)
+
+
+def get_agents_map_patch(
+    cache_path: Path, 
+    map_name: str,
+    patch_params: Dict[str, int], 
+    agent_world_states_xyh: Union[np.ndarray, torch.Tensor], 
+    allow_nan: float = False,
+) -> List[RasterizedMapPatch]:
+
+    if isinstance(agent_world_states_xyh, torch.Tensor):
+        agent_world_states_xyh = agent_world_states_xyh.cpu().numpy()
+    assert agent_world_states_xyh.ndim == 2
+    assert agent_world_states_xyh.shape[-1] == 3
+    
+    desired_patch_size: int = patch_params["map_size_px"]
+    resolution: float = patch_params["px_per_m"]
+    offset_xy: Tuple[float, float] = patch_params.get("offset_frac_xy", (0.0, 0.0))
+    return_rgb: bool = patch_params.get("return_rgb", True)
+    no_map_fill_val: float = patch_params.get("no_map_fill_value", 0.0)
+
+    env_name, location_name = map_name.split(':')  # assumes map_name format nusc_mini:boston-seaport
+
+    map_patches = list()
+
+    (
+        maps_path,
+        _,
+        _,
+        raster_map_path,
+        raster_metadata_path,
+    ) = DataFrameCache.get_map_paths(
+        cache_path, env_name, location_name, resolution
+    )
+
+    for i in range(agent_world_states_xyh.shape[0]):
+        patch_data, raster_from_world_tf, has_data = load_map_patch(
+            raster_map_path,
+            raster_metadata_path,
+            agent_world_states_xyh[i, 0],
+            agent_world_states_xyh[i, 1],
+            desired_patch_size,
+            resolution,
+            offset_xy,
+            agent_world_states_xyh[i, 2],
+            return_rgb,
+            rot_pad_factor=np.sqrt(2),
+            no_map_val=no_map_fill_val,
+        )
+        map_patches.append(
+            RasterizedMapPatch(
+                data=patch_data,
+                rot_angle=agent_world_states_xyh[i, 2],
+                crop_size=desired_patch_size,
+                resolution=resolution,
+                raster_from_world_tf=raster_from_world_tf,
+                has_data=has_data,
+            )
+        )
+
+    return map_patches
+
+
+def get_raster_maps_for_scene_batch(batch: SceneBatch, cache_path: Path, raster_map_params: Dict):
+
+    # Get current states
+    agent_states = batch.agent_hist.as_format('x,y,xd,yd,xdd,ydd,s,c')
+    if batch.history_pad_dir == PadDirection.AFTER:
+        agent_states = batch_select(agent_states, index=batch.agent_hist_len-1, batch_dims=2)  # b, N, t, 8           
+    else:
+        agent_states = agent_states[:, :, -1]
+
+    agent_world_states_xyvvaahh = batch_nd_transform_xyvvaahh_pt(
+        agent_states.type_as(batch.centered_world_from_agent_tf), 
+        batch.centered_world_from_agent_tf
+    )
+
+    agent_world_states_xyh = torch.concat((
+        agent_world_states_xyvvaahh[..., :2], 
+        torch.atan2(agent_world_states_xyvvaahh[..., 6:7], agent_world_states_xyvvaahh[..., 7:8])), dim=-1)
+
+    maps: List[torch.Tensor] = []
+    maps_resolution: List[torch.Tensor] = []
+    raster_from_world_tf: List[torch.Tensor] = []
+
+    # Collect map patches for all elements and agents into a flat list
+    num_agents: List[int] = []
+    map_patches: List[RasterizedMapPatch] = []
+
+    for b_i in range(agent_world_states_xyh.shape[0]):
+        num_agents.append(batch.num_agents[b_i])
+        map_patches += get_agents_map_patch(
+            cache_path, batch.map_names[b_i], raster_map_params, agent_world_states_xyh[b_i, :batch.num_agents[b_i]])
+
+    # Batch transform map patches and pad
+    (
+        maps, 
+        maps_resolution, 
+        raster_from_world_tf
+    ) = batch_rotate_raster_maps_for_agents_in_scene(
+        map_patches, num_agents, agent_world_states_xyh.shape[1], pad_value=np.nan,
+    )
+
+    return maps, maps_resolution, raster_from_world_tf
diff --git a/src/trajdata/utils/comm_utils.py b/src/trajdata/utils/comm_utils.py
new file mode 100644
index 0000000..594ccb0
--- /dev/null
+++ b/src/trajdata/utils/comm_utils.py
@@ -0,0 +1,24 @@
+import numpy as np
+import socket
+
+from contextlib import closing
+from typing import Callable, Optional
+
+
+def find_open_port():
+    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
+        s.bind(("", 0))
+        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        return s.getsockname()[1]
+
+
+def find_open_port_in_range(start_port, end_port):
+    for port in range(start_port, end_port+1):
+        try:
+            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+                s.bind(('localhost', port))
+                s.listen(1)
+                return port
+        except OSError:
+            continue
+    return None
diff --git a/src/trajdata/utils/env_utils.py b/src/trajdata/utils/env_utils.py
index 4726537..a429f68 100644
--- a/src/trajdata/utils/env_utils.py
+++ b/src/trajdata/utils/env_utils.py
@@ -33,6 +33,7 @@ except ModuleNotFoundError:
     # This can happen if the user did not install trajdata
     # with the "trajdata[nuplan]" option.
     pass
+from trajdata.dataset_specific.drivesim import DrivesimDataset
 
 try:
     from trajdata.dataset_specific.waymo import WaymoDataset
@@ -60,7 +61,10 @@ def get_raw_dataset(dataset_name: str, data_dir: str) -> RawDataset:
         )
 
     if "nuplan" in dataset_name:
-        return NuplanDataset(dataset_name, data_dir, parallelizable=True, has_maps=True)
+        return NuplanDataset(dataset_name, data_dir, parallelizable=True, has_maps=True)    
+    
+    if "drivesim" in dataset_name:
+        return DrivesimDataset(dataset_name, data_dir, parallelizable=True, has_maps=True)
 
     if "waymo" in dataset_name:
         return WaymoDataset(dataset_name, data_dir, parallelizable=True, has_maps=True)
diff --git a/src/trajdata/utils/map_utils.py b/src/trajdata/utils/map_utils.py
index 01844b7..45c08fd 100644
--- a/src/trajdata/utils/map_utils.py
+++ b/src/trajdata/utils/map_utils.py
@@ -3,19 +3,300 @@ from __future__ import annotations
 from typing import TYPE_CHECKING
 
 if TYPE_CHECKING:
-    from trajdata.maps import map_kdtree, vec_map
+    from trajdata.maps import map_kdtree, vec_map, RasterizedMapMetadata, RasterizedMapPatch
 
 from pathlib import Path
-from typing import Dict, Final, Optional
+from typing import Dict, Final, Optional, Tuple, List, Union
 
 import dill
+import kornia
 import numpy as np
+import math
+import torch
+import zarr
 from scipy.stats import circmean
 
 import trajdata.proto.vectorized_map_pb2 as map_proto
 from trajdata.utils import arr_utils
 
-MM_PER_M: Final[float] = 1000
+NUM_DECIMALS: Final[int] = 5
+COMPRESSION_SCALE: Final[float] = 10**NUM_DECIMALS
+import enum
+
+class LaneSegRelation(enum.IntEnum):
+    """
+    Categorical token describing the relationship between an agent and a Lane
+    """
+    NOTCONNECTED = 0
+    NEXT = 1
+    PREV = 2
+    LEFT = 3
+    RIGHT = 4
+
+def pad_map_patch(
+    patch: np.ndarray,
+    #                 top, bot, left, right
+    patch_sides: Tuple[int, int, int, int],
+    patch_size: int,
+    map_dims: Tuple[int, int, int],
+) -> np.ndarray:
+    # TODO(pkarkus) remove equivalent function from df_cache
+
+    if patch.shape[-2:] == (patch_size, patch_size):
+        return patch
+
+    top, bot, left, right = patch_sides
+    channels, height, width = map_dims
+
+    # If we're off the map, just return zeros in the
+    # desired size of the patch.
+    if bot <= 0 or top >= height or right <= 0 or left >= width:
+        return np.zeros((channels, patch_size, patch_size))
+
+    pad_top, pad_bot, pad_left, pad_right = 0, 0, 0, 0
+    if top < 0:
+        pad_top = 0 - top
+    if bot >= height:
+        pad_bot = bot - height
+    if left < 0:
+        pad_left = 0 - left
+    if right >= width:
+        pad_right = right - width
+
+    return np.pad(patch, [(0, 0), (pad_top, pad_bot), (pad_left, pad_right)])
+
+
+def load_map_patch(
+    raster_map_path: Path,
+    raster_metadata_path: Path,
+    world_x: float,
+    world_y: float,
+    desired_patch_size: int,
+    resolution: float,
+    offset_xy: Tuple[float, float],
+    agent_heading: float,
+    return_rgb: bool,
+    rot_pad_factor: float = 1.0,
+    no_map_val: float = 0.0,
+    allow_missing_map: bool = False,
+) -> Tuple[np.ndarray, np.ndarray, bool]:
+
+    # TODO(pkarkus) remove equivalent function from df_cache
+
+    if not raster_metadata_path.exists():
+        if not allow_missing_map:
+            raise ValueError(f"Missing map at {raster_metadata_path}")
+        # This dataset (or location) does not have any maps,
+        # so we return an empty map.
+        patch_size: int = math.ceil((rot_pad_factor * desired_patch_size) / 2) * 2
+
+        return (
+            np.full(
+                (1 if not return_rgb else 3, patch_size, patch_size),
+                fill_value=no_map_val,
+            ),
+            np.eye(3),
+            False,
+        )
+
+    with open(raster_metadata_path, "rb") as f:
+        map_info: RasterizedMapMetadata = dill.load(f)
+
+    raster_from_world_tf: np.ndarray = map_info.map_from_world
+    map_coords: np.ndarray = map_info.map_from_world @ np.array(
+        [world_x, world_y, 1.0]
+    )
+    map_x, map_y = map_coords[0].item(), map_coords[1].item()
+
+    raster_from_world_tf = (
+        np.array(
+            [
+                [1.0, 0.0, -map_x],
+                [0.0, 1.0, -map_y],
+                [0.0, 0.0, 1.0],
+            ]
+        )
+        @ raster_from_world_tf
+    )
+
+    # This first size is how much of the map we
+    # need to extract to match the requested metric size (meters x meters) of
+    # the patch.
+    data_patch_size: int = math.ceil(
+        desired_patch_size * map_info.resolution / resolution
+    )
+
+    # Incorporating offsets.
+    if offset_xy != (0.0, 0.0):
+        # x is negative here because I am moving the map
+        # center so that the agent ends up where the user wishes
+        # (the agent is pinned from the end user's perspective).
+        map_offset: Tuple[float, float] = (
+            -offset_xy[0] * data_patch_size // 2,
+            offset_xy[1] * data_patch_size // 2,
+        )
+
+        rotated_offset: np.ndarray = (
+            arr_utils.rotation_matrix(agent_heading) @ map_offset
+        )
+
+        off_x = rotated_offset[0]
+        off_y = rotated_offset[1]
+
+        map_x += off_x
+        map_y += off_y
+
+        raster_from_world_tf = (
+            np.array(
+                [
+                    [1.0, 0.0, -off_x],
+                    [0.0, 1.0, -off_y],
+                    [0.0, 0.0, 1.0],
+                ]
+            )
+            @ raster_from_world_tf
+        )
+
+    # This is the size of the patch taking into account expansion to allow for
+    # rotation to match the agent's heading. We also ensure the final size is
+    # divisible by two so that the // 2 below does not chop any information off.
+    data_with_rot_pad_size: int = math.ceil((rot_pad_factor * data_patch_size) / 2) * 2
+
+    disk_data = zarr.open_array(raster_map_path, mode="r")
+
+    map_x = round(map_x)
+    map_y = round(map_y)
+
+    # Half of the patch's side length.
+    half_extent: int = data_with_rot_pad_size // 2
+
+    top: int = map_y - half_extent
+    bot: int = map_y + half_extent
+    left: int = map_x - half_extent
+    right: int = map_x + half_extent
+
+    data_patch: np.ndarray = pad_map_patch(
+        disk_data[
+            ...,
+            max(top, 0) : min(bot, disk_data.shape[1]),
+            max(left, 0) : min(right, disk_data.shape[2]),
+        ],
+        (top, bot, left, right),
+        data_with_rot_pad_size,
+        disk_data.shape,
+    )
+
+    if return_rgb:
+        rgb_groups = map_info.layer_rgb_groups
+        data_patch = np.stack(
+            [
+                np.amax(data_patch[rgb_groups[0]], axis=0),
+                np.amax(data_patch[rgb_groups[1]], axis=0),
+                np.amax(data_patch[rgb_groups[2]], axis=0),
+            ],
+        )
+
+    if desired_patch_size != data_patch_size:
+        scale_factor: float = desired_patch_size / data_patch_size
+        data_patch = (
+            kornia.geometry.rescale(
+                torch.from_numpy(data_patch).unsqueeze(0),
+                scale_factor,
+                # Default align_corners value, just putting it to remove warnings
+                align_corners=False,
+                antialias=True,
+            )
+            .squeeze(0)
+            .numpy()
+        )
+
+        raster_from_world_tf = (
+            np.array(
+                [
+                    [1 / scale_factor, 0.0, 0.0],
+                    [0.0, 1 / scale_factor, 0.0],
+                    [0.0, 0.0, 1.0],
+                ]
+            )
+            @ raster_from_world_tf
+        )
+
+    return data_patch, raster_from_world_tf, True
+
+
+def batch_transform_raster_maps(
+    map_patches: List[RasterizedMapPatch],
+):
+
+    patch_size: int = map_patches[0].crop_size
+    assert all(
+        x.crop_size == patch_size for x in map_patches
+    )
+
+    agents_rasters_from_world_tfs: List[np.ndarray] = [
+        x.raster_from_world_tf for x in map_patches
+    ]
+    agents_patches: List[np.ndarray] = [x.data for x in map_patches]
+    agents_rot_angles_list: List[float] = [
+        x.rot_angle for x in map_patches]
+    agents_res_list: List[float] = [x.resolution for x in map_patches]
+
+    patch_data: torch.Tensor = torch.as_tensor(np.stack(agents_patches), dtype=torch.float)
+    agents_rot_angles: torch.Tensor = torch.as_tensor(
+        np.stack(agents_rot_angles_list), dtype=torch.float
+    )
+    agents_rasters_from_world_tf: torch.Tensor = torch.as_tensor(
+        np.stack(agents_rasters_from_world_tfs), dtype=torch.float
+    )
+    agents_resolution: torch.Tensor = torch.as_tensor(
+        np.stack(agents_res_list), dtype=torch.float
+    )
+
+    patch_size_y, patch_size_x = patch_data.shape[-2:]
+    center_y: int = patch_size_y // 2
+    center_x: int = patch_size_x // 2
+    half_extent: int = patch_size // 2
+
+    if torch.count_nonzero(agents_rot_angles) == 0:
+        agents_rasters_from_world_tf = torch.bmm(
+            torch.tensor(
+                [
+                    [
+                        [1.0, 0.0, half_extent],
+                        [0.0, 1.0, half_extent],
+                        [0.0, 0.0, 1.0],
+                    ]
+                ],
+                dtype=agents_rasters_from_world_tf.dtype,
+                device=agents_rasters_from_world_tf.device,
+            ).expand((agents_rasters_from_world_tf.shape[0], -1, -1)),
+            agents_rasters_from_world_tf,
+        )
+
+        rot_crop_patches = patch_data
+    else:
+        agents_rasters_from_world_tf = torch.bmm(
+            arr_utils.transform_matrices(
+                -agents_rot_angles,
+                torch.tensor([[half_extent, half_extent]]).expand(
+                    (agents_rot_angles.shape[0], -1)
+                ),
+            ),
+            agents_rasters_from_world_tf,
+        )
+
+        # Batch rotating patches by rot_angles.
+        rot_patches: torch.Tensor = kornia.geometry.transform.rotate(
+            patch_data, torch.rad2deg(agents_rot_angles))
+
+        # Center cropping via slicing.
+        rot_crop_patches = rot_patches[
+            ...,
+            center_y - half_extent : center_y + half_extent,
+            center_x - half_extent : center_x + half_extent,
+        ]
+
+    return rot_crop_patches, agents_resolution, agents_rasters_from_world_tf
 
 
 def decompress_values(data: np.ndarray) -> np.ndarray:
@@ -23,11 +304,11 @@ def decompress_values(data: np.ndarray) -> np.ndarray:
     # The delta for the first point is just its coordinates tuple, i.e. it is a "delta" from
     # the origin. For subsequent points, this field stores the difference between the point's
     # coordinates and the previous point's coordinates. This is for representation efficiency.
-    return np.cumsum(data, axis=0, dtype=float) / MM_PER_M
+    return np.cumsum(data, axis=0, dtype=float) / COMPRESSION_SCALE
 
 
 def compress_values(data: np.ndarray) -> np.ndarray:
-    return (np.diff(data, axis=0, prepend=0.0) * MM_PER_M).astype(np.int32)
+    return (np.diff(data, axis=0, prepend=0.0) * COMPRESSION_SCALE).astype(np.int64)
 
 
 def get_polyline_headings(points: np.ndarray) -> np.ndarray:
diff --git a/src/trajdata/utils/py_utils.py b/src/trajdata/utils/py_utils.py
new file mode 100644
index 0000000..c302aab
--- /dev/null
+++ b/src/trajdata/utils/py_utils.py
@@ -0,0 +1,13 @@
+import hashlib
+import json
+from typing import Dict, List, Set, Tuple, Union
+
+
+def hash_dict(o: Union[Dict, List, Tuple, Set]) -> str:
+    """
+    Makes a hash from a dictionary, list, tuple or set to any level, that contains
+    only other hashable types (including any lists, tuples, sets, and
+    dictionaries).
+    """
+    string_rep: str = json.dumps(o)
+    return hashlib.sha1(str.encode(string_rep)).hexdigest()
diff --git a/src/trajdata/utils/scene_utils.py b/src/trajdata/utils/scene_utils.py
index 804c498..1c29bc5 100644
--- a/src/trajdata/utils/scene_utils.py
+++ b/src/trajdata/utils/scene_utils.py
@@ -60,12 +60,13 @@ def interpolate_scene_dt(scene: Scene, desired_dt: float) -> None:
 
 def subsample_scene_dt(scene: Scene, desired_dt: float) -> None:
     dt_ratio: float = desired_dt / scene.dt
-    if not dt_ratio.is_integer():
+
+    if not is_integer_robust(dt_ratio):
         raise ValueError(
             f"Cannot subsample scene: {desired_dt} is not integer divisible by {scene.dt} for {str(scene)}"
         )
 
-    dt_factor: int = int(dt_ratio)
+    dt_factor: int = int(round(dt_ratio))
 
     # E.g., the scene is currently at dt = 0.1s (10 Hz),
     # but we want desired_dt = 0.5s (2 Hz).
@@ -86,3 +87,6 @@ def subsample_scene_dt(scene: Scene, desired_dt: float) -> None:
     scene.dt = desired_dt
     # Note we do not touch scene_info.env_metadata.dt, this will serve as our
     # source of the "original" data dt information.
+
+def is_integer_robust(x):
+    return abs(x-round(x))<1e-6
\ No newline at end of file
diff --git a/src/trajdata/utils/vis_utils.py b/src/trajdata/utils/vis_utils.py
index aab5acb..54fc036 100644
--- a/src/trajdata/utils/vis_utils.py
+++ b/src/trajdata/utils/vis_utils.py
@@ -1,12 +1,15 @@
 from collections import defaultdict
-from typing import List, Optional, Tuple
+from typing import List, Optional, Tuple, Dict
+
 
 import geopandas as gpd
 import numpy as np
 import pandas as pd
 import seaborn as sns
 from bokeh.models import ColumnDataSource, GlyphRenderer
-from bokeh.plotting import figure
+from bokeh.plotting import figure, curdoc
+import bokeh
+from bokeh.io import export_png
 from shapely.geometry import LineString, Polygon
 
 from trajdata.data_structures.agent import AgentType
@@ -20,7 +23,12 @@ from trajdata.maps.vec_map_elements import (
     RoadArea,
     RoadLane,
 )
-from trajdata.utils.arr_utils import transform_coords_2d_np
+from trajdata.utils.arr_utils import (
+    transform_coords_2d_np,
+    batch_nd_transform_points_pt,
+    batch_nd_transform_points_np,
+)
+from PIL import Image
 
 
 def apply_default_settings(fig: figure) -> None:
@@ -481,7 +489,7 @@ def draw_map_elems(
     vec_map: VectorMap,
     map_from_world_tf: np.ndarray,
     bbox: Optional[Tuple[float, float, float, float]] = None,
-    **kwargs
+    **kwargs,
 ) -> Tuple[GlyphRenderer, GlyphRenderer, GlyphRenderer, GlyphRenderer, GlyphRenderer]:
     """_summary_
 
diff --git a/tests/Drivesim_scene_generation.py b/tests/Drivesim_scene_generation.py
new file mode 100644
index 0000000..fadd074
--- /dev/null
+++ b/tests/Drivesim_scene_generation.py
@@ -0,0 +1,207 @@
+from collections import namedtuple
+from typing import Any, List, Optional
+from collections import defaultdict
+import numpy as np
+import pandas as pd
+
+from trajdata.data_structures.agent import AgentMetadata
+from trajdata.data_structures.environment import EnvMetadata
+from trajdata.data_structures.scene_metadata import Scene
+from trajdata import AgentType
+from trajdata.data_structures.agent import FixedExtent
+from trajdata.caching.scene_cache import SceneCache
+from trajdata.caching.env_cache import EnvCache
+from trajdata.caching.df_cache import DataFrameCache
+from trajdata.dataset_specific.scene_records import DrivesimSceneRecord
+from pathlib import Path
+import dill
+from trajdata import MapAPI, VectorMap
+import tbsim.utils.lane_utils as LaneUtils
+from bokeh.plotting import figure, show, save
+import bokeh
+
+def generate_agentmeta(initial_state,agent_names,agent_extents,T,dt,hist_types):
+    total_agent_data = list()
+    total_agent_metadata = list()
+    for x0,name,extent,htype in zip(initial_state,agent_names,agent_extents,hist_types):
+        agent_meta = AgentMetadata(name=name,
+                                   agent_type= AgentType.VEHICLE,
+                                   first_timestep = 0,
+                                   last_timestep = T-1,
+                                   extent=extent)
+        x,y,v,yaw = x0
+        if htype=="constvel":
+            vx = v*np.cos(yaw)
+            vy = v*np.sin(yaw)
+            ax = np.zeros_like(vx)
+            ay = np.zeros_like(vy)
+            yaw = yaw*np.ones(T)
+            scene_ts = np.arange(0,T)
+            x = x+vx*(scene_ts-T+1)*dt
+            y = y+vy*(scene_ts-T+1)*dt
+        elif htype in ["brake","accelerate"]:
+            acce = -2.0 if htype=="brake" else 2.0
+            seq = np.concatenate([np.ones(T-int(T/2))*int(T/2),np.arange(int(T/2)-1,-1,-1)])
+            vseq = np.clip(v-acce*seq*dt,0.0,10.0)
+            vx = vseq*np.cos(yaw)
+            vy = vseq*np.sin(yaw)
+            ax = vx[1:]-vx[:-1]
+            ax = np.concatenate([ax,ax[-1:]])
+            ay = vy[1:]-vy[:-1]
+            ay = np.concatenate([ay,ay[-1:]])
+            yaw = yaw*np.ones(T)
+            scene_ts = np.arange(0,T)
+            x = x+(vx.cumsum()-vx.sum())*dt
+            y = y+(vy.cumsum()-vy.sum())*dt
+
+        track_id = [name]*T
+        z = np.zeros(T)
+        
+        
+        pd_frame = pd.DataFrame({"agent_id":track_id,"scene_ts":scene_ts,"x":x,"y":y,"z":z,"heading":yaw,"vx":vx,"vy":vy,"ax":ax,"ay":ay})
+        total_agent_data.append(pd_frame)
+        total_agent_metadata.append(agent_meta)
+    total_agent_data = pd.concat(total_agent_data).set_index(["agent_id", "scene_ts"])
+    return total_agent_data,total_agent_metadata
+
+
+        
+def generate_drivesim_scene():
+    
+    repeat = 10
+
+    dt = 0.1
+    data_dir = ""
+    T = 20
+    cache_path = Path("/home/yuxiaoc/.unified_data_cache")
+    env_metadata = EnvMetadata("drivesim",
+                               data_dir,
+                               dt,
+                               parts=[("train",),("main",)],
+                               scene_split_map=defaultdict(lambda: "train"),
+                               map_locations=("main",))
+    env_cache = EnvCache(cache_path)
+    scene_records = list()
+    
+    agent_initial_state = [(np.array([-565,-1001,3.0,0]),"accelerate"),
+                           (np.array([-573,-1001,3.0,0]),"accelerate"),
+                           (np.array([-573,-1005,3.0,0.0]),"accelerate"),
+                           (np.array([-530.0,-976,0.0,-0.95*np.pi/2]),"constvel"),
+                           (np.array([-526.4,-976,0.0,-0.95*np.pi/2]),"brake"),
+                           (np.array([-507.8,-1027,0.0,np.pi/2+0.08]),"brake"),
+                           (np.array([-507.8,-1021,0.0,np.pi/2+0.08]),"brake"),
+                           (np.array([-504,-1021,0.0,np.pi/2+0.08]),"brake"),
+                           (np.array([-500.5,-1021,0.0,np.pi/2+0.08]),"brake"),
+                           (np.array([-480.5,-992,0.0,np.pi]),"brake"),
+                           (np.array([-473,-992,0.0,np.pi]),"brake"),
+                           (np.array([-489,-1049,8.0,np.pi/2-0.1]),"constvel"),
+                           (np.array([-488.4,-983,5.0,np.pi*0.75]),"accelerate"),
+                           (np.array([-524.2,-964.6,2.0,-0.95*np.pi/2]),"brake"),
+                           (np.array([-523.3,-976,0.0,-0.95*np.pi/2]),"brake"),
+                           (np.array([-519.5,-975,0.0,-0.92*np.pi/2]),"brake"),
+                           (np.array([-563.4,-1011,8.0,-0.18*np.pi/2]),"constvel"),
+                            ]
+
+    noise_spec = [1,1,1,2,2,3,3,3]
+    for r in range(repeat):
+        group1_xn =np.random.randn()*5
+        group1_yn = np.random.randn()*0.5
+        group1_vn = np.random.randn()*0.5
+        group1_psin = np.random.randn()*0.01
+        group2_xn = np.random.randn()*0.2
+        group2_yn = np.random.randn()*0.2
+        group2_vn = 0
+        group2_psin = np.random.randn()*0.01
+        group3_xn = np.random.randn()*0.2
+        group3_yn = np.random.randn()*0.2
+        group3_vn = 0
+        group3_psin = np.random.randn()*0.01
+        group1_n = np.array([group1_xn,group1_yn,group1_vn,group1_psin])
+        group2_n = np.array([group2_xn,group2_yn,group2_vn,group2_psin])
+        group3_n = np.array([group3_xn,group3_yn,group3_vn,group3_psin])
+        init_state=np.stack([x0 for x0,_ in agent_initial_state])
+        hist_types = [htype for _,htype in agent_initial_state]
+        for i in range(len(init_state)):
+            if i <len(noise_spec):
+                if noise_spec[i] == 1:
+                    init_state[i]+=group1_n
+                elif noise_spec[i] == 2:
+                    init_state[i]+=group2_n
+                elif noise_spec[i] == 3:
+                    init_state[i]+=group3_n
+        
+        agent_names =["ego"]+[str(i) for i in range(769,769+len(agent_initial_state)-1)]
+        agent_extents = [FixedExtent(length=4.0,width=2.0,height=2.0)]*len(agent_names)
+        
+        total_agent_data,total_agent_metadata = generate_agentmeta(init_state,agent_names,agent_extents,T,dt,hist_types)
+        Scene_main = Scene(env_metadata,
+                        name= f"scene_{r}",
+                        location="main",
+                        data_split = "train",
+                        length_timesteps=T,
+                        raw_data_idx=0,
+                        data_access_info=None,
+                        description = None,
+                        agents = total_agent_metadata,
+                        agent_presence=[total_agent_metadata]*T,)
+        
+        DataFrameCache.save_agent_data(total_agent_data, cache_path, Scene_main)
+    
+        env_cache.save_scene(Scene_main)
+        scene_r_rec = DrivesimSceneRecord(name=f"scene_{r}",
+                                        location = "main",
+                                        length = T,
+                                        split = "train",
+                                        # desc: str
+                                        data_idx=0)
+        scene_records.append(scene_r_rec)
+    
+    with open(cache_path/"drivesim"/"scenes_list.dill", "wb") as f:
+        dill.dump(scene_records, f)
+    visualize_scene(agent_initial_state)
+
+def plot_lane(lane,plot,color="grey"):
+    bdry_l,_ = LaneUtils.get_edge(lane,dir="L",num_pts=15)
+    bdry_r,_ = LaneUtils.get_edge(lane,dir="R",num_pts=15)
+    lane_center,_ = LaneUtils.get_edge(lane,dir="C",num_pts=15)
+    bdry_xy = np.concatenate([bdry_l,np.flip(bdry_r,0)],0)
+    patch_glyph = plot.patch(x=bdry_xy[:,0],y=bdry_xy[:,1],fill_alpha=0.5,color = color)
+    centerline_glyph = plot.line(x=lane_center[:,0],y=lane_center[:,1],line_dash="dashed",line_width=2)
+    return patch_glyph,centerline_glyph
+def get_agent_edge(xy,h,extent):
+    
+    edges = np.array([[0.5,0.5],[0.5,-0.5],[-0.5,-0.5],[-0.5,0.5]])*extent[np.newaxis,:2]
+    rotM = np.array([[np.cos(h),-np.sin(h)],[np.sin(h),np.cos(h)]])
+    edges = (rotM@edges[...,np.newaxis]).squeeze(-1)+xy[np.newaxis,:]
+    return edges
+def visualize_scene(agent_initial_state):
+    plot = figure(name='base',height=1000, width=1000, title="traffic Animation",  
+                tools="reset,save,pan,hover,wheel_zoom",toolbar_location="below",match_aspect=True)
+    plot.xgrid.grid_line_color = None
+    plot.ygrid.grid_line_color = None  
+    plot.axis.visible=False
+    cache_path = Path("~/.unified_data_cache").expanduser()
+    mapAPI = MapAPI(cache_path)
+    map_name = "drivesim:main"
+    vec_map = mapAPI.get_map(map_name, scene_cache=None)
+    xyz = np.array([-500,-1000,0])
+    lanes_t = vec_map.get_lanes_within(xyz,60)
+    lanecenter_glyph = dict()
+    lanes = set()
+    lanepatch_glyph = dict()
+    for lane in lanes_t:
+        patch_glyph,centerline_glyph = plot_lane(lane,plot)
+        lanecenter_glyph[lane] = centerline_glyph
+        lanepatch_glyph[lane] = patch_glyph
+        lanes.add(lane)
+    extent = np.array([4.5,2.5])
+    agent_patch = dict()
+    palette = bokeh.palettes.Category20[20]
+    agent_color = ["blueviolet"] + [palette[i%20] for i in range(len(agent_initial_state)-1)]
+    # for i in range(len(agent_initial_state)):
+    #     edges = get_agent_edge(agent_initial_state[i][0][:2],agent_initial_state[i][0][3],extent)
+    #     agent_patch[i] = plot.patch(x=edges[:,0],y=edges[:,1],color=agent_color[i])
+    save(plot)
+    show(plot)
+if __name__ == "__main__":
+    generate_drivesim_scene()
diff --git a/tests/test_raster_map.py b/tests/test_raster_map.py
new file mode 100644
index 0000000..9bfc197
--- /dev/null
+++ b/tests/test_raster_map.py
@@ -0,0 +1,100 @@
+import unittest
+from pathlib import Path
+from typing import Dict, List
+
+from trajdata import MapAPI, VectorMap
+
+import unittest
+from collections import defaultdict
+
+import torch
+
+from trajdata import AgentType, UnifiedDataset, SceneBatch
+from trajdata.dataset import DataLoader
+from trajdata.utils.batch_utils import get_raster_maps_for_scene_batch
+
+
+class TestRasterMap(unittest.TestCase):
+    def __init__(self, methodName: str = "batchConversion") -> None:
+        super().__init__(methodName)
+
+        data_source = "nusc_mini"
+        history_sec = 2.0
+        prediction_sec = 6.0
+
+        attention_radius = defaultdict(
+            lambda: 20.0
+        )  # Default range is 20m unless otherwise specified.
+        attention_radius[(AgentType.PEDESTRIAN, AgentType.PEDESTRIAN)] = 10.0
+        attention_radius[(AgentType.PEDESTRIAN, AgentType.VEHICLE)] = 20.0
+        attention_radius[(AgentType.VEHICLE, AgentType.PEDESTRIAN)] = 20.0
+        attention_radius[(AgentType.VEHICLE, AgentType.VEHICLE)] = 30.0
+
+        self._map_params = {"px_per_m": 2, "map_size_px": 100, "offset_frac_xy": (-0.75, 0.0)}
+
+        self._scene_dataset = UnifiedDataset(
+            centric="scene",
+            desired_data=[data_source],
+            history_sec=(history_sec, history_sec),
+            future_sec=(prediction_sec, prediction_sec),
+            agent_interaction_distances=attention_radius,
+            incl_robot_future=False,
+            incl_raster_map=True,
+            raster_map_params=self._map_params,
+            only_predict=[AgentType.VEHICLE, AgentType.PEDESTRIAN],
+            no_types=[AgentType.UNKNOWN],
+            num_workers=0,
+            standardize_data=True,
+            data_dirs={
+                "nusc_mini": "~/datasets/nuScenes",
+            },
+        )
+
+        self._scene_dataloader = DataLoader(
+            self._scene_dataset,
+            batch_size=4,
+            shuffle=False,
+            collate_fn=self._scene_dataset.get_collate_fn(),
+            num_workers=0,
+        )
+
+    def _assert_allclose_with_nans(self, tensor1, tensor2, atol=1e-8):
+        """
+        asserts that the two tensors have nans in the same locations, and the non-nan
+        elements all are close.
+        """
+        # Check nans are in the same place
+        self.assertFalse(
+            torch.any(  # True if there's any mismatch
+                torch.logical_xor(  # True where either tensor1 or tensor 2 has nans, but not both (mismatch)
+                    torch.isnan(tensor1),  # True where tensor1 has nans
+                    torch.isnan(tensor2),  # True where tensor2 has nans
+                )
+            ),
+            msg="Nans occur in different places.",
+        )
+        valid_mask = torch.logical_not(torch.isnan(tensor1))
+        self.assertTrue(
+            torch.allclose(tensor1[valid_mask], tensor2[valid_mask], atol=atol),
+            msg="Non-nan values don't match.",
+        )
+
+    def test_map_transform_scenebatch(self):
+        scene_batch: SceneBatch
+        for i, scene_batch in enumerate(self._scene_dataloader):
+
+            # Make the tf double for more accurate transform.
+            scene_batch.centered_world_from_agent_tf = scene_batch.centered_world_from_agent_tf.double()
+
+            maps, maps_resolution, raster_from_world_tf = get_raster_maps_for_scene_batch(
+                scene_batch, self._scene_dataset.cache_path, "nusc_mini", self._map_params)
+
+            self._assert_allclose_with_nans(scene_batch.rasters_from_world_tf, raster_from_world_tf, atol=1e-2)
+            self._assert_allclose_with_nans(scene_batch.maps_resolution, maps_resolution)
+            self._assert_allclose_with_nans(scene_batch.maps, maps, atol=1e-4)
+
+            if i > 50:
+                break
+
+if __name__ == "__main__":
+    unittest.main(catchbreak=False)
