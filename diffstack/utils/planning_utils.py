from collections import defaultdict
import pdb
import numpy as np
from scipy.interpolate import interp1d
import torch

from diffstack.models.cnn_roi_encoder import rasterized_ROI_align
from diffstack.utils.batch_utils import batch_utils
import diffstack.utils.geometry_utils as GeoUtils
from diffstack.utils.tensor_utils import unsqueeze
TRAJ_INDEX = [0, 1, 4]  
from diffstack.utils.tree import Tree
from Pplan.Sampling.trajectory_tree import TrajTree
class AgentTrajTree(Tree):
    def __init__(self, traj, parent, depth, prob=None):
        self.traj = traj
        self.children = list()
        self.parent = parent
        if parent is not None:
            parent.expand(self)
        self.depth = depth
        self.prob = prob
        self.attribute = dict()

# The state in Pplan contains more higher order∆í derivatives, TRAJ_INDEX selects x,y, and heading 
# out of the longer state vector



def get_collision_loss(
    ego_trajectories,
    agent_trajectories,
    ego_extents,
    agent_extents,
    raw_types,
    prob=None,
    col_funcs=None,
    discount_rear = None,
):
    """Get veh-veh and veh-ped collision loss."""
    # with torch.no_grad():
    ego_edges, type_mask = batch_utils().gen_ego_edges(
        ego_trajectories, agent_trajectories, ego_extents, agent_extents, raw_types
    )
    if discount_rear is not None:
        theta = ego_trajectories[...,None,0,2]
        behind_flag = ((agent_trajectories[...,0,0]-ego_trajectories[...,None,0,0])*torch.cos(theta)+ (agent_trajectories[...,0,1]-ego_trajectories[...,None,0,1])*torch.sin(theta)) < 0.5
        scale = (0.02*behind_flag + 1.0*~behind_flag)[...,None]
    else:
        scale = 1.0
    if col_funcs is None:
        col_funcs = {
            "VV": GeoUtils.VEH_VEH_collision,
            "VP": GeoUtils.VEH_PED_collision,
        }
    B, N, T = ego_trajectories.shape[:3]
    col_loss = torch.zeros([B, N]).to(ego_trajectories.device)
    for et, func in col_funcs.items():
        dis = func(ego_edges[..., 0:3],ego_edges[..., 3:6],ego_edges[..., 6:8],ego_edges[..., 8:])
        if dis.nelement() > 0:
            col_loss += torch.sum(torch.sigmoid(-dis*4) * scale*type_mask[et][:,None,:,None], dim=[2,3])/T
    
    return col_loss


def get_drivable_area_loss(
    ego_trajectories, raster_from_agent, dis_map, ego_extents
):
    """Cost for road departure."""
    with torch.no_grad():

        lane_flags = rasterized_ROI_align(
            dis_map,
            ego_trajectories[..., :2],
            ego_trajectories[..., 2:],
            raster_from_agent,
            torch.ones(*ego_trajectories.shape[:3]
                       ).to(ego_trajectories.device),
            ego_extents.unsqueeze(1).repeat(1, ego_trajectories.shape[1], 1),
            1,
        ).squeeze(-1)
    return lane_flags.max(dim=-1)[0]

def get_lane_loss_simple(ego_trajectories, raster_from_agent, dis_map):
    h,w = dis_map.shape[-2:]
    
    raster_xy = GeoUtils.batch_nd_transform_points(ego_trajectories[...,:2],raster_from_agent)
    raster_xy[...,0] = raster_xy[...,0].clip(0,w-1e-5)
    raster_xy[...,1] = raster_xy[...,1].clip(0,h-1e-5)
    raster_xy = raster_xy.long()
    raster_xy_flat = (raster_xy[...,1]*w+raster_xy[...,0])
    raster_xy_flat = raster_xy_flat.flatten()
    lane_loss = (dis_map.flatten()[raster_xy_flat]).reshape(*raster_xy.shape[:2])
    return lane_loss.max(dim=-1)[0]

def get_terminal_likelihood_reward(
    ego_trajectories, raster_from_agent, log_likelihood
):
    """Cost for road departure."""

    log_likelihood = (log_likelihood-log_likelihood.mean())/log_likelihood.std()
    h,w = log_likelihood.shape[-2:]
    
    raster_xy = GeoUtils.batch_nd_transform_points(ego_trajectories[...,-1,:2],raster_from_agent)
    raster_xy[...,0] = raster_xy[...,0].clip(0,w-1e-5)
    raster_xy[...,1] = raster_xy[...,1].clip(0,h-1e-5)
    raster_xy = raster_xy.long()
    raster_xy_flat = (raster_xy[...,1]*w+raster_xy[...,0])

    ll_reward = log_likelihood.flatten()[raster_xy_flat]
    return ll_reward

def get_progress_reward(ego_trajectories,d_sat = 10):
    dis = torch.linalg.norm(ego_trajectories[...,-1,:2]-ego_trajectories[...,0,:2],dim=-1)
    return 2/np.pi*torch.atan(dis/d_sat)


def get_total_distance(ego_trajectories):
    """Reward that incentivizes progress."""
    # Assume format [..., T, 3]
    assert ego_trajectories.shape[-1] == 3
    diff = ego_trajectories[..., 1:, :] - ego_trajectories[..., :-1, :]
    dist = torch.norm(diff[..., :2], dim=-1)
    total_dist = torch.sum(dist, dim=-1)
    return total_dist


def ego_sample_planning(
        ego_trajectories,
        agent_trajectories,
        ego_extents,
        agent_extents,
        raw_types,
        raster_from_agent,
        dis_map,
        weights,
        log_likelihood=None,
        col_funcs=None,
):
    """A basic cost function for prediction-and-planning"""
    col_loss = get_collision_loss(
        ego_trajectories,
        agent_trajectories,
        ego_extents,
        agent_extents,
        raw_types,
        col_funcs,
    )
    lane_loss = get_drivable_area_loss(
        ego_trajectories, raster_from_agent, dis_map, ego_extents
    )
    progress = get_total_distance(ego_trajectories)

    log_likelihood = 0 if log_likelihood is None else log_likelihood
    if log_likelihood.ndim==3:
        log_likelihood = get_terminal_likelihood_reward(ego_trajectories, raster_from_agent, log_likelihood)

    total_score = (
            + weights["likelihood_weight"] * log_likelihood
            + weights["progress_weight"] * progress
            - weights["collision_weight"] * col_loss
            - weights["lane_weight"] * lane_loss
    )


    return torch.argmax(total_score, dim=1)


class TreeMotionPolicy(object):
    """ A trajectory tree policy as the result of contingency planning

    """
    def __init__(self,stage,num_frames_per_stage,ego_root,scenario_root,cost_to_go,leaf_idx,curr_node):
        self.stage = stage
        self.num_frames_per_stage = num_frames_per_stage
        self.ego_root = ego_root
        self.scenario_root = scenario_root
        self.cost_to_go = cost_to_go
        self.leaf_idx = leaf_idx
        self.curr_node= curr_node

    def identify_branch(self,ego_node,scene_traj):

        assert scene_traj.shape[-2]<self.stage*self.num_frames_per_stage
        assert ego_node.total_traj.shape[0]-1>=scene_traj.shape[-2]

        remain_traj = scene_traj
        curr_scenario_node = self.scenario_root
        ego_leaf_index = self.leaf_idx[ego_node]
        while remain_traj.shape[1]>0:
            seg_length = min(remain_traj.shape[-2],self.num_frames_per_stage)
            dis = [torch.linalg.norm(child.traj[ego_leaf_index,:,:seg_length,:2]-remain_traj[:,:seg_length,:2],dim=-1).sum().item()\
                    for child in curr_scenario_node.children]
            idx = torch.argmin(torch.tensor(dis)).item()

            curr_scenario_node = curr_scenario_node.children[idx]
            
            remain_traj = remain_traj[...,seg_length:,:]
            remain_num_frames = curr_scenario_node.traj.shape[-2]-seg_length
            if curr_scenario_node.stage>=self.curr_node.stage:
                break
        return curr_scenario_node

    def get_plan(self,scene_traj,horizon):
        if scene_traj is None:
            T = 0
            remain_num_frames = self.num_frames_per_stage
        else:
            T = scene_traj.shape[-2]
            remain_num_frames = self.curr_node.total_traj.shape[0]-1-T
            assert remain_num_frames>-self.num_frames_per_stage
            if remain_num_frames<=0:
                assert not self.curr_node.isleaf()
                curr_scenario_node = self.identify_branch(self.curr_node,scene_traj)
                Q = [self.cost_to_go[(child,curr_scenario_node)] for child in self.curr_node.children]
                idx = torch.argmin(torch.tensor(Q)).item()
                self.curr_node = self.curr_node.children[idx]
                remain_num_frames+=self.curr_node.traj.shape[0]
        traj = self.curr_node.traj[-remain_num_frames:,TRAJ_INDEX]
        if not self.curr_node.isleaf():
            traj = torch.cat((traj,self.curr_node.children[0].traj[:,TRAJ_INDEX]),-2)
        if traj.shape[0]>=horizon:
            return traj[:horizon]
        else:
            traj_patched = torch.cat((traj,traj[-1].tile(horizon-traj.shape[0],1)))
            return traj_patched

class VectorizedTreeMotionPolicy(TreeMotionPolicy):
    """ A vectorized trajectory tree policy as the result of contingency planning

    """
    def __init__(self,stage,num_frames_per_stage,ego_tree,children_indices,scenario_tree,cost_to_go,leaf_idx,curr_node):
        self.stage = stage
        self.num_frames_per_stage = num_frames_per_stage
        self.ego_tree = ego_tree
        self.ego_root = ego_tree[0][0]
        self.children_indices = children_indices
        self.scenario_tree = scenario_tree
        self.scenario_root = scenario_tree[0][0]
        self.cost_to_go = cost_to_go
        self.leaf_idx = leaf_idx
        self.curr_node= curr_node

    def identify_branch(self,ego_node,scene_traj):

        assert scene_traj.shape[-2]<self.stage*self.num_frames_per_stage
        assert ego_node.total_traj.shape[0]-1>=scene_traj.shape[-2]

        remain_traj = scene_traj
        curr_scenario_node = self.scenario_root
        
        stage = ego_node.depth
        ego_stage_index = self.ego_tree[stage].index(ego_node)
        ego_leaf_index = self.leaf_idx[stage][ego_stage_index].item()
        while remain_traj.shape[1]>0:
            seg_length = min(remain_traj.shape[-2],self.num_frames_per_stage)
            dis = [torch.linalg.norm(child.traj[ego_leaf_index,:,:seg_length,:2]-remain_traj[:,:seg_length,:2],dim=-1).sum().item()\
                    for child in curr_scenario_node.children]
            idx = torch.argmin(torch.tensor(dis)).item()

            curr_scenario_node = curr_scenario_node.children[idx]
            
            remain_traj = remain_traj[...,seg_length:,:]
            remain_num_frames = curr_scenario_node.traj.shape[-2]-seg_length
            if curr_scenario_node.stage>=self.curr_node.stage:
                break
        return curr_scenario_node

    def get_plan(self,scene_traj,horizon):
        if scene_traj is None:
            T = 0
            remain_num_frames = self.num_frames_per_stage
        else:
            T = scene_traj.shape[-2]
            remain_num_frames = self.curr_node.total_traj.shape[0]-1-T
            assert remain_num_frames>-self.num_frames_per_stage
            if remain_num_frames<=0:
                assert not self.curr_node.isleaf()
                curr_scenario_node = self.identify_branch(self.curr_node,scene_traj)
                assert curr_scenario_node.depth==self.curr_node.depth
                stage = self.curr_node.depth
                scene_node_idx = self.scenario_tree[stage].index(curr_scenario_node)
                curr_node_idx = self.ego_tree[stage].index(self.curr_node)
                Q = self.cost_to_go[stage][scene_node_idx,self.children_indices[stage][curr_node_idx]]
                idx = torch.argmin(Q).item()
                self.curr_node = self.curr_node.children[idx]
                remain_num_frames+=self.curr_node.traj.shape[0]

        traj = self.curr_node.traj[-remain_num_frames:,TRAJ_INDEX]
        if not self.curr_node.isleaf():
            traj = torch.cat((traj,self.curr_node.children[0].traj[:,TRAJ_INDEX]),-2)
        if traj.shape[0]>=horizon:
            return traj[:horizon]
        else:
            traj_patched = torch.cat((traj,traj[-1].tile(horizon-traj.shape[0],1)))
            return traj_patched
            

def tiled_to_tree(total_traj,prob,num_stage,num_frames_per_stage,M):
    """Turning a trajectory tree in tiled form to a tree data structure

    Args:
        total_traj (torch.tensor or np.ndarray): tiled trajectory tree
        prob (torch.tensor or np.ndarray): probability of the modes
        num_stage (int): number of layers of the tree
        num_frames_per_stage (int): number of time frames per layer
        M (int): branching factor

    Returns:
        nodes (dict[int:List(AgentTrajTree)]): all branches of the trajectory tree nodes indexed by layer
    """

    # total_traj = TensorUtils.reshape_dimensions_single(total_traj,2,3,[M]*num_stage)
    x0 = AgentTrajTree(None, None, 0)
    nodes = defaultdict(lambda:list())
    nodes[0].append(x0)
    for t in range(num_stage):
        interval = M**(num_stage-t-1)
        tiled_traj = total_traj[...,::interval,:,t*num_frames_per_stage:(t+1)*num_frames_per_stage,:]
        for i in range(M**(t+1)):
            parent_idx = int(i/M)
            p = prob[:,i*interval:(i+1)*interval].sum(-1)
            node = AgentTrajTree(tiled_traj[:,i], nodes[t][parent_idx], t + 1, prob=p)
            nodes[t+1].append(node)
    return nodes


def contingency_planning(ego_tree,
                         ego_extents,
                         agent_traj,
                         mode_prob,
                         agent_extents,
                         agent_types,
                         raster_from_agent,
                         dis_map,
                         weights,
                         num_frames_per_stage,
                         M,
                         dt,
                         col_funcs=None,
                         log_likelihood=None,
                         pert_std = None):
    """A sampling-based contingency planning algorithm

    Args:
        ego_tree (Dict[int:List[TrajTree]]): ego trajectory tree
        ego_extents (Tensor): [2]
        agent_traj (Tensor): EC x M^s x Na x (s*Ts) x 3 scenario tree predicted
        mode_prob (Tensor): EC x M^s
        agent_extents (Tensor): Na x 2
        agent_types (Tensor): Na
        raster_from_agent (Tensor): 3 x 3
        dis_map (Tensor): 224 x 224 (same as rasterized feature dimension) distance map
        weights (Dict): weights of various costs
        num_frames_per_stage (int): Ts
        M (int): branching factor
        col_funcs (function handle, optional): in case custom collision function is used. Defaults to None.

    Returns:
        TreeMotionPolicy: optimal motion policy
    """
    
    num_stage = len(ego_tree)-1
    ego_root = ego_tree[0][0]
    device = agent_traj.device
    leaf_idx = defaultdict(lambda:list())
    for stage in range(num_stage,-1,-1):
        for node in ego_tree[stage]:
            if node.isleaf():
                leaf_idx[node]=[ego_tree[stage].index(node)]
            else:
                leaf_idx[node] = []
                for child in node.children:
                    leaf_idx[node] = leaf_idx[node]+leaf_idx[child]
    

    V = dict()
    L = dict()
    Q = dict()
    scenario_tree = tiled_to_tree(agent_traj,mode_prob,num_stage,num_frames_per_stage,M)
    scenario_root = scenario_tree[0][0]
    v0 = ego_root.traj[0,2]
    d_sat = v0.clip(min=2.0)*num_frames_per_stage*dt
    for stage in range(num_stage,0,-1):
        if stage==0:
            total_loss = torch.zeros([1,1],device=device)
        else:
            ego_nodes = ego_tree[stage]
            indices = [leaf_idx[node][0] for node in ego_nodes]
            ego_traj = [node.traj[:,TRAJ_INDEX] for node in ego_nodes]
            ego_traj = torch.stack(ego_traj,0)
            agent_nodes = scenario_tree[stage]
            agent_traj = [node.traj[indices] for node in agent_nodes]
            agent_traj = torch.stack(agent_traj,0)
            ego_traj_tiled = ego_traj.unsqueeze(0).repeat(len(agent_nodes),1,1,1)
            col_loss = get_collision_loss(ego_traj_tiled,
                                        agent_traj,
                                        ego_extents.tile(len(agent_nodes),1),
                                        agent_extents.tile(len(agent_nodes),1,1),
                                        agent_types.tile(len(agent_nodes),1),
                                        col_funcs,
                                        )


            lane_loss = get_drivable_area_loss(ego_traj.unsqueeze(0), raster_from_agent.unsqueeze(0), dis_map.unsqueeze(0), ego_extents.unsqueeze(0))
            # lane_loss = get_lane_loss_simple(ego_traj,raster_from_agent,dis_map).unsqueeze(0)

            progress_reward = get_progress_reward(ego_traj,d_sat=d_sat)
            
            total_loss = weights["collision_weight"]*col_loss+weights["lane_weight"]*lane_loss-weights["progress_weight"]*progress_reward.unsqueeze(0)
            if pert_std is not None:
                total_loss +=torch.randn(total_loss.shape[1],device=device).unsqueeze(0)*pert_std
            if log_likelihood is not None and stage==num_stage:
                ll_reward = get_terminal_likelihood_reward(ego_traj, raster_from_agent, log_likelihood)
                total_loss = total_loss-weights["likelihood_weight"]*ll_reward
            

        for i in range(len(ego_nodes)):
            for j in range(len(agent_nodes)):
                L[(ego_nodes[i],agent_nodes[j])] = total_loss[j,i]
                if stage==num_stage:
                    V[(ego_nodes[i],agent_nodes[j])] = float(total_loss[j,i])
                else:
                    children_cost_to_go = [Q[(child,agent_nodes[j])] for child in ego_nodes[i].children]
                    V[(ego_nodes[i],agent_nodes[j])] = float(total_loss[j,i])+min(children_cost_to_go)

            if stage>0:
                for agent_node in scenario_tree[stage-1]:
                    cost_i = []
                    prob_i = []
                    for child in agent_node.children:
                        cost_i.append(V[ego_nodes[i],child])
                        prob_i.append(child.prob[leaf_idx[ego_nodes[i]]].sum())
                    cost_i = torch.tensor(cost_i,device=device)
                    prob_i = torch.stack(prob_i)
                    Q[(ego_nodes[i],agent_node)] = float((cost_i*prob_i).sum()/prob_i.sum())
    Q_root = [Q[(child,scenario_root)] for child in ego_root.children]
    idx = torch.argmin(torch.tensor(Q_root)).item()
    optimal_node = ego_root.children[idx]
    motion_policy = TreeMotionPolicy(num_stage,
                                     num_frames_per_stage,
                                     ego_root,
                                     scenario_root,
                                     Q,
                                     leaf_idx,
                                     optimal_node)
    motion_policy.get_plan(None,num_stage*num_frames_per_stage)
    return motion_policy

def contingency_planning_parallel(ego_tree,
                         ego_extents,
                         agent_traj,
                         mode_prob,
                         agent_extents,
                         agent_types,
                         raster_from_agent,
                         dis_map,
                         weights,
                         num_frames_per_stage,
                         M,
                         dt,
                         col_funcs=None,
                         log_likelihood=None,
                         pert_std = None):
    """A sampling-based contingency planning algorithm

    Args:
        ego_tree (Dict[int:List[TrajTree]]): ego trajectory tree
        ego_extents (Tensor): [2]
        agent_traj (Tensor): EC x M^s x Na x (s*Ts) x 3 scenario tree predicted
        mode_prob (Tensor): EC x M^s
        agent_extents (Tensor): Na x 2
        agent_types (Tensor): Na
        raster_from_agent (Tensor): 3 x 3
        dis_map (Tensor): 224 x 224 (same as rasterized feature dimension) distance map
        weights (Dict): weights of various costs
        num_frames_per_stage (int): Ts
        M (int): branching factor
        col_funcs (function handle, optional): in case custom collision function is used. Defaults to None.

    Returns:
        TreeMotionPolicy: optimal motion policy
    """
    device=agent_traj.device
    children_indices = TrajTree.get_children_index_torch(ego_tree)
    num_stage = len(ego_tree)-1
    ego_root = ego_tree[0][0]

    leaf_idx = {num_stage:torch.arange(len(ego_tree[num_stage]),device=device)}
    stage_prob = {num_stage:mode_prob.T}
    for stage in range(num_stage-1,-1,-1):
        leaf_idx[stage] = leaf_idx[stage+1][children_indices[stage][:,0]]
        prob_next = stage_prob[stage+1]
        stage_prob[stage] = prob_next.reshape(-1,M,prob_next.shape[-1])[:,:,children_indices[stage][:,0]].sum(1)

    V = dict()
    L = dict()
    Q = dict()


    scenario_tree = tiled_to_tree(agent_traj,mode_prob,num_stage,num_frames_per_stage,M)
    scenario_root = scenario_tree[0][0]
    v0 = ego_root.traj[0,2]
    d_sat = v0.clip(min=2.0)*num_frames_per_stage*dt

    
    
    for stage in range(num_stage,-1,-1):
        if stage==0:
            total_loss = torch.zeros([1,1],device=device)
        else:
            #calculate stage cost
            ego_nodes = ego_tree[stage]
            ego_traj = [node.traj[:,TRAJ_INDEX] for node in ego_nodes]
            ego_traj = torch.stack(ego_traj,0)
            agent_nodes = scenario_tree[stage]
            
            agent_traj = [node.traj[leaf_idx[stage]] for node in agent_nodes]

            agent_traj = torch.stack(agent_traj,0)
            ego_traj_tiled = ego_traj.unsqueeze(0).repeat(len(agent_nodes),1,1,1)
            col_loss = get_collision_loss(ego_traj_tiled,
                                        agent_traj,
                                        ego_extents.tile(len(agent_nodes),1),
                                        agent_extents.tile(len(agent_nodes),1,1),
                                        agent_types.tile(len(agent_nodes),1),
                                        col_funcs,
                                        )


            lane_loss = get_drivable_area_loss(ego_traj.unsqueeze(0), raster_from_agent.unsqueeze(0), dis_map.unsqueeze(0), ego_extents.unsqueeze(0))
            # lane_loss = get_lane_loss_simple(ego_traj,raster_from_agent,dis_map).unsqueeze(0)

            progress_reward = get_progress_reward(ego_traj,d_sat=d_sat)
            
            total_loss = weights["collision_weight"]*col_loss+weights["lane_weight"]*lane_loss-weights["progress_weight"]*progress_reward.unsqueeze(0)
            if pert_std is not None:
                total_loss +=torch.randn(total_loss.shape[1],device=device).unsqueeze(0)*pert_std
            if log_likelihood is not None and stage==num_stage:
                ll_reward = get_terminal_likelihood_reward(ego_traj, raster_from_agent, log_likelihood)
                total_loss = total_loss-weights["likelihood_weight"]*ll_reward
            
        L[stage] = total_loss
        if stage==num_stage:
            V[stage] = total_loss
        else:
            children_idx = children_indices[stage]
            # add the last Q value as inf since empty children index are padded with -1
            Q_prime = torch.cat((Q[stage],torch.full([Q[stage].shape[0],1],np.inf,device=device)),1)
            Q_by_node = Q_prime[:,children_idx]
            V[stage] = total_loss+Q_by_node.min(dim=-1)[0]
        
        if stage>0:
            children_V = V[stage]
            children_V = children_V.reshape(-1,M,children_V.shape[-1])
            prob = stage_prob[stage]
            prob = prob.reshape(-1,M,prob.shape[-1])
            prob_normalized = prob/prob.sum(dim=1,keepdim=True)
            Q[stage-1] = (children_V*prob_normalized).sum(dim=1)

    idx = Q[0].argmin().item()

    motion_policy = VectorizedTreeMotionPolicy(num_stage,
                                               num_frames_per_stage,
                                               ego_tree,
                                               children_indices,
                                               scenario_tree,
                                               Q,
                                               leaf_idx,
                                               ego_root.children[idx])
    return motion_policy

def one_shot_planning(ego_tree,
                         ego_extents,
                         agent_traj,
                         mode_prob,
                         agent_extents,
                         agent_types,
                         raster_from_agent,
                         dis_map,
                         weights,
                         num_frames_per_stage,
                         M,
                         dt,
                         col_funcs=None,
                         log_likelihood=None,
                         pert_std = None,
                         strategy="all"):

    """Alternative of contingency planning, try to avoid all predicted trajectories

    Args:
        ego_tree (Dict[int:List[TrajTree]]): ego trajectory tree
        ego_extents (Tensor): [2]
        agent_traj (Tensor): EC x M^s x Na x (s*Ts) x 3 scenario tree predicted
        mode_prob (Tensor): EC x M^s
        agent_extents (Tensor): Na x 2
        agent_types (Tensor): Na
        raster_from_agent (Tensor): 3 x 3
        dis_map (Tensor): 224 x 224 (same as rasterized feature dimension) distance map
        weights (Dict): weights of various costs
        num_frames_per_stage (int): Ts
        M (int): branching factor
        col_funcs (function handle, optional): in case custom collision function is used. Defaults to None.

    Returns:
        TreeMotionPolicy: optimal motion policy
    """
    assert strategy=="all" or strategy=="maximum"
    num_stage = len(ego_tree)-1
    ego_root = ego_tree[0][0]
    
    ego_traj = [node.total_traj[1:,TRAJ_INDEX] for node in ego_tree[num_stage]]
    ego_traj = torch.stack(ego_traj,0)
    ego_traj_tiled = ego_traj.unsqueeze(1).repeat_interleave(agent_traj.shape[1],1)
    Ne = ego_traj.shape[0]
    if strategy=="maximum":
        idx = mode_prob.argmax(dim=1)
        idx = idx.reshape(Ne,*[1]*(agent_traj.ndim-1))
        agent_traj = agent_traj.take_along_dim(idx,1)
    col_loss = get_collision_loss(ego_traj_tiled,agent_traj,ego_extents.tile(Ne,1),agent_extents.tile(Ne,1,1),agent_types.tile(Ne,1),col_funcs)
    col_loss = col_loss.max(dim=1)[0]
    lane_loss = get_drivable_area_loss(ego_traj.unsqueeze(0), raster_from_agent.unsqueeze(0), dis_map.unsqueeze(0), ego_extents.unsqueeze(0)).squeeze(0)
    v0 = ego_root.traj[0,2]
    d_sat = v0.clip(min=2.0)*num_frames_per_stage*dt
    progress_reward = get_progress_reward(ego_traj,d_sat=d_sat)
    total_loss = weights["collision_weight"]*col_loss+weights["lane_weight"]*lane_loss-weights["progress_weight"]*progress_reward
    if pert_std is not None:
        total_loss +=torch.randn(total_loss.shape[0],device=total_loss.device)*pert_std
    if log_likelihood is not None:
        ll_reward = get_terminal_likelihood_reward(ego_traj, raster_from_agent, log_likelihood)
        total_loss = total_loss-weights["likelihood_weight"]*ll_reward

    idx = total_loss.argmin()
    return ego_traj[idx]

            
def obtain_ref(line, x, v, N, dt):
    """obtain desired trajectory for the MPC controller

    Args:
        line (np.ndarray): centerline of the lane [n, 3]
        x (np.ndarray): position of the vehicle
        v (np.ndarray): desired velocity
        N (int): number of time steps
        dt (float): time step

    Returns:
        refx (np.ndarray): desired trajectory [N,3]
    """
    line_length = line.shape[0]
    delta_x = line[..., 0:2] - np.repeat(x[..., np.newaxis, 0:2], line_length, axis=-2)
    dis = np.linalg.norm(delta_x, axis=-1)
    idx = np.argmin(dis, axis=-1)
    line_min = line[idx]
    dx = x[0] - line_min[0]
    dy = x[1] - line_min[1]
    delta_y = -dx * np.sin(line_min[2]) + dy * np.cos(line_min[2])
    delta_x = dx * np.cos(line_min[2]) + dy * np.sin(line_min[2])
    refx0 = np.array(
        [
            line_min[0] + delta_x * np.cos(line_min[2]),
            line_min[1] + delta_x * np.sin(line_min[2]),
            line_min[2],
        ]
    )
    s = [np.linalg.norm(line[idx + 1, 0:2] - refx0[0:2])]
    for i in range(idx + 2, line_length):
        s.append(s[-1] + np.linalg.norm(line[i, 0:2] - line[i - 1, 0:2]))
    f = interp1d(
        np.array(s),
        line[idx + 1 :],
        kind="linear",
        axis=0,
        copy=True,
        bounds_error=False,
        fill_value="extrapolate",
        assume_sorted=True,
    )
    s1 = v * np.arange(1, N + 1) * dt
    refx = f(s1)

    return refx

def Unicycle_braking_traj(x0,dt,T,brake_acc):
    """generate braking trajectory for the unicycle model

    Args:
        x0 (np.ndarray): initial state
        dt (float): time step
        T (int): total time step
        brake_acc (float): deceleration

    Returns:
        np.ndarray: braking trajectory
    """
    if isinstance(x0,np.ndarray):
        x = np.zeros((T+1,4))
        x[0] = x0
        x[:,3]=x0[3]
        x[:,2] = np.clip(x0[2]+dt*np.arange(T+1)*brake_acc,a_min=0,a_max=np.inf)
        x[1:,:2] = x[0:1,:2] + np.cumsum(x[:-1,2])[:,None]*dt*np.array([[np.cos(x0[3]),np.sin(x0[3])]])

    elif isinstance(x0,torch.Tensor):
        x = torch.zeros((T+1,4))
        x[0] = x0
        x[:,3]=x0[3]
        x[:,2] = torch.clip(x0[2]+dt*torch.arange(T+1)*brake_acc,min=0,max=np.inf)
        x[1:,:2] = x[0:1,:2] + torch.cumsum(x[:-1,2],0)[:,None]*dt*torch.tensor([[torch.cos(x0[3]),torch.sin(x0[3])]])
    return x[1:]


def test():
    x0=torch.tensor([4,5,7.0,0])
    dt=0.1
    T=30
    brake_acc=-5.0
    x=Unicycle_braking_traj(x0,dt,T,brake_acc)
    print(x)

if __name__ == "__main__":
    test()